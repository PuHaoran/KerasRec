{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f260bf9a",
   "metadata": {},
   "source": [
    "## DIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe1c2b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import  MinMaxScaler, LabelEncoder\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import classification_report, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4caf031",
   "metadata": {},
   "source": [
    "##### 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fdc3a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape: (1956191, 14), test_df.shape: (12078, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>hist_item_id</th>\n",
       "      <th>hist_s1</th>\n",
       "      <th>hist_s2</th>\n",
       "      <th>item_id</th>\n",
       "      <th>label</th>\n",
       "      <th>rating</th>\n",
       "      <th>click_timestamp</th>\n",
       "      <th>hist_len</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>item_date</th>\n",
       "      <th>item_title</th>\n",
       "      <th>item_cate_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1181738</th>\n",
       "      <td>4791</td>\n",
       "      <td>728,3021,1888,298,2549,391,1210,45,385,3358,24...</td>\n",
       "      <td>1062,534,1576,2504,2222,3746,3207,1225,2971,10...</td>\n",
       "      <td>1179,762,1707,1202,1268,660,1405,1895,1212,374...</td>\n",
       "      <td>1621</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>78</td>\n",
       "      <td>2653,2654,0,0,0,0,0,0,0,0,0,0,0,0,0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1269480</th>\n",
       "      <td>2572</td>\n",
       "      <td>1337,1373,1074,161,548,3197,1637,1105,3350,472...</td>\n",
       "      <td>150,624,1469,2693,1603,2285,258,2729,1082,901,...</td>\n",
       "      <td>2287,1681,3106,296,1697,3199,16,25,3091,270,0,...</td>\n",
       "      <td>1187</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>973909898</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>43</td>\n",
       "      <td>2091,11,2092,0,0,0,0,0,0,0,0,0,0,0,0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270329</th>\n",
       "      <td>710</td>\n",
       "      <td>2995,1602,3766,2734,2053,3640,2625,2591,2232,2...</td>\n",
       "      <td>957,1914,1327,1935,3771,1886,3039,3206,1766,42...</td>\n",
       "      <td>1952,3456,2401,909,1275,1200,958,642,2185,712,...</td>\n",
       "      <td>1147</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>2028,2029,26,0,0,0,0,0,0,0,0,0,0,0,0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827379</th>\n",
       "      <td>3556</td>\n",
       "      <td>1352,1540,3298,346,2338,2572,1674,2205,2047,85...</td>\n",
       "      <td>163,699,892,2694,2106,3411,2032,2560,648,2093,...</td>\n",
       "      <td>32,2848,1082,1376,1544,3107,2917,3367,2919,169...</td>\n",
       "      <td>1347</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>77</td>\n",
       "      <td>2317,2318,0,0,0,0,0,0,0,0,0,0,0,0,0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224295</th>\n",
       "      <td>1404</td>\n",
       "      <td>771,2998,463,353,1371,1852,2334,1546,899,2790,...</td>\n",
       "      <td>1220,186,3683,1246,3356,233,2316,3293,1,1250,0...</td>\n",
       "      <td>1206,3617,2281,3436,34,2905,2140,1996,3475,39,...</td>\n",
       "      <td>2289</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>79</td>\n",
       "      <td>142,3411,3412,3413,3414,0,0,0,0,0,0,0,0,0,0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id                                       hist_item_id  \\\n",
       "1181738     4791  728,3021,1888,298,2549,391,1210,45,385,3358,24...   \n",
       "1269480     2572  1337,1373,1074,161,548,3197,1637,1105,3350,472...   \n",
       "1270329      710  2995,1602,3766,2734,2053,3640,2625,2591,2232,2...   \n",
       "827379      3556  1352,1540,3298,346,2338,2572,1674,2205,2047,85...   \n",
       "224295      1404  771,2998,463,353,1371,1852,2334,1546,899,2790,...   \n",
       "\n",
       "                                                   hist_s1  \\\n",
       "1181738  1062,534,1576,2504,2222,3746,3207,1225,2971,10...   \n",
       "1269480  150,624,1469,2693,1603,2285,258,2729,1082,901,...   \n",
       "1270329  957,1914,1327,1935,3771,1886,3039,3206,1766,42...   \n",
       "827379   163,699,892,2694,2106,3411,2032,2560,648,2093,...   \n",
       "224295   1220,186,3683,1246,3356,233,2316,3293,1,1250,0...   \n",
       "\n",
       "                                                   hist_s2  item_id  label  \\\n",
       "1181738  1179,762,1707,1202,1268,660,1405,1895,1212,374...     1621      0   \n",
       "1269480  2287,1681,3106,296,1697,3199,16,25,3091,270,0,...     1187      1   \n",
       "1270329  1952,3456,2401,909,1275,1200,958,642,2185,712,...     1147      0   \n",
       "827379   32,2848,1082,1376,1544,3107,2917,3367,2919,169...     1347      0   \n",
       "224295   1206,3617,2281,3436,34,2905,2140,1996,3475,39,...     2289      0   \n",
       "\n",
       "         rating  click_timestamp  hist_len  gender  age  item_date  \\\n",
       "1181738       0                0        50       1    2         78   \n",
       "1269480       5        973909898        50       1    4         43   \n",
       "1270329       0                0        50       1    3         53   \n",
       "827379        0                0        50       1    3         77   \n",
       "224295        0                0        50       1    6         79   \n",
       "\n",
       "                                          item_title  item_cate_id  \n",
       "1181738          2653,2654,0,0,0,0,0,0,0,0,0,0,0,0,0            14  \n",
       "1269480         2091,11,2092,0,0,0,0,0,0,0,0,0,0,0,0             2  \n",
       "1270329         2028,2029,26,0,0,0,0,0,0,0,0,0,0,0,0             5  \n",
       "827379           2317,2318,0,0,0,0,0,0,0,0,0,0,0,0,0             4  \n",
       "224295   142,3411,3412,3413,3414,0,0,0,0,0,0,0,0,0,0             8  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/ml-1m/train_df.csv')\n",
    "test_df = pd.read_csv('../data/ml-1m/test_df.csv')\n",
    "data = train_df.append(test_df)\n",
    "print('train_df.shape: {}, test_df.shape: {}'.format(train_df.shape, test_df.shape))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b15335f",
   "metadata": {},
   "source": [
    "##### 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c206a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseFeature(name='user_id', vocabulary_size=6041, embedding_size=4),\n",
       " SparseFeature(name='gender', vocabulary_size=3, embedding_size=4),\n",
       " SparseFeature(name='age', vocabulary_size=8, embedding_size=4),\n",
       " SparseFeature(name='item_id', vocabulary_size=3884, embedding_size=4),\n",
       " SparseFeature(name='item_cate_id', vocabulary_size=19, embedding_size=4),\n",
       " DenseFeature(name='hist_len', dimension=1),\n",
       " VarLenSparseFeature(name='hist_item_id', vocabulary_size=3884, embedding_size=4, maxlen=50)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparseFeature = namedtuple('SparseFeature', ['name', 'vocabulary_size', 'embedding_size'])\n",
    "DenseFeature = namedtuple('DenseFeature', ['name', 'dimension'])\n",
    "VarLenSparseFeature = namedtuple('VarLenSparseFeature', ['name', 'vocabulary_size', 'embedding_size', 'maxlen'])\n",
    "\n",
    "feature_columns = [\n",
    "    SparseFeature('user_id', data.user_id.max()+1, embedding_size=4),\n",
    "    SparseFeature('gender', data.gender.max()+1, embedding_size=4),\n",
    "    SparseFeature('age', data.age.max()+1, embedding_size=4),\n",
    "    SparseFeature('item_id', data.item_id.max()+1, embedding_size=4),\n",
    "    SparseFeature('item_cate_id', data.item_cate_id.max()+1, embedding_size=4),\n",
    "    DenseFeature('hist_len', 1),\n",
    "    VarLenSparseFeature('hist_item_id', data.item_id.max()+1, embedding_size=4, maxlen=50)\n",
    "]\n",
    "\n",
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62763314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_id (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gender (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "age (InputLayer)                [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_id (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_cate_id (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "emb_user_id (Embedding)         (None, 1, 4)         24168       user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "emb_gender (Embedding)          (None, 1, 4)         16          gender[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "emb_age (Embedding)             (None, 1, 4)         36          age[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "emb_item_id (Embedding)         (None, 1, 4)         15540       item_id[0][0]                    \n",
      "                                                                 item_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "emb_item_cate_id (Embedding)    (None, 1, 4)         80          item_cate_id[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "hist_item_id (InputLayer)       [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 4)            0           emb_user_id[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 4)            0           emb_gender[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 4)            0           emb_age[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 4)            0           emb_item_id[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 4)            0           emb_item_cate_id[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "var_emb_hist_item_id (Embedding (None, 50, 4)        15540       hist_item_id[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "hist_len (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 20)           0           flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_pooling_layer_1 (Atte (None, 4)            20097       emb_item_id[1][0]                \n",
      "                                                                 var_emb_hist_item_id[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 25)           0           hist_len[0][0]                   \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 attention_pooling_layer_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 64)           1728        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 32)           2112        dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 1)            33          dense_10[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 79,350\n",
      "Trainable params: 79,350\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class LocalActivationUnit(Layer):\n",
    "    \"\"\" 对用户行为embedding和物品embedding做元素减、乘运算，进一步挖掘二者之间的关系 \"\"\"\n",
    "    def __init__(self, hidden_units=(128, 64), activation='prelu'):\n",
    "        super(LocalActivationUnit, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.linear = Dense(1)\n",
    "        self.dnn = [Dense(unit, activation=PReLU() if activation=='prelu' else Dice()) for unit in hidden_units]\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        query, keys = inputs\n",
    "        key_len = keys.get_shape()[1]\n",
    "        \n",
    "        # 复制query使其与keys维度一致\n",
    "        query = tf.tile(query, multiples=[1, key_len, 1]) # (None, 1, 4) => (None, 50, 4)\n",
    "        \n",
    "        # 对特征进行拼接（原始向量、向量差、积）\n",
    "        concat_attention_input = tf.concat([query, keys, query-keys, query*keys], axis=-1) # (None, 50, 4*4)\n",
    "        # 全连接层\n",
    "        attention_out = concat_attention_input\n",
    "        for fc in self.dnn:\n",
    "            attention_out = fc(attention_out) # (None, 50, 64)\n",
    "        attention_out = self.linear(attention_out) # (None, 50, 1)\n",
    "        attention_out = tf.squeeze(attention_out, -1) # (None, 50)\n",
    "        return attention_out\n",
    "    \n",
    "class AttentionPoolingLayer(Layer):\n",
    "    def __init__(self, attention_hidden_units=(128, 64)):\n",
    "        super(AttentionPoolingLayer, self).__init__()\n",
    "        self.attention_hidden_units = attention_hidden_units\n",
    "        self.activation_unit = LocalActivationUnit(self.attention_hidden_units)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        query, keys = inputs # (None, 1, 4) (None, 50, 4)\n",
    "        \n",
    "        # 得到行为序列下每个物品对应的注意力权重\n",
    "        attention_score = self.activation_unit([query, keys]) # (None, 50)\n",
    "        \n",
    "        # 1. 将attention_score中padding所对应的items保持权重为0。\n",
    "        keys_mask = tf.not_equal(keys[:, :, 0], 0) # (None, 50) embedding矩阵中的非零向量设置为True(这里根据向量第一个元素是否为0判断是否为非零向量)\n",
    "        paddings = tf.zeros_like(attention_score) # (None, 50) 创建一个所有元素都为0的张量\n",
    "        outputs = tf.where(keys_mask, attention_score, paddings) # (None, 50) keys_mask为True的元素值用attention_score填充，其余用paddings填充\n",
    "        \n",
    "        # 2. 将权重扩充到keys相同维度，进行矩阵乘法（等价于向量与权重加权求和操作）。\n",
    "        outputs = tf.expand_dims(outputs, axis=1) # (None, 1, 50)\n",
    "        outputs = tf.matmul(outputs, keys) # (None, 1, 50) * (None, 50, 4) => (None, 1, 4) 矩阵乘法相当于对所有向量进行加权求和，返回用户兴趣向量\n",
    "        outputs = tf.squeeze(outputs, axis=1) # (None, 4)\n",
    "        return outputs # (None, 4)\n",
    "    \n",
    "def model_metric(prob, label, thr=0.5):\n",
    "    \"\"\" 模型评估 \"\"\"\n",
    "    # AUC\n",
    "    fpr, tpr, threshold = metrics.roc_curve(label, prob)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    score = metrics.accuracy_score(label, prob > thr)\n",
    "    # LogLoss\n",
    "    logloss = log_loss(label, prob)\n",
    "    print('模型准确率:{}, AUC得分:{}, LogLoss:{}'.format(score, auc, logloss))\n",
    "    print(classification_report(label, prob > thr, digits=2))\n",
    "    print('==========================================================')\n",
    "\n",
    "def build_input_layers(feature_columns):\n",
    "    \"\"\" 构建输入层 \"\"\"\n",
    "    dense_input_dict, sparse_input_dict, varlen_sparse_input_dict = {}, {}, {}\n",
    "    for f in feature_columns:\n",
    "        if isinstance(f, DenseFeature):\n",
    "            dense_input_dict[f.name] = Input(shape=(f.dimension, ), name=f.name)\n",
    "        elif isinstance(f, SparseFeature):\n",
    "            sparse_input_dict[f.name] = Input(shape=(1, ), name=f.name)\n",
    "        elif isinstance(f, VarLenSparseFeature):\n",
    "            varlen_sparse_input_dict[f.name] = Input(shape=(f.maxlen, ), name=f.name)\n",
    "    return dense_input_dict, sparse_input_dict, varlen_sparse_input_dict\n",
    "    \n",
    "def concat_input_list(input_list):\n",
    "    \"\"\" 合并input列表 \"\"\"\n",
    "    _num = len(input_list)\n",
    "    if _num > 1:\n",
    "        return Concatenate(axis=1)(input_list)\n",
    "    elif len(input_list) == 1:\n",
    "        return input_list[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def build_embedding_layers(feature_columns):\n",
    "    \"\"\" 构建embedding层 \"\"\"\n",
    "    embedding_layer_dict = {}\n",
    "    for f in feature_columns:\n",
    "        if isinstance(f, SparseFeature):\n",
    "            embedding_layer_dict[f.name] = Embedding(f.vocabulary_size+1, f.embedding_size, name='emb_' + f.name)\n",
    "        elif isinstance(f, VarLenSparseFeature):\n",
    "            embedding_layer_dict[f.name] = Embedding(f.vocabulary_size+1, f.embedding_size, name='var_emb_' + f.name, mask_zero=True)\n",
    "    return embedding_layer_dict\n",
    "\n",
    "def embedding_lookup(columns, input_dict, embedding_layer_dict, flatten=False):\n",
    "    \"\"\" 根据feature_columns或column_names查表，得到对应embedding向量列表 \"\"\"\n",
    "    embedding_list = []\n",
    "    for f in columns:\n",
    "        if type(f) == str:\n",
    "            column_name = f\n",
    "        else:\n",
    "            column_name = f.name\n",
    "        _input = input_dict[column_name]\n",
    "        _embed = embedding_layer_dict[column_name]\n",
    "        embed_layer = _embed(_input)\n",
    "        if flatten:\n",
    "            embed_layer = Flatten()(embed_layer)\n",
    "        embedding_list.append(embed_layer)\n",
    "    return embedding_list\n",
    "\n",
    "def get_dnn_logits(dnn_input, hidden_units=(100, 40), activation='prelu'):\n",
    "    dnn_list = [Dense(unit, activation=PReLU() if activation=='prelu' else Dice()) for unit in hidden_units]\n",
    "    dnn_out = dnn_input\n",
    "    for dnn in dnn_list:\n",
    "        dnn_out = dnn(dnn_out)\n",
    "    dnn_logits = Dense(1, activation='sigmoid')(dnn_out)\n",
    "    return dnn_logits\n",
    "\n",
    "def DIN(feature_columns, behavior_column_names, behavior_seq_column_names):\n",
    "    \"\"\" Deep Interest Network \"\"\"\n",
    "    dense_input_dict, sparse_input_dict, varlen_sparse_input_dict = build_input_layers(feature_columns)\n",
    "    # Input\n",
    "    input_list = list(dense_input_dict.values()) + list(sparse_input_dict.values()) + list(varlen_sparse_input_dict.values())\n",
    "    \n",
    "    # dense feature (input->concat)\n",
    "    concat_dense_input_list = concat_input_list(list(dense_input_dict.values()))\n",
    "    \n",
    "    # sparse feature (input->embed->concat)\n",
    "    embedding_layer_dict = build_embedding_layers(feature_columns)\n",
    "    sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeature), feature_columns))\n",
    "    flatten_sparse_embed_list = embedding_lookup(sparse_feature_columns, sparse_input_dict, embedding_layer_dict, flatten=True)\n",
    "    concat_flatten_sparse_embed_list = concat_input_list(flatten_sparse_embed_list)\n",
    "    \n",
    "    # 当前物品的embedding列表\n",
    "    query_embed_list = embedding_lookup(behavior_column_names, sparse_input_dict, embedding_layer_dict, flatten=False) # [(None, 1, 4)]\n",
    "    # 当前行为序列的embedding列表\n",
    "    keys_embed_list = embedding_lookup(behavior_seq_column_names, varlen_sparse_input_dict, embedding_layer_dict, flatten=False) # [(None, 50, 4)]\n",
    "    \n",
    "    seq_embed_list = []\n",
    "    # 使用注意力机制将历史行为序列对应的embedding进行池化(这里可能有多个ID及ID对应的历史序列，eg：点击物品与点击行为序列，搜索物品和搜索行为序列)\n",
    "    for i in range(len(query_embed_list)):\n",
    "        seq_embed = AttentionPoolingLayer()([query_embed_list[i], keys_embed_list[i]])\n",
    "        seq_embed_list.append(seq_embed)\n",
    "    \n",
    "    # 拼接用户兴趣向量(拼接序列embeddings和注意力权重，然后加权求和)\n",
    "    concat_seq_embed_list = concat_input_list(seq_embed_list) # (None, 4)\n",
    "    \n",
    "    #dnn_input = Concatenate(axis=1)([concat_dense_input_list, concat_flatten_sparse_embed_list]) # Embedding + MLP结构\n",
    "    # concat dense feature + concat sparse embeddings + concat seq embeddings\n",
    "    dnn_input = Concatenate(axis=1)([concat_dense_input_list, concat_flatten_sparse_embed_list, concat_seq_embed_list]) # DIN结构\n",
    "    \n",
    "    dnn_logits = get_dnn_logits(dnn_input, hidden_units=(64, 32), activation='prelu')\n",
    "    model = Model(input_list, dnn_logits)\n",
    "    return model\n",
    "\n",
    "behavior_column_names, behavior_seq_column_names = ['item_id'], ['hist_item_id']\n",
    "model = DIN(feature_columns, behavior_column_names, behavior_seq_column_names)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdcb060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1529/1529 [==============================] - 150s 97ms/step - loss: 0.4645 - binary_crossentropy: 0.4645 - auc: 0.8559 - val_loss: 0.4364 - val_binary_crossentropy: 0.4364 - val_auc: 0.8756\n",
      "Epoch 2/100\n",
      "1529/1529 [==============================] - 164s 107ms/step - loss: 0.3934 - binary_crossentropy: 0.3934 - auc: 0.9008 - val_loss: 0.3639 - val_binary_crossentropy: 0.3639 - val_auc: 0.9176\n",
      "Epoch 3/100\n",
      "1529/1529 [==============================] - 174s 114ms/step - loss: 0.3368 - binary_crossentropy: 0.3368 - auc: 0.9289 - val_loss: 0.3307 - val_binary_crossentropy: 0.3307 - val_auc: 0.9327\n",
      "Epoch 4/100\n",
      "1529/1529 [==============================] - 181s 119ms/step - loss: 0.3086 - binary_crossentropy: 0.3086 - auc: 0.9407 - val_loss: 0.3115 - val_binary_crossentropy: 0.3115 - val_auc: 0.9399\n",
      "Epoch 5/100\n",
      "1529/1529 [==============================] - 174s 114ms/step - loss: 0.2931 - binary_crossentropy: 0.2931 - auc: 0.9465 - val_loss: 0.3197 - val_binary_crossentropy: 0.3197 - val_auc: 0.9428\n",
      "Epoch 6/100\n",
      "1529/1529 [==============================] - 179s 117ms/step - loss: 0.2837 - binary_crossentropy: 0.2837 - auc: 0.9500 - val_loss: 0.2999 - val_binary_crossentropy: 0.2999 - val_auc: 0.9449\n",
      "Epoch 7/100\n",
      "1529/1529 [==============================] - 179s 117ms/step - loss: 0.2774 - binary_crossentropy: 0.2774 - auc: 0.9522 - val_loss: 0.2954 - val_binary_crossentropy: 0.2954 - val_auc: 0.9463\n",
      "Epoch 8/100\n",
      "1318/1529 [========================>.....] - ETA: 24s - loss: 0.2722 - binary_crossentropy: 0.2722 - auc: 0.9539"
     ]
    }
   ],
   "source": [
    "train_input = {\n",
    "    'user_id': np.array(train_df['user_id']),\n",
    "    'gender': np.array(train_df['gender']),\n",
    "    'age': np.array(train_df['age']),\n",
    "    'item_id': np.array(train_df['item_id']),\n",
    "    'item_cate_id': np.array(train_df['item_cate_id']),\n",
    "    'hist_item_id': np.array([[int(i) for i in s.split(',')] for s in train_df['hist_item_id']]),\n",
    "    'hist_len': np.array(train_df['hist_len']),\n",
    "}\n",
    "test_input = {\n",
    "    'user_id': np.array(test_df['user_id']),\n",
    "    'gender': np.array(test_df['gender']),\n",
    "    'age': np.array(test_df['age']),\n",
    "    'item_id': np.array(test_df['item_id']),\n",
    "    'item_cate_id': np.array(test_df['item_cate_id']),\n",
    "    'hist_item_id': np.array([[int(i) for i in s.split(',')] for s in test_df['hist_item_id']]),\n",
    "    'hist_len': np.array(test_df['hist_len']),\n",
    "}\n",
    "\n",
    "# 模型训练\n",
    "my_callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, verbose=2, mode='auto')\n",
    "]\n",
    "model.compile('adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[\"binary_crossentropy\", tf.keras.metrics.AUC(name='auc')])\n",
    "model.fit(train_input,\n",
    "          train_df['label'].values,\n",
    "          batch_size=1024,\n",
    "          epochs=100,\n",
    "          validation_split=0.2,\n",
    "          callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75d8b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型预测与评估\n",
    "result = model.predict(test_input)\n",
    "model_metric(np.array([i[0] for i in result]), test_df['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe89bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
