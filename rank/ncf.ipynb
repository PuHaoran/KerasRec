{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c27380",
   "metadata": {},
   "source": [
    "## NCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8974f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import  MinMaxScaler, LabelEncoder\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import classification_report, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf5d05b",
   "metadata": {},
   "source": [
    "##### 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be6f4210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape: (1956191, 14), test_df.shape: (12078, 14)\n"
     ]
    }
   ],
   "source": [
    "def get_ml1m_data():\n",
    "    \"\"\" 读取ml1m数据集 \"\"\"\n",
    "    train_path = '../data/ml-1m/train_df.csv'\n",
    "    test_path = '../data/ml-1m/test_df.csv'\n",
    "    encoder_dict_path = '../data/ml-1m/encoder_dict.pkl'\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    encoder_dict = joblib.load(encoder_dict_path)\n",
    "    return train_df, test_df, encoder_dict\n",
    "\n",
    "def parse_strlist(x):\n",
    "    return [int(i) for i in x.split(',') if i.strip() != '']\n",
    "\n",
    "train_df, test_df, encoder_dict = get_ml1m_data()\n",
    "train_df = train_df.sample(frac=1.0)\n",
    "data = pd.concat([train_df, test_df], axis=0)\n",
    "print('train_df.shape: {}, test_df.shape: {}'.format(train_df.shape, test_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dbc167",
   "metadata": {},
   "source": [
    "##### 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "170e5e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([SparseFeature(name='user_id', vocabulary_size=6041, embedding_size=8)],\n",
       " [SparseFeature(name='item_id', vocabulary_size=3884, embedding_size=8)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_embeding_dim(x):\n",
    "    return int(x**.25) + 1\n",
    "\n",
    "SparseFeature = namedtuple('SparseFeature', ['name', 'vocabulary_size', 'embedding_size'])\n",
    "DenseFeature = namedtuple('DenseFeature', ['name', 'dimension'])\n",
    "\n",
    "user_feature_columns = [\n",
    "    SparseFeature('user_id', data.user_id.max()+1, embedding_size=8),\n",
    "]\n",
    "item_feature_columns = [\n",
    "    SparseFeature('item_id', data.item_id.max()+1, embedding_size=8),\n",
    "]\n",
    "user_feature_columns, item_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4473e4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_id': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'user_id')>} {'user_id': <keras.layers.embeddings.Embedding object at 0x000001CDE2D50BE0>}\n",
      "hidden_units:  [8]\n",
      "dnn_output.shape:  (None, 8)\n",
      "gmf_output.shape:  (None, 8)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_id (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_id (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mlpemb_user_id (Embedding)      (None, 1, 8)         48336       user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mlpemb_item_id (Embedding)      (None, 1, 8)         31080       item_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8)            0           mlpemb_user_id[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 8)            0           mlpemb_item_id[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16)           0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gmfemb_user_id (Embedding)      (None, 1, 8)         48336       user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gmfemb_item_id (Embedding)      (None, 1, 8)         31080       item_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 8)            136         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8)            0           gmfemb_user_id[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8)            0           gmfemb_item_id[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 8)            0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply (TFOpLambda)   (None, 8)            0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 16)           0           dropout[0][0]                    \n",
      "                                                                 tf.math.multiply[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            17          concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 158,985\n",
      "Trainable params: 158,985\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def model_metric(prob, label, thr=0.5):\n",
    "    \"\"\" 模型评估 \"\"\"\n",
    "    # AUC\n",
    "    fpr, tpr, threshold = metrics.roc_curve(label, prob)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    score = metrics.accuracy_score(label, prob > thr)\n",
    "    # LogLoss\n",
    "    logloss = log_loss(label, prob)\n",
    "    print('模型准确率:{}, AUC得分:{}, LogLoss:{}'.format(score, auc, logloss))\n",
    "    print(classification_report(label, prob > thr, digits=2))\n",
    "    print('==========================================================')\n",
    "    \n",
    "def build_input_layers(feature_columns):\n",
    "    \"\"\" 构建输入层 \"\"\"\n",
    "    dense_input_dict, sparse_input_dict, varlen_sparse_input_dict = {}, {}, {}\n",
    "    for f in feature_columns:\n",
    "        if isinstance(f, DenseFeature):\n",
    "            dense_input_dict[f.name] = Input(shape=(f.dimension, ), name=f.name)\n",
    "        elif isinstance(f, SparseFeature):\n",
    "            sparse_input_dict[f.name] = Input(shape=(1, ), name=f.name)\n",
    "        elif isinstance(f, VarLenSparseFeature):\n",
    "            varlen_sparse_input_dict[f.name] = Input(shape=(f.maxlen, ), name=f.name)\n",
    "    return dense_input_dict, sparse_input_dict, varlen_sparse_input_dict\n",
    "    \n",
    "def build_embedding_layers(feature_columns, prefix='sparse_'):\n",
    "    \"\"\" 构建embedding层 \"\"\"\n",
    "    embedding_layer_dict = {}\n",
    "    for f in feature_columns:\n",
    "        if isinstance(f, SparseFeature):\n",
    "            embedding_layer_dict[f.name] = Embedding(f.vocabulary_size+1, f.embedding_size, name=prefix + 'emb_' + f.name)\n",
    "        elif isinstance(f, VarLenSparseFeature):\n",
    "            embedding_layer_dict[f.name] = Embedding(f.vocabulary_size+1, f.embedding_size, name=prefix + 'var_emb_' + f.name, mask_zero=True)\n",
    "    return embedding_layer_dict\n",
    "    \n",
    "def embedding_lookup(columns, input_dict, embedding_layer_dict, flatten=False):\n",
    "    \"\"\" 根据feature_columns或column_names查表，得到对应embedding向量列表 \"\"\"\n",
    "    embedding_list = []\n",
    "    for f in columns:\n",
    "        if type(f) == str:\n",
    "            column_name = f\n",
    "        else:\n",
    "            column_name = f.name\n",
    "        _input = input_dict[column_name]\n",
    "        _embed = embedding_layer_dict[column_name]\n",
    "        embed_layer = _embed(_input)\n",
    "        if flatten:\n",
    "            embed_layer = Flatten()(embed_layer)\n",
    "        embedding_list.append(embed_layer)\n",
    "    return embedding_list\n",
    "  \n",
    "def get_dnn(dnn_input, hidden_units=[64, 32], activation='relu', l2=0.01):\n",
    "    print('hidden_units: ', hidden_units)\n",
    "    dnn_list = [Dense(unit, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(l2=l2)) for unit in hidden_units]\n",
    "    dnn_out = dnn_input\n",
    "    for dnn in dnn_list:\n",
    "        dnn_out = Dropout(0.5)(dnn(dnn_out)) # dnn(dnn_out)\n",
    "    return dnn_out\n",
    "    \n",
    "def NCF(user_feature_columns,\n",
    "        item_feature_columns, \n",
    "        hidden_units):\n",
    "    user_dense_input_dict, user_sparse_input_dict, _ = build_input_layers(user_feature_columns)\n",
    "    item_dense_input_dict, item_sparse_input_dict, _ = build_input_layers(item_feature_columns)\n",
    "    \n",
    "    # user/item Input\n",
    "    user_input_list = list(user_sparse_input_dict.values())#list(gmf_user_sparse_input_dict.values()) + list(mlp_user_sparse_input_dict.values())\n",
    "    item_input_list = list(item_sparse_input_dict.values())#list(gmf_item_sparse_input_dict.values()) + list(mlp_item_sparse_input_dict.values())\n",
    "\n",
    "    gmf_user_embedding_layer_dict = build_embedding_layers(user_feature_columns, prefix='gmf')\n",
    "    gmf_item_embedding_layer_dict = build_embedding_layers(item_feature_columns, prefix='gmf')\n",
    "    print(user_sparse_input_dict, gmf_user_embedding_layer_dict)\n",
    "    gmf_user_sparse_embed_list = embedding_lookup(user_feature_columns, user_sparse_input_dict, gmf_user_embedding_layer_dict, flatten=True)\n",
    "    gmf_item_sparse_embed_list = embedding_lookup(item_feature_columns, item_sparse_input_dict, gmf_item_embedding_layer_dict, flatten=True)\n",
    "    \n",
    "    \n",
    "    mlp_user_embedding_layer_dict = build_embedding_layers(user_feature_columns, prefix='mlp')\n",
    "    mlp_item_embedding_layer_dict = build_embedding_layers(item_feature_columns, prefix='mlp')\n",
    "        \n",
    "    mlp_user_sparse_embed_list = embedding_lookup(user_feature_columns, user_sparse_input_dict, mlp_user_embedding_layer_dict, flatten=True)\n",
    "    mlp_item_sparse_embed_list = embedding_lookup(item_feature_columns, item_sparse_input_dict, mlp_item_embedding_layer_dict, flatten=True)\n",
    "    \n",
    "    # GMF\n",
    "    gmf_output = gmf_user_sparse_embed_list[0] * gmf_item_sparse_embed_list[0]\n",
    "    \n",
    "    # MLP\n",
    "    dnn_input = Concatenate(axis=1)(mlp_user_sparse_embed_list+mlp_item_sparse_embed_list)\n",
    "    dnn_output = get_dnn(dnn_input, hidden_units=hidden_units)\n",
    "    \n",
    "    print('dnn_output.shape: ', dnn_output.shape)\n",
    "    print('gmf_output.shape: ', gmf_output.shape)\n",
    "    concate_mlp_gmf = Concatenate(axis=1)([dnn_output, gmf_output])\n",
    "    \n",
    "    output_layer = Dense(1, activation='sigmoid')(concate_mlp_gmf)\n",
    "    \n",
    "    model = Model(user_input_list+item_input_list, output_layer)\n",
    "    \n",
    "    model.__setattr__(\"user_input\", user_input_list)\n",
    "    model.__setattr__(\"item_input\", item_input_list)\n",
    "    model.__setattr__(\"user_embedding\", mlp_user_sparse_embed_list[0])\n",
    "    model.__setattr__(\"item_embedding\", mlp_item_sparse_embed_list[0])\n",
    "    return model\n",
    "\n",
    "model = NCF(user_feature_columns,\n",
    "            item_feature_columns,  \n",
    "            hidden_units=[8])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b335655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.3178 - binary_crossentropy: 0.3174 - auc: 0.9363 - val_loss: 0.3705 - val_binary_crossentropy: 0.3702 - val_auc: 0.9161ss: 0.3152 - binary_crossentropy: 0.3\n",
      "Epoch 2/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.3172 - binary_crossentropy: 0.3168 - auc: 0.9366 - val_loss: 0.3704 - val_binary_crossentropy: 0.3700 - val_auc: 0.9162\n",
      "Epoch 3/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.3169 - binary_crossentropy: 0.3165 - auc: 0.9367 - val_loss: 0.3706 - val_binary_crossentropy: 0.3702 - val_auc: 0.9162 - loss: 0.3129 - binary_crossentropy: 0.3125 - auc: 0. - ETA: 1s - loss: 0.3137 - binary_crossentropy:\n",
      "Epoch 4/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.3166 - binary_crossentropy: 0.3163 - auc: 0.9368 - val_loss: 0.3712 - val_binary_crossentropy: 0.3708 - val_auc: 0.9160\n",
      "Epoch 5/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.3162 - binary_crossentropy: 0.3159 - auc: 0.9369 - val_loss: 0.3716 - val_binary_crossentropy: 0.3713 - val_auc: 0.9158\n",
      "Epoch 6/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.3160 - binary_crossentropy: 0.3156 - auc: 0.9370 - val_loss: 0.3716 - val_binary_crossentropy: 0.3712 - val_auc: 0.9161- binary_crossentropy: 0.3\n",
      "Epoch 7/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.3158 - binary_crossentropy: 0.3154 - auc: 0.9371 - val_loss: 0.3718 - val_binary_crossentropy: 0.3714 - val_auc: 0.9159\n",
      "Epoch 8/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.3155 - binary_crossentropy: 0.3152 - auc: 0.9372 - val_loss: 0.3720 - val_binary_crossentropy: 0.3717 - val_auc: 0.9158\n",
      "Epoch 9/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.3154 - binary_crossentropy: 0.3150 - auc: 0.9372 - val_loss: 0.3728 - val_binary_crossentropy: 0.3724 - val_auc: 0.9157- loss: 0.3125 - binary_crossentropy\n",
      "Epoch 10/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.3151 - binary_crossentropy: 0.3147 - auc: 0.9373 - val_loss: 0.3728 - val_binary_crossentropy: 0.3725 - val_auc: 0.9157loss: 0.3097 - binary_crossentropy: 0.3094 - ETA: 1s - loss: 0.3113 - binary_crossentrop\n",
      "Epoch 11/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.3150 - binary_crossentropy: 0.3146 - auc: 0.9374 - val_loss: 0.3733 - val_binary_crossentropy: 0.3730 - val_auc: 0.91566 - binary_crossentropy: 0.3133 - a - ETA: 0s - loss: 0.3147 - binary_crossentropy: 0.3144 - auc: 0.\n",
      "Epoch 12/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.3146 - binary_crossentropy: 0.3143 - auc: 0.9375 - val_loss: 0.3730 - val_binary_crossentropy: 0.3727 - val_auc: 0.9157ntropy: 0.3128 - a\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cde35f99e8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input = {\n",
    "    'user_id': np.array(train_df['user_id']),\n",
    "    'item_id': np.array(train_df['item_id'])\n",
    "}\n",
    "test_input = {\n",
    "    'user_id': np.array(test_df['user_id']),\n",
    "    'item_id': np.array(test_df['item_id'])\n",
    "}\n",
    "# 物品表\n",
    "item_df = data[['item_id', 'item_cate_id']].drop_duplicates(['item_id'])\n",
    "item_input = {\n",
    "    'item_id': np.array(item_df['item_id']),\n",
    "    'item_cate_id': np.array(item_df['item_cate_id']),\n",
    "}\n",
    "\n",
    "# 模型训练\n",
    "my_callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, verbose=2, mode='auto')\n",
    "]\n",
    "model.compile('adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[\"binary_crossentropy\", tf.keras.metrics.AUC(name='auc')])\n",
    "model.fit(train_input,\n",
    "          train_df['label'].values,\n",
    "          batch_size=1024,\n",
    "          epochs=100,\n",
    "          validation_split=0.2,\n",
    "          callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70f9e4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型准确率:0.7977314124855108, AUC得分:0.877665595818097, LogLoss:0.4834402552561367\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.80      6038\n",
      "           1       0.81      0.78      0.79      6040\n",
      "\n",
      "    accuracy                           0.80     12078\n",
      "   macro avg       0.80      0.80      0.80     12078\n",
      "weighted avg       0.80      0.80      0.80     12078\n",
      "\n",
      "==========================================================\n"
     ]
    }
   ],
   "source": [
    "# 模型预测与评估\n",
    "result = model.predict(test_input)\n",
    "model_metric(np.array([i[0] for i in result]).astype(np.float64), test_df['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11349c40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
