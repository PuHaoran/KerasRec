{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "538dc221",
   "metadata": {},
   "source": [
    "## DeepFM+Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c54d7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import  MinMaxScaler, LabelEncoder\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import classification_report, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d86ccf7",
   "metadata": {},
   "source": [
    "##### 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f25d26c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape: (1956191, 14), test_df.shape: (12078, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>hist_item_id</th>\n",
       "      <th>hist_s1</th>\n",
       "      <th>hist_s2</th>\n",
       "      <th>item_id</th>\n",
       "      <th>label</th>\n",
       "      <th>rating</th>\n",
       "      <th>click_timestamp</th>\n",
       "      <th>hist_len</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>item_date</th>\n",
       "      <th>item_title</th>\n",
       "      <th>item_cate_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1741</td>\n",
       "      <td>2904,2008,230,1169,2064,3475,1952,1228,943,275...</td>\n",
       "      <td>898,948,3022,1888,2891,1080,2204,618,1372,852,...</td>\n",
       "      <td>3722,1244,3023,1594,3348,288,323,36,1226,1254,...</td>\n",
       "      <td>3266</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>974711543</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>4566,4567,0,0,0,0,0,0,0,0,0,0,0,0,0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1880</td>\n",
       "      <td>1280,1213,1075,1274,1135,3292,2902,2008,3000,3...</td>\n",
       "      <td>3410,2666,3723,1109,2046,2078,1891,3639,2874,2...</td>\n",
       "      <td>3101,2671,1271,2406,3616,1158,2483,2858,2861,2...</td>\n",
       "      <td>3266</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>4566,4567,0,0,0,0,0,0,0,0,0,0,0,0,0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3292</td>\n",
       "      <td>1895,1016,2030,2981,3008,2068,998,1002,1006,28...</td>\n",
       "      <td>3438,1948,3692,1963,1964,1178,1360,2134,3027,2...</td>\n",
       "      <td>2171,2972,3031,3176,1110,2041,2711,1059,3554,3...</td>\n",
       "      <td>3266</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>968098376</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>4566,4567,0,0,0,0,0,0,0,0,0,0,0,0,0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>566</td>\n",
       "      <td>2789,2069,1013,613,3657,3677,774,1965,3219,164...</td>\n",
       "      <td>1248,590,897,605,2118,2694,110,3483,2751,1024,...</td>\n",
       "      <td>1644,643,3377,1557,1599,2058,159,2270,50,1074,...</td>\n",
       "      <td>3266</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>976210413</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>4566,4567,0,0,0,0,0,0,0,0,0,0,0,0,0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1088</td>\n",
       "      <td>52,1394,2851,2643,2909,416,1665,733,984,2266,1...</td>\n",
       "      <td>1233,2772,1815,539,3431,2761,838,1225,11,3029,...</td>\n",
       "      <td>2703,886,1253,1261,2365,3087,941,305,3061,711,...</td>\n",
       "      <td>3266</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1023534057</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>4566,4567,0,0,0,0,0,0,0,0,0,0,0,0,0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                       hist_item_id  \\\n",
       "0     1741  2904,2008,230,1169,2064,3475,1952,1228,943,275...   \n",
       "1     1880  1280,1213,1075,1274,1135,3292,2902,2008,3000,3...   \n",
       "2     3292  1895,1016,2030,2981,3008,2068,998,1002,1006,28...   \n",
       "3      566  2789,2069,1013,613,3657,3677,774,1965,3219,164...   \n",
       "4     1088  52,1394,2851,2643,2909,416,1665,733,984,2266,1...   \n",
       "\n",
       "                                             hist_s1  \\\n",
       "0  898,948,3022,1888,2891,1080,2204,618,1372,852,...   \n",
       "1  3410,2666,3723,1109,2046,2078,1891,3639,2874,2...   \n",
       "2  3438,1948,3692,1963,1964,1178,1360,2134,3027,2...   \n",
       "3  1248,590,897,605,2118,2694,110,3483,2751,1024,...   \n",
       "4  1233,2772,1815,539,3431,2761,838,1225,11,3029,...   \n",
       "\n",
       "                                             hist_s2  item_id  label  rating  \\\n",
       "0  3722,1244,3023,1594,3348,288,323,36,1226,1254,...     3266      1       4   \n",
       "1  3101,2671,1271,2406,3616,1158,2483,2858,2861,2...     3266      0       0   \n",
       "2  2171,2972,3031,3176,1110,2041,2711,1059,3554,3...     3266      1       5   \n",
       "3  1644,643,3377,1557,1599,2058,159,2270,50,1074,...     3266      1       2   \n",
       "4  2703,886,1253,1261,2365,3087,941,305,3061,711,...     3266      1       5   \n",
       "\n",
       "   click_timestamp  hist_len  gender  age  item_date  \\\n",
       "0        974711543        50       1    4         29   \n",
       "1                0        50       1    4         29   \n",
       "2        968098376        50       2    6         29   \n",
       "3        976210413        50       1    3         29   \n",
       "4       1023534057        50       2    1         29   \n",
       "\n",
       "                            item_title  item_cate_id  \n",
       "0  4566,4567,0,0,0,0,0,0,0,0,0,0,0,0,0             6  \n",
       "1  4566,4567,0,0,0,0,0,0,0,0,0,0,0,0,0             6  \n",
       "2  4566,4567,0,0,0,0,0,0,0,0,0,0,0,0,0             6  \n",
       "3  4566,4567,0,0,0,0,0,0,0,0,0,0,0,0,0             6  \n",
       "4  4566,4567,0,0,0,0,0,0,0,0,0,0,0,0,0             6  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/ml-1m/train_df.csv')\n",
    "test_df = pd.read_csv('../data/ml-1m/test_df.csv')\n",
    "data = train_df.append(test_df)\n",
    "print('train_df.shape: {}, test_df.shape: {}'.format(train_df.shape, test_df.shape))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0c4b1c",
   "metadata": {},
   "source": [
    "##### 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99af813b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseFeature(name='user_id', vocabulary_size=6041, embedding_size=4),\n",
       " SparseFeature(name='gender', vocabulary_size=3, embedding_size=4),\n",
       " SparseFeature(name='age', vocabulary_size=8, embedding_size=4),\n",
       " SparseFeature(name='item_id', vocabulary_size=3884, embedding_size=4),\n",
       " SparseFeature(name='item_cate_id', vocabulary_size=19, embedding_size=4),\n",
       " DenseFeature(name='hist_len', dimension=1),\n",
       " VarLenSparseFeature(name='hist_item_id', vocabulary_size=3884, embedding_size=4, maxlen=50)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparseFeature = namedtuple('SparseFeature', ['name', 'vocabulary_size', 'embedding_size'])\n",
    "DenseFeature = namedtuple('DenseFeature', ['name', 'dimension'])\n",
    "VarLenSparseFeature = namedtuple('VarLenSparseFeature', ['name', 'vocabulary_size', 'embedding_size', 'maxlen'])\n",
    "\n",
    "feature_columns = [\n",
    "    SparseFeature('user_id', data.user_id.max()+1, embedding_size=4),\n",
    "    SparseFeature('gender', data.gender.max()+1, embedding_size=4),\n",
    "    SparseFeature('age', data.age.max()+1, embedding_size=4),\n",
    "    SparseFeature('item_id', data.item_id.max()+1, embedding_size=4),\n",
    "    SparseFeature('item_cate_id', data.item_cate_id.max()+1, embedding_size=4),\n",
    "    DenseFeature('hist_len', 1),\n",
    "    VarLenSparseFeature('hist_item_id', data.item_id.max()+1, embedding_size=4, maxlen=50)\n",
    "]\n",
    "\n",
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8799d2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concat_embed_values.shape:  (None, 5, 4)\n",
      "sum_square.shape:  (None, 1, 4)\n",
      "square_sum.shape:  (None, 1, 4)\n",
      "output.shape:  (None, 1)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_id (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gender (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "age (InputLayer)                [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_id (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_cate_id (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "kd_emb_user_id (Embedding)      (None, 1, 4)         24168       user_id[0][0]                    \n",
      "                                                                 user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "kd_emb_gender (Embedding)       (None, 1, 4)         16          gender[0][0]                     \n",
      "                                                                 gender[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "kd_emb_age (Embedding)          (None, 1, 4)         36          age[0][0]                        \n",
      "                                                                 age[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "kd_emb_item_id (Embedding)      (None, 1, 4)         15540       item_id[0][0]                    \n",
      "                                                                 item_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "kd_emb_item_cate_id (Embedding) (None, 1, 4)         80          item_cate_id[0][0]               \n",
      "                                                                 item_cate_id[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 4)            0           kd_emb_user_id[1][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 4)            0           kd_emb_gender[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 4)            0           kd_emb_age[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 4)            0           kd_emb_item_id[1][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 4)            0           kd_emb_item_cate_id[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 20)           0           flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4)            84          concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "hist_len (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "1d_emb_user_id (Embedding)      (None, 1, 1)         6042        user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "1d_emb_gender (Embedding)       (None, 1, 1)         4           gender[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "1d_emb_age (Embedding)          (None, 1, 1)         9           age[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "1d_emb_item_id (Embedding)      (None, 1, 1)         3885        item_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "1d_emb_item_cate_id (Embedding) (None, 1, 1)         20          item_cate_id[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 4)            16          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1)            0           hist_len[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1)            0           1d_emb_user_id[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1)            0           1d_emb_gender[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 1)            0           1d_emb_age[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 1)            0           1d_emb_item_id[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 1)            0           1d_emb_item_cate_id[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 4)            0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            2           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 1)            0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 5, 4)         0           kd_emb_user_id[0][0]             \n",
      "                                                                 kd_emb_gender[0][0]              \n",
      "                                                                 kd_emb_age[0][0]                 \n",
      "                                                                 kd_emb_item_id[0][0]             \n",
      "                                                                 kd_emb_item_cate_id[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 4)            0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1)            0           dense[0][0]                      \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "fm__layer (FM_Layer)            (None, 1)            0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            5           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 1)            0           add_1[0][0]                      \n",
      "                                                                 fm__layer[0][0]                  \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           add_2[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 49,907\n",
      "Trainable params: 49,899\n",
      "Non-trainable params: 8\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class FM_Layer(Layer):\n",
    "    def __init__(self):\n",
    "        super(FM_Layer, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        concat_embed_values = inputs\n",
    "        print('concat_embed_values.shape: ', concat_embed_values.shape) # (None, 26, 4)\n",
    "        sum_square = tf.square(tf.reduce_sum(concat_embed_values, axis=1, keepdims=True)) # (None, 1, 4)\n",
    "        print('sum_square.shape: ', sum_square.shape)\n",
    "        square_sum = tf.reduce_sum(concat_embed_values * concat_embed_values, axis=1, keepdims=True) # (None, 1, 4)\n",
    "        print('square_sum.shape: ', square_sum.shape)\n",
    "        output = sum_square - square_sum # 和的平方-平方的和\n",
    "        output = 0.5 * tf.reduce_sum(output, axis=2, keepdims=False) # (None 1)\n",
    "        print('output.shape: ', output.shape)\n",
    "        return output\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 1)\n",
    "    \n",
    "def model_metric(prob, label, thr=0.5):\n",
    "    \"\"\" 模型评估 \"\"\"\n",
    "    # AUC\n",
    "    fpr, tpr, threshold = metrics.roc_curve(label, prob)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    score = metrics.accuracy_score(label, prob > thr)\n",
    "    # LogLoss\n",
    "    logloss = log_loss(label, prob)\n",
    "    print('模型准确率:{}, AUC得分:{}, LogLoss:{}'.format(score, auc, logloss))\n",
    "    print(classification_report(label, prob > thr, digits=2))\n",
    "    print('==========================================================')\n",
    "    \n",
    "def build_input_layers(feature_columns):\n",
    "    \"\"\" 构建输入层 \"\"\"\n",
    "    dense_input_dict, sparse_input_dict, varlen_sparse_input_dict = {}, {}, {}\n",
    "    for f in feature_columns:\n",
    "        if isinstance(f, DenseFeature):\n",
    "            dense_input_dict[f.name] = Input(shape=(f.dimension, ), name=f.name)\n",
    "        elif isinstance(f, SparseFeature):\n",
    "            sparse_input_dict[f.name] = Input(shape=(1, ), name=f.name)\n",
    "        elif isinstance(f, VarLenSparseFeature):\n",
    "            varlen_sparse_input_dict[f.name] = Input(shape=(f.maxlen, ), name=f.name)\n",
    "    return dense_input_dict, sparse_input_dict, varlen_sparse_input_dict\n",
    "    \n",
    "def build_embedding_layers(feature_columns, is_linear):\n",
    "    \"\"\" 构建embedding层 \"\"\"\n",
    "    embedding_layer_dict = {}\n",
    "    sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeature), feature_columns))\n",
    "    varlen_sparse_feature_columns = list(filter(lambda x: isinstance(x, VarLenSparseFeature), feature_columns))\n",
    "    if is_linear:\n",
    "        # 序列特征不参与线性模型的运算\n",
    "        for f in sparse_feature_columns:\n",
    "            embedding_layer_dict[f.name] = Embedding(f.vocabulary_size+1, 1, name='1d_emb_' + f.name)\n",
    "    else:\n",
    "        for f in sparse_feature_columns:\n",
    "            embedding_layer_dict[f.name] = Embedding(f.vocabulary_size+1, f.embedding_size, name='kd_emb_' + f.name)\n",
    "        for f in varlen_sparse_feature_columns:\n",
    "            embedding_layer_dict[f.name] = Embedding(f.vocabulary_size+1, f.embedding_size, name='var_emb_' + f.name, mask_zero=True)\n",
    "    return embedding_layer_dict\n",
    "    \n",
    "def get_linear_logits(dense_input_dict, sparse_input_dict, feature_columns):\n",
    "    \"\"\" 数值特征拼接一起传入全连接层 + 类别特征onehot，flatten，add \"\"\"\n",
    "    concat_dense_inputs = Concatenate(axis=1)(list(dense_input_dict.values()))\n",
    "    dense_logits_output = Dense(1)(concat_dense_inputs)\n",
    "    \n",
    "    sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeature), feature_columns))\n",
    "    embedding_layer_dict = build_embedding_layers(sparse_feature_columns, is_linear=True)\n",
    "    \n",
    "    # embedding(input)查表操作，返回对应input的嵌入向量\n",
    "    sparse_1d_embed_list = []\n",
    "    for f in sparse_feature_columns:\n",
    "        _input = sparse_input_dict[f.name]\n",
    "        _embed = Flatten()(embedding_layer_dict[f.name](_input))\n",
    "        sparse_1d_embed_list.append(_embed)\n",
    "    \n",
    "    sparse_logits_output = Add()(sparse_1d_embed_list)\n",
    "    linear_logits = Add()([dense_logits_output, sparse_logits_output])\n",
    "    return linear_logits\n",
    "    \n",
    "def get_fm_logits(sparse_input_dict, kd_embedding_layer_dict, feature_columns):\n",
    "    \"\"\" 取出input所对应的嵌入向量拼接在一起，计算和的平和-平方的和 \"\"\"\n",
    "    sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeature), feature_columns))\n",
    "    sparse_kd_embed_list = []\n",
    "    for f in sparse_feature_columns:\n",
    "        _input = sparse_input_dict[f.name]\n",
    "        _embed = kd_embedding_layer_dict[f.name](_input)\n",
    "        sparse_kd_embed_list.append(_embed)\n",
    "    \n",
    "    concat_sparse_kd_embed_list = Concatenate(axis=1)(sparse_kd_embed_list)\n",
    "    fm_logits = FM_Layer()(concat_sparse_kd_embed_list)\n",
    "    return fm_logits\n",
    "    \n",
    "def get_dnn_logits(sparse_input_dict, kd_embedding_layer_dict, feature_columns):\n",
    "    sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeature), feature_columns))\n",
    "    sparse_kd_embed_list = []\n",
    "    for f in sparse_feature_columns:\n",
    "        _input = sparse_input_dict[f.name]\n",
    "        _embed = kd_embedding_layer_dict[f.name](_input)\n",
    "        flatten_embed = Flatten()(_embed)\n",
    "        sparse_kd_embed_list.append(flatten_embed)\n",
    "    concat_sparse_kd_embed = Concatenate(axis=1)(sparse_kd_embed_list)\n",
    "    #print('concat_sparse_kd_embed.shape: ', concat_sparse_kd_embed.shape)\n",
    "    \n",
    "    # DNN\n",
    "    dnn_out = Dropout(0.2)(Activation(activation='relu')(BatchNormalization()(Dense(4)(concat_sparse_kd_embed))))\n",
    "    #dnn_out = Dropout(0.2)(Activation(activation='relu')(BatchNormalization()(Dense(4)(dnn_out))))\n",
    "    dnn_logits = Dense(1)(dnn_out)\n",
    "    return dnn_logits\n",
    "\n",
    "def DeepFM(feature_columns):\n",
    "    \"\"\" Instantiates FM architecture\n",
    "    :param feature_columns \n",
    "    :return: A kears model instance\n",
    "    \"\"\"\n",
    "    dense_input_dict, sparse_input_dict, _ = build_input_layers(feature_columns)\n",
    "    input_list = list(dense_input_dict.values()) + list(sparse_input_dict.values())\n",
    "    \n",
    "    # linear w * x + b\n",
    "    linear_logits = get_linear_logits(dense_input_dict, sparse_input_dict, feature_columns)\n",
    "    \n",
    "    # fm 0.5 * [sum(vixi)**2 - sum(vixi*vixi)\n",
    "    kd_embedding_layer_dict = build_embedding_layers(feature_columns, is_linear=False)\n",
    "    fm_logits = get_fm_logits(sparse_input_dict, kd_embedding_layer_dict, feature_columns)\n",
    "    \n",
    "    # dnn next_a = σ(w * a + b)\n",
    "    dnn_logits = get_dnn_logits(sparse_input_dict, kd_embedding_layer_dict, feature_columns)\n",
    "    \n",
    "    output_logits = Add()([linear_logits, fm_logits, dnn_logits])\n",
    "    output_layer = Activation(\"sigmoid\")(output_logits)\n",
    "    model = Model(input_list, output_layer)\n",
    "    return model\n",
    "\n",
    "model = DeepFM(feature_columns)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33c8bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_mask(seq):\n",
    "    \"\"\" 为避免padding对句子语义产生影响，需要将padding位mask掉，该函数作为所有为0的位置mask后为1。 \"\"\"\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32) # (None, seq_len)\n",
    "    # 扩充维度用于attention矩阵\n",
    "    return seq[:, np.newaxis, np.newaxis, :] # (None, 1, 1, seq_len)\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\" Q、K向量做点积操作，然后通过softmax归一化得到权重，对V加权求和得到atention向量 \"\"\"\n",
    "    # q (None, head_num, seq_len, depth) (None, 2, 50, 2)\n",
    "    # mask (None, 1, 1, seq_len) (None, 1, 1, 50)\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True) # (None, 2, 50, 50)\n",
    "    #print('matmul_qk.shape: ', matmul_qk.shape) \n",
    "    \n",
    "    k_dim = tf.cast(tf.shape(k)[-1], tf.float32) # 分头后emb_size\n",
    "    #print('k_dim: ', k_dim)\n",
    "    scaled_attention = matmul_qk / tf.math.sqrt(k_dim) # (None, 2, 50, 50)\n",
    "    if mask is not None:\n",
    "        scaled_attention += (mask * -1e9)\n",
    "    #print('scaled_attention.shape: ', scaled_attention.shape)\n",
    "    attention_weight = tf.nn.softmax(scaled_attention, axis=-1) # (None, 2, 50, 50)\n",
    "    #print('attention_weight.shape: ', attention_weight.shape)\n",
    "    \n",
    "    output = tf.matmul(attention_weight, v) # (None, 2, 50, 50) * (None, 2, 50, 2) \n",
    "    #print('output.shape: ', output.shape) \n",
    "    #print('******')\n",
    "    return output # (None, 2, 50, 2)\n",
    "    \n",
    "class MultiHeadAttention(Layer):\n",
    "    \"\"\" 多头attention\n",
    "        q, k, v input => Dense => 分头 => scaled_dot_product_attention => concat, dense => attention embedding \n",
    "    \"\"\"\n",
    "    def __init__(self, emb_size, head_num):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.head_num = head_num\n",
    "        \n",
    "        # emb的维度可以分为多个部分\n",
    "        assert emb_size % head_num == 0\n",
    "        \n",
    "        self.depth = emb_size // head_num\n",
    "        self.wq = Dense(emb_size)\n",
    "        self.wk = Dense(emb_size)\n",
    "        self.wv = Dense(emb_size)\n",
    "        self.dense = Dense(emb_size)\n",
    "        \n",
    "    def split_head(self, x, batch_size):\n",
    "        \"\"\" 分头操作，将嵌入矩阵的维度分成多个部分 \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.head_num, self.depth)) # (None, 50, 4) => (None, 50, 2, 2) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) # (None, 2, 50, 2) => (None, head_num, seq_len, depth)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        q, k, v, mask = inputs\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        # 分头前的前向网络\n",
    "        q = self.wq(q) # (None, 50, 4)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        \n",
    "        # 分头操作, 将嵌入矩阵的最后一个维度emb，拆分为两个维度，head_num和depth(emb)\n",
    "        q = self.split_head(q, batch_size) # (None, head_num, seq_len, depth)\n",
    "        k = self.split_head(k, batch_size)\n",
    "        v = self.split_head(v, batch_size)\n",
    "    \n",
    "        # 缩放点积注意力层(PS:这里可以切换为其他注意力方式)\n",
    "        scaled_attention = scaled_dot_product_attention(q, k, v, mask) # (None, head_num, seq_len, depth)\n",
    "        #print('scaled_attention.shape: ', scaled_attention.shape)\n",
    "        \n",
    "        # 合并多头的维度\n",
    "        scaled_attention = tf.transpose(scaled_attention, [0, 2, 1, 3]) # (None, seq_len, head_num, depth)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.emb_size)) # (None, max_seq_len, emb_size)\n",
    "        \n",
    "        # 全连接\n",
    "        output = self.dense(concat_attention)\n",
    "        #print('output.shape: ', output.shape) # (None, 50, 4)\n",
    "        return output # (None, 50, 4)\n",
    "        \n",
    "def feed_forward_network(emb_size, middle_unit_num):\n",
    "    return Sequential([\n",
    "        Dense(middle_unit_num, activation='relu'),\n",
    "        Dense(emb_size, activation='relu')\n",
    "    ])\n",
    "         \n",
    "class LayerNormalization(Layer):\n",
    "    \"\"\" 层标准化 \"\"\"\n",
    "    def __init__(self, epsilon=1e-6):\n",
    "        super(LayerNormalization, self).__init__()\n",
    "        self.eps = epsilon\n",
    "    def build(self, input_shape):\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "        self.gamma = self.add_weight(name='gamma',\n",
    "                                     shape=input_shape[-1:],\n",
    "                                     initializer=tf.ones_initializer(),\n",
    "                                     trainable=True)\n",
    "        self.beta = self.add_weight(name='beta',\n",
    "                                    shape=input_shape[-1:],\n",
    "                                    initializer=tf.zeros_initializer(),\n",
    "                                    trainable=True)\n",
    "    def call(self, x):\n",
    "        mean = tf.keras.backend.mean(x, axis=-1, keepdims=True)\n",
    "        std = tf.keras.backend.std(x, axis=-1, keepdims=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "\n",
    "class EncoderLayer(Layer):\n",
    "    \"\"\" Encoder层，输入序列所对应的嵌入矩阵 \"\"\"\n",
    "    def __init__(self,\n",
    "                 emb_size, \n",
    "                 head_num, \n",
    "                 middle_unit_num,\n",
    "                 max_seq_len,\n",
    "                 dropout_rate=0.2, dropout_training=False):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(emb_size, head_num)\n",
    "        self.ffn = feed_forward_network(emb_size, middle_unit_num)\n",
    "        \n",
    "        self.norm1 = LayerNormalization()\n",
    "        self.norm2 = LayerNormalization()\n",
    "        \n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "        \n",
    "        self.dropout_training = dropout_training\n",
    "        \n",
    "    def call(self, inputs, mask):\n",
    "        # inputs Embedding (None, 50, 4)\n",
    "        # 多头Attention => add & norm => feed forward => add & norm, 返回包含上下文信息的嵌入矩阵\n",
    "        # 多层注意力网络\n",
    "        mha_out = self.mha([inputs, inputs, inputs, mask]) # 嵌入矩阵(None, 50, 4)\n",
    "        #print('mha_out.shape: ', mha_out.shape) #  (None, 50, 4)\n",
    "        # add & norm\n",
    "        mha_out = self.dropout1(mha_out, training=self.dropout_training)\n",
    "        #print('mha_out.shape: ', mha_out.shape)\n",
    "        norm_mha_out = self.norm1(inputs + mha_out) # (None, 50, 4)\n",
    "        #print('norm_mha_out.shape: ', norm_mha_out.shape)\n",
    "        # 前馈神经网络\n",
    "        ffn_out = self.ffn(norm_mha_out) # (None, 50, 4)\n",
    "        #print('ffn_out.shape: ', ffn_out.shape)\n",
    "        ffn_out = self.dropout2(ffn_out, training=self.dropout_training)\n",
    "        norm_ffn_out = self.norm2(norm_mha_out + ffn_out)\n",
    "        #print('norm_ffn_out.shape: ', norm_ffn_out.shape)\n",
    "        return norm_ffn_out # (None, 50, 4)\n",
    "\n",
    "class PositionalEncoding(Layer):\n",
    "    def __init__(self, sequence_len=None, emb_size=None):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.sequence_len = sequence_len\n",
    "        self.emb_size = emb_size\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        if self.emb_size == None:\n",
    "            self.emb_size = int(inputs.shape[-1])\n",
    "        \n",
    "        position_emb = np.array([\n",
    "            [pos/np.power(10000, 2.0*i/self.emb_size) for i in range(self.emb_size)] for pos in range(self.sequence_len)\n",
    "        ])\n",
    "        \n",
    "        position_emb[:, 0::2] = np.sin(position_emb[:, 0::2]) # 2i\n",
    "        position_emb[:, 1::2] = np.cos(position_emb[:, 1::2]) # 2i+1\n",
    "        \n",
    "        #print('position_emb.shape: ', position_emb.shape)\n",
    "        #print('inputs.shape: ', inputs.shape)\n",
    "        \n",
    "        position_emb = tf.cast(position_emb, dtype=tf.float32) # 位置编码和原始embedding相加 (50, 4)\n",
    "        #print('position_emb+inputs.shape: ', (position_emb + inputs).shape)\n",
    "        return position_emb + inputs # (None, 50, 4)\n",
    "     \n",
    "class Encoder(Layer):\n",
    "    def __init__(self, \n",
    "                 layer_num, # Encoder层数\n",
    "                 emb_size, # 嵌入向量维度\n",
    "                 head_num, # 多头attention的头数\n",
    "                 middle_unit_num, # 前馈神经网络隐层神经元数量\n",
    "                 max_seq_len, # 序列长度\n",
    "                 dropout_rate=0.2, \n",
    "                 dropout_training=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layer_num = layer_num\n",
    "        self.pos_embedding = PositionalEncoding(sequence_len=max_seq_len, emb_size=emb_size)\n",
    "        self.encoder_layers = [EncoderLayer(emb_size, \n",
    "                                            head_num, \n",
    "                                            middle_unit_num,\n",
    "                                            max_seq_len,\n",
    "                                            dropout_rate=dropout_rate, \n",
    "                                            dropout_training=dropout_training) for _ in range(layer_num)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        emb, mask = inputs # (None, 50, 4) (None, 1, 1, 50)\n",
    "        \n",
    "        emb = self.pos_embedding(emb)\n",
    "        \n",
    "        for i in range(self.layer_num):\n",
    "            emb = self.encoder_layers[i](emb, mask)        \n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4389d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_lookup(columns, input_dict, embedding_layer_dict, flatten=False):\n",
    "    \"\"\" 根据feature_columns或column_names查表，得到对应embedding向量列表 \"\"\"\n",
    "    embedding_list = []\n",
    "    for f in columns:\n",
    "        if type(f) == str:\n",
    "            column_name = f\n",
    "        else:\n",
    "            column_name = f.name\n",
    "        _input = input_dict[column_name]\n",
    "        _embed = embedding_layer_dict[column_name]\n",
    "        embed_layer = _embed(_input)\n",
    "        if flatten:\n",
    "            embed_layer = Flatten()(embed_layer)\n",
    "        embedding_list.append(embed_layer)\n",
    "    return embedding_list\n",
    "\n",
    "def get_dnn_transformer_logits(sparse_input_dict,\n",
    "                               varlen_sparse_input_dict,\n",
    "                               kd_embedding_layer_dict, \n",
    "                               feature_columns,\n",
    "                               behavior_column_names, \n",
    "                               behavior_seq_column_names):\n",
    "    \"\"\" DNN侧: concate(Attention pooling embedding + flatten embedding) =》DNN \"\"\"\n",
    "    sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeature), feature_columns))\n",
    "    sparse_kd_embed_list = []\n",
    "    for f in sparse_feature_columns:\n",
    "        _input = sparse_input_dict[f.name]\n",
    "        _embed = kd_embedding_layer_dict[f.name](_input)\n",
    "        flatten_embed = Flatten()(_embed)\n",
    "        sparse_kd_embed_list.append(flatten_embed)\n",
    "    # attention pooling\n",
    "    # 当前物品的embedding列表\n",
    "    query_embed_list = embedding_lookup(behavior_column_names, sparse_input_dict, kd_embedding_layer_dict, flatten=False) # [(None, 1, 4)]\n",
    "    # 当前行为序列的embedding列表\n",
    "    keys_embed_list = embedding_lookup(behavior_seq_column_names, varlen_sparse_input_dict, kd_embedding_layer_dict, flatten=False) # [(None, 50, 4)]\n",
    "    \n",
    "    # Transformer\n",
    "    layer_num = 3\n",
    "    emb_size = 4 \n",
    "    head_num = 2\n",
    "    middle_unit_num = 8\n",
    "    max_seq_len = 50 # 序列长度\n",
    "    \n",
    "    seq_embed_list = []\n",
    "    for i in range(len(query_embed_list)):\n",
    "        padding_mask_list = padding_mask(varlen_sparse_input_dict[behavior_seq_column_names[i]])\n",
    "        print('padding_mask_list: ', padding_mask_list)\n",
    "        encoder_out = Encoder(layer_num,\n",
    "                              emb_size,\n",
    "                              head_num,\n",
    "                              middle_unit_num, \n",
    "                              max_seq_len)([keys_embed_list[i], padding_mask_list]) # (None, 50, 4)\n",
    "        \n",
    "        flatten_embed = Flatten()(encoder_out)\n",
    "        seq_embed_list.append(flatten_embed)\n",
    "    \n",
    "    # Attention pooling\n",
    "#     seq_embed_list = []\n",
    "#     # 使用注意力机制将历史行为序列对应的embedding进行池化(这里可能有多个ID及ID对应的历史序列，eg：点击物品与点击行为序列，搜索物品和搜索行为序列)\n",
    "#     for i in range(len(query_embed_list)):\n",
    "#         seq_embed = AttentionPoolingLayer()([query_embed_list[i], keys_embed_list[i]])\n",
    "#         seq_embed_list.append(seq_embed)\n",
    "    print('sparse_kd_embed_list.shape: ', sparse_kd_embed_list[0].shape)\n",
    "    print('seq_embed_list.shape: ', seq_embed_list[0].shape)\n",
    "    concat_sparse_kd_embed = Concatenate(axis=1)(sparse_kd_embed_list+seq_embed_list)\n",
    "    #print('concat_sparse_kd_embed.shape: ', concat_sparse_kd_embed.shape)\n",
    "    \n",
    "    # DNN\n",
    "    dnn_out = Dropout(0.2)(Activation(activation='relu')(BatchNormalization()(Dense(4)(concat_sparse_kd_embed))))\n",
    "    #dnn_out = Dropout(0.2)(Activation(activation='relu')(BatchNormalization()(Dense(4)(dnn_out))))\n",
    "    dnn_logits = Dense(1)(dnn_out)\n",
    "    return dnn_logits\n",
    "\n",
    "def DeepFM_Transformer(feature_columns, behavior_column_names, behavior_seq_column_names):\n",
    "    dense_input_dict, sparse_input_dict, varlen_sparse_input_dict = build_input_layers(feature_columns)\n",
    "    input_list = list(dense_input_dict.values()) + list(sparse_input_dict.values()) + list(varlen_sparse_input_dict.values())\n",
    "    \n",
    "    # linear w * x + b\n",
    "    linear_logits = get_linear_logits(dense_input_dict, sparse_input_dict, feature_columns)\n",
    "    \n",
    "    # fm 0.5 * [sum(vixi)**2 - sum(vixi*vixi)\n",
    "    kd_embedding_layer_dict = build_embedding_layers(feature_columns, is_linear=False)\n",
    "    fm_logits = get_fm_logits(sparse_input_dict, kd_embedding_layer_dict, feature_columns)\n",
    "    \n",
    "    # transformer dnn next_a = σ(w * a + b)\n",
    "    dnn_logits = get_dnn_transformer_logits(sparse_input_dict,\n",
    "                                            varlen_sparse_input_dict,\n",
    "                                            kd_embedding_layer_dict, \n",
    "                                            feature_columns,\n",
    "                                            behavior_column_names, \n",
    "                                            behavior_seq_column_names)\n",
    "    \n",
    "    output_logits = Add()([linear_logits, fm_logits, dnn_logits])\n",
    "    output_layer = Activation(\"sigmoid\")(output_logits)\n",
    "    model = Model(input_list, output_layer)\n",
    "    return model\n",
    "\n",
    "behavior_column_names, behavior_seq_column_names = ['item_id'], ['hist_item_id']\n",
    "model = DeepFM_Transformer(feature_columns, behavior_column_names, behavior_seq_column_names)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae59bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_input = {\n",
    "    'user_id': np.array(train_df['user_id']),\n",
    "    'gender': np.array(train_df['gender']),\n",
    "    'age': np.array(train_df['age']),\n",
    "    'item_id': np.array(train_df['item_id']),\n",
    "    'item_cate_id': np.array(train_df['item_cate_id']),\n",
    "    'hist_item_id': np.array([[int(i) for i in s.split(',')] for s in train_df['hist_item_id']]),\n",
    "    'hist_len': np.array(train_df['hist_len']),\n",
    "}\n",
    "test_input = {\n",
    "    'user_id': np.array(test_df['user_id']),\n",
    "    'gender': np.array(test_df['gender']),\n",
    "    'age': np.array(test_df['age']),\n",
    "    'item_id': np.array(test_df['item_id']),\n",
    "    'item_cate_id': np.array(test_df['item_cate_id']),\n",
    "    'hist_item_id': np.array([[int(i) for i in s.split(',')] for s in test_df['hist_item_id']]),\n",
    "    'hist_len': np.array(test_df['hist_len']),\n",
    "}\n",
    "\n",
    "# 模型训练\n",
    "my_callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, verbose=2, mode='auto')\n",
    "]\n",
    "model.compile('adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[\"binary_crossentropy\", tf.keras.metrics.AUC(name='auc')])\n",
    "model.fit(train_input,\n",
    "          train_df['label'].values,\n",
    "          batch_size=1024,\n",
    "          epochs=100,\n",
    "          validation_split=0.2,\n",
    "          callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a89839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型预测与评估\n",
    "result = model.predict(test_input)\n",
    "model_metric(np.array([i[0] for i in result]), test_df['label'].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
