{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55ec282b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 物品画像\n",
    "\n",
    "采用TFIDF、TextRank提取关键词，并将其共现的词作为物品画像标签；对TFIDF关键词进行平均embedding，得到向量结果通过最近邻搜索可以进行相关推荐。\n",
    "\n",
    "##### TFIDF\n",
    "\n",
    "TFIDF(词频逆文档频率)可以衡量词在文档中的重要程度,核心思想是当一个词在当前文档中出现频率很高，但当前词在所有文档中出现频率很低，说明这个词对于这篇文档很重要。\n",
    "\n",
    "$tfidf=TF(词频) * IDF(逆文档频率)$\n",
    "\n",
    "TF(词频)=$\\frac{词在当前文档中出现的次数}{文档的总词数}$，即当前词在当前文档中出现的频率。\n",
    "\n",
    "IDF(逆文档频率)=log$\\frac{文件总数}{包含词语的文件数目}$。\n",
    "\n",
    "##### TextRank\n",
    "\n",
    "TextRank利用文档内部的词语之间的共线关系来抽取关键词。\n",
    "\n",
    "$S(V_{i})=1-d+d*\\sum_{j∈In(V_{i})}\\frac{S(V_{j})}{|Out(V_{j})|}$\n",
    "\n",
    "d的作用是使计算结果更加平滑，公式最后一部分表示当前词i的权重是所有与i相邻的词的权重和，公式的分母部分是对类似“虽然、的”这类常见（与其他词共现多）且不重要的词进行惩罚。\n",
    "\n",
    "##### Doc2vec\n",
    "\n",
    "加载训练好的Word2vec模型，然后对TFIDF关键词进行平均embedding。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "102ac8bf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 6 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\SoftwareInstall\\Anaconda\\envs\\py36\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import jieba.analyse\n",
    "import jieba.posseg as posseg\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "def timmer(func):\n",
    "    \"\"\" 装饰器，监控运行时间 \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        before_time = time.time()\n",
    "        f = func(*args, **kwargs)\n",
    "        print(\"--> RUN TIME: <%s> : %s\" % (func.__name__, time.time() - before_time))\n",
    "        return f\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6be878",
   "metadata": {},
   "source": [
    "##### 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6015b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  (1000, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>source</th>\n",
       "      <th>head_img</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>keyword</th>\n",
       "      <th>tag</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021/4/29 20:02:38</td>\n",
       "      <td>对话张恩华：武磊的言行代表着中国足球 希望更多人留洋</td>\n",
       "      <td>tencent</td>\n",
       "      <td>https://inews.gtimg.com/newsapp_ls/0/905250541...</td>\n",
       "      <td>2021-04-29 18:36:10</td>\n",
       "      <td>http://new.qq.com/cmsn/20190522/20190522003669...</td>\n",
       "      <td>运动</td>\n",
       "      <td>张恩华,武磊,国足,腾讯体育,博斯克,中超</td>\n",
       "      <td>张恩华;武磊;国足;腾讯体育;博斯克;中超</td>\n",
       "      <td>对话张恩华：武磊的言行代表着中国足球 希望更多人留洋</td>\n",
       "      <td>腾讯体育5月22日马德里讯（文/吴昊宇）距离万众瞩目的欧冠决战还有10天左右，马德里的万达大...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021/6/5 05:02:23</td>\n",
       "      <td>为了生个优质宝宝，孕前、孕期检查很重要，这些项目必须查！</td>\n",
       "      <td>tencent</td>\n",
       "      <td>https://inews.gtimg.com/newsapp_ls/0/136089782...</td>\n",
       "      <td>2021-06-03 16:44:03</td>\n",
       "      <td>https://new.qq.com/omn/20180827/20180827A0TXHJ...</td>\n",
       "      <td>育儿</td>\n",
       "      <td>NaN</td>\n",
       "      <td>孕期检查;宝宝;胎儿;人流;怀孕;顺产;畸形儿;婴儿</td>\n",
       "      <td>随着环境的恶劣、食品安全问题日益严重，早产儿、畸形儿等各种病态患儿越来越多，孕前检查显得尤为...</td>\n",
       "      <td>随着环境的恶劣、食品安全问题日益严重，早产儿、畸形儿等各种病态患儿越来越多，孕前检查显得尤为...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp                         title   source  \\\n",
       "0  2021/4/29 20:02:38    对话张恩华：武磊的言行代表着中国足球 希望更多人留洋  tencent   \n",
       "1   2021/6/5 05:02:23  为了生个优质宝宝，孕前、孕期检查很重要，这些项目必须查！  tencent   \n",
       "\n",
       "                                            head_img         publish_time  \\\n",
       "0  https://inews.gtimg.com/newsapp_ls/0/905250541...  2021-04-29 18:36:10   \n",
       "1  https://inews.gtimg.com/newsapp_ls/0/136089782...  2021-06-03 16:44:03   \n",
       "\n",
       "                                                 url category  \\\n",
       "0  http://new.qq.com/cmsn/20190522/20190522003669...       运动   \n",
       "1  https://new.qq.com/omn/20180827/20180827A0TXHJ...       育儿   \n",
       "\n",
       "                 keyword                         tag  \\\n",
       "0  张恩华,武磊,国足,腾讯体育,博斯克,中超       张恩华;武磊;国足;腾讯体育;博斯克;中超   \n",
       "1                    NaN  孕期检查;宝宝;胎儿;人流;怀孕;顺产;畸形儿;婴儿   \n",
       "\n",
       "                                         description  \\\n",
       "0                         对话张恩华：武磊的言行代表着中国足球 希望更多人留洋   \n",
       "1  随着环境的恶劣、食品安全问题日益严重，早产儿、畸形儿等各种病态患儿越来越多，孕前检查显得尤为...   \n",
       "\n",
       "                                             content  \n",
       "0  腾讯体育5月22日马德里讯（文/吴昊宇）距离万众瞩目的欧冠决战还有10天左右，马德里的万达大...  \n",
       "1  随着环境的恶劣、食品安全问题日益严重，早产儿、畸形儿等各种病态患儿越来越多，孕前检查显得尤为...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/news/news.csv')\n",
    "df = df.head(1000)\n",
    "print('shape: ', df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471daedc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 分词\n",
    "\n",
    "加载用户字典、停止词词典进行分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "555e045d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_stopwords(stopwords_path):\n",
    "    \"\"\" 得到stopwords列表 \"\"\"\n",
    "    stopwords = [\n",
    "        i.strip() for i in codecs.open(stopwords_path, encoding='utf-8').readlines()\n",
    "    ]\n",
    "    return stopwords\n",
    "\n",
    "def cut_sentence(sentence, stopwords):\n",
    "    \"\"\" 分词结果过滤，保留名词、英文和自定义词库中的词，以及长度大于2的词 \"\"\"\n",
    "    import jieba.posseg as posseg\n",
    "    seg_list = posseg.lcut(sentence)\n",
    "    seg_list = [i for i in seg_list if i.word not in stopwords]\n",
    "    filtered_words_list = []\n",
    "    \n",
    "    for seg in seg_list:\n",
    "        if len(seg.word) <= 1:\n",
    "            continue\n",
    "        elif seg.flag == \"eng\":\n",
    "            if len(seg.word) <= 2:\n",
    "                continue\n",
    "            else:\n",
    "                filtered_words_list.append(seg.word)\n",
    "        elif seg.flag.startswith(\n",
    "                \"n\") or seg.flag == \"x\" or seg.flag == \"v\" or seg.flag == \"j\" or seg.flag == \"s\" or seg.flag == \"t\":\n",
    "            filtered_words_list.append((seg.word, seg.flag))\n",
    "\n",
    "    return filtered_words_list\n",
    "\n",
    "@timmer\n",
    "def get_segments(df, stopwords):\n",
    "    \"\"\" 获取分词结果 \"\"\"\n",
    "    df['segments'] = df['content'].parallel_apply(cut_sentence, args=(stopwords,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17efca3a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> RUN TIME: <get_segments> : 23.699240684509277\n"
     ]
    }
   ],
   "source": [
    "# 通过重复title增加文本标题关键词权重\n",
    "def get_content(row):\n",
    "    return str(row.title)*3 + ' ' + str(row.description) + ' ' + str(row.content)\n",
    "df['content'] = df.parallel_apply(get_content, axis=1)\n",
    "\n",
    "# 加载用户字典， 获取停用词\n",
    "abspath = \"../data/news/\"\n",
    "user_dict_path = os.path.join(abspath, \"dictionary.txt\")\n",
    "jieba.load_userdict(user_dict_path)\n",
    "stopwords_path = os.path.join(abspath, \"stopwords.txt\")\n",
    "idf_path = os.path.join(abspath, \"idf.txt\")\n",
    "wv_model_path = os.path.join(abspath, \"wv_50features_5mincount_5window\")\n",
    "\n",
    "# 分词(保留词和词性)\n",
    "stopwords = get_stopwords(stopwords_path)\n",
    "get_segments(df, stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60800c0a",
   "metadata": {},
   "source": [
    "##### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36fcc28c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> RUN TIME: <get_tfidf> : 3.3137106895446777\n"
     ]
    }
   ],
   "source": [
    "@timmer\n",
    "def get_tfidf(docs, df, idf_path):\n",
    "    \"\"\" 根据语料训练tfidf模型 \"\"\"\n",
    "    tfidf = Tfidf(idf_path)\n",
    "    tfidf_result = []\n",
    "    for doc in docs:\n",
    "        keywords = tfidf.extract_keywords(doc)\n",
    "        tfidf_result.append(keywords)\n",
    "    df['tfidf'] = tfidf_result\n",
    "    \n",
    "class Tfidf:\n",
    "    def __init__(self, idf_file):\n",
    "        \"\"\"\n",
    "        读取词典idf值，并计算出平均idf\n",
    "        Args:\n",
    "            idf_file IDF文件\n",
    "        \"\"\"\n",
    "        self._idf = {}\n",
    "        self._idf_default = 0\n",
    "        with open(idf_file, 'r', encoding='utf-8') as idf:\n",
    "            for line in idf:\n",
    "                word = line.strip().split()\n",
    "                self._idf[word[0]] = float(word[1])\n",
    "                self._idf_default += float(word[1])\n",
    "        self._idf_default /= float(len(self._idf))\n",
    "\n",
    "    def extract_keywords(self, items, top=10):\n",
    "        \"\"\"\n",
    "        抽取关键词，关键词根据tf*idf排名\n",
    "        Args:\n",
    "            words 分好词后的文档列表\n",
    "            top 关键词个数\n",
    "        \"\"\"\n",
    "        keywords = {}\n",
    "        count = len(items)\n",
    "        for item in items:\n",
    "            word = item[0]\n",
    "            if word not in keywords:\n",
    "                keywords[word] = 0\n",
    "            keywords[word] += 1\n",
    "\n",
    "        for word in keywords:\n",
    "            idf = self._idf_default\n",
    "            if word in self._idf:\n",
    "                idf = self._idf[word]\n",
    "            keywords[word] = keywords[word] / count * idf\n",
    "\n",
    "        return sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:top]\n",
    "\n",
    "######## 根据全量数据统计idf文档(可选) START ########\n",
    "@timmer\n",
    "def save_idf_file(docs, idf_path):\n",
    "    \"\"\" 获取idf文档并保存到文件 \"\"\" \n",
    "    idf_dict = {}\n",
    "    for doc in docs:\n",
    "        for item in set(doc):\n",
    "            word = item[0]\n",
    "            if word not in idf_dict:\n",
    "                idf_dict[word] = 0.0\n",
    "            idf_dict[word] += 1.0\n",
    "    doc_count = len(docs)\n",
    "    for word in idf_dict:\n",
    "        idf_dict[word] = np.log1p(doc_count / idf_dict[word])\n",
    "        \n",
    "    f = open(idf_path, 'w', encoding='utf-8')\n",
    "    for word, idf in idf_dict.items():\n",
    "        line = word + ' ' + str(idf)\n",
    "        f.write(line)\n",
    "        f.write('\\n')\n",
    "    f.close()\n",
    "# 重新训练IDF文件时打开\n",
    "#save_idf_file(df.segments.values, idf_path)\n",
    "######## 根据全量数据统计idf文档 END ########\n",
    "\n",
    "get_tfidf(df.segments.values, df, idf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2852f05",
   "metadata": {},
   "source": [
    "##### TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62fa5455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> RUN TIME: <get_textrank> : 3.400704860687256\n"
     ]
    }
   ],
   "source": [
    "class TextRank(jieba.analyse.TextRank):\n",
    "    def __init__(self, window=20, word_min_len=2):\n",
    "        super(TextRank, self).__init__()  # 首先找到TextRank父类，然后把当前类对象转化为父类对象\n",
    "        self.span = window  # 窗口大小\n",
    "        self.word_min_len = word_min_len  # 单词的最小长度\n",
    "        self.pos_filter = frozenset(\n",
    "            ('n', 'x', 'eng', 'f', 's', 't', 'nr', 'ns', 'nt', \"nw\", \"nz\", \"PER\", \"LOC\", \"ORG\")\n",
    "        )\n",
    "    \n",
    "    def textrank(self, words_list, flags_list, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'), withFlag=False):\n",
    "        \"\"\"save_tfidf_textrank\n",
    "        Extract keywords from sentence using TextRank algorithm.\n",
    "        Parameter:\n",
    "            - topK: return how many top keywords. `None` for all possible words.\n",
    "            - withWeight: if True, return a list of (word, weight);\n",
    "                          if False, return a list of words.\n",
    "            - allowPOS: the allowed POS list eg. ['ns', 'n', 'vn', 'v'].\n",
    "                        if the POS of w is not in this list, it will be filtered.\n",
    "            - withFlag: if True, return a list of pair(word, weight) like posseg.cut\n",
    "                        if False, return a list of words\n",
    "        \"\"\"\n",
    "        class Wp:\n",
    "            def __init__(self, word, flag):\n",
    "                self.__word = word\n",
    "                self.__flag = flag\n",
    "\n",
    "            @property\n",
    "            def word(self):\n",
    "                return self.__word\n",
    "\n",
    "            @word.setter\n",
    "            def word(self, word):\n",
    "                self.__word = word\n",
    "\n",
    "            @property\n",
    "            def flag(self):\n",
    "                return self.__flag\n",
    "\n",
    "            @flag.setter\n",
    "            def flag(self, flag):\n",
    "                self.__flag = flag\n",
    "\n",
    "        class UndirectWeightedGraph:\n",
    "            d = 0.85\n",
    "\n",
    "            def __init__(self):\n",
    "                self.graph = defaultdict(list)\n",
    "\n",
    "            def addEdge(self, start, end, weight):\n",
    "                # use a tuple (start, end, weight) instead of a Edge object\n",
    "                self.graph[start].append((start, end, weight))\n",
    "                self.graph[end].append((end, start, weight))\n",
    "\n",
    "            def itervalues(self, d):\n",
    "                return iter(d.values())\n",
    "                \n",
    "            def rank(self):\n",
    "                ws = defaultdict(float)\n",
    "                outSum = defaultdict(float)\n",
    "\n",
    "                wsdef = 1.0 / (len(self.graph) or 1.0)\n",
    "                for n, out in self.graph.items():\n",
    "                    ws[n] = wsdef\n",
    "                    outSum[n] = sum((e[2] for e in out), 0.0)\n",
    "\n",
    "                # this line for build stable iteration\n",
    "                sorted_keys = sorted(self.graph.keys())\n",
    "                for x in range(10):  # 10 iters\n",
    "                    for n in sorted_keys:\n",
    "                        s = 0\n",
    "                        for e in self.graph[n]:\n",
    "                            s += e[2] / outSum[e[1]] * ws[e[1]]\n",
    "                        ws[n] = (1 - self.d) + self.d * s\n",
    "\n",
    "                (min_rank, max_rank) = (sys.float_info[0], sys.float_info[3])\n",
    "                for w in self.itervalues(ws):\n",
    "                    if w < min_rank:\n",
    "                        min_rank = w\n",
    "                    if w > max_rank:\n",
    "                        max_rank = w\n",
    "\n",
    "                for n, w in ws.items():\n",
    "                    # to unify the weights, don't *100.\n",
    "                    ws[n] = (w - min_rank / 10.0) / (max_rank - min_rank / 10.0)\n",
    "\n",
    "                return ws\n",
    "\n",
    "        self.pos_filt = frozenset(allowPOS)\n",
    "        g = UndirectWeightedGraph()\n",
    "        cm = defaultdict(int)\n",
    "\n",
    "        wp_list = []\n",
    "        for i in range(len(words_list)):\n",
    "            wp = Wp(words_list[i], flags_list[i])\n",
    "            wp_list.append(wp)\n",
    "\n",
    "        words = tuple(wp_list)\n",
    "        for i, wp in enumerate(words):\n",
    "            if self.pairfilter(wp):\n",
    "                for j in range(i + 1, i + self.span):\n",
    "                    if j >= len(words):\n",
    "                        break\n",
    "                    if not self.pairfilter(words[j]):\n",
    "                        continue\n",
    "                    if allowPOS and withFlag:\n",
    "                        cm[(wp, words[j])] += 1\n",
    "                    else:\n",
    "                        cm[(wp.word, words[j].word)] += 1\n",
    "        for terms, w in cm.items():\n",
    "            g.addEdge(terms[0], terms[1], w)\n",
    "        nodes_rank = g.rank()\n",
    "        if withWeight:\n",
    "            sorted_tags = sorted(nodes_rank.items(), key=itemgetter(1), reverse=True)\n",
    "        else:\n",
    "            sorted_tags = sorted(nodes_rank, key=nodes_rank.__getitem__, reverse=True)\n",
    "        if topK:\n",
    "            return sorted_tags[:topK]\n",
    "        else:\n",
    "            return sorted_tags\n",
    "    extract_tags = textrank\n",
    "\n",
    "@timmer\n",
    "def get_textrank(docs, df):\n",
    "    # TextRank过滤窗口大小为5，单词最小为2\n",
    "    textrank_model = TextRank(window=5, word_min_len=2)\n",
    "    # 允许词性：名词、未知数或符号、英文、人名、地名、机构名、新词、其他名词、连词\n",
    "    allow_pos = (\"n\", \"x\", \"eng\", \"nr\", \"ns\", \"nt\", \"nw\", \"nz\", \"c\")  \n",
    "    \n",
    "    textrank_result = []\n",
    "    for doc in docs:\n",
    "        word_list, flag_list = [], []\n",
    "        for item in doc:\n",
    "            word_list.append(item[0])\n",
    "            flag_list.append(item[1])\n",
    "        keywords = textrank_model.textrank(word_list, flag_list, topK=20, withWeight=True, allowPOS=allow_pos, withFlag=False)\n",
    "        textrank_result.append(keywords)\n",
    "    df['textrank'] = textrank_result\n",
    "    \n",
    "get_textrank(df.segments.values, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc90fd77",
   "metadata": {},
   "source": [
    "##### Doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1d6ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(object):\n",
    "    \"\"\" word2vec模型 \"\"\"\n",
    "    def __init__(self, size=128, window=5, \n",
    "                 min_count=5, workers=5, \n",
    "                 epochs=50, pretrained_model=None):\n",
    "        \"\"\"\n",
    "        训练词嵌入向量\n",
    "        Args:\n",
    "            size - 向量维度\n",
    "            window - 窗口长度\n",
    "            min_count - 最小词频\n",
    "            workers - 并行化\n",
    "            epochs - 迭代次数\n",
    "            pretrained_model - 预训练模型\n",
    "        \"\"\"\n",
    "        self._model = None\n",
    "        self._size = size\n",
    "        self._window = window\n",
    "        self._min_count = min_count\n",
    "        self._workers = workers\n",
    "        self._epochs = epochs\n",
    "        if pretrained_model:\n",
    "            self._model = Word2Vec.load(pretrained_model)\n",
    "            \n",
    "    @timmer\n",
    "    def train(self, sentences=[]):\n",
    "        if self._model:\n",
    "            self._model.train(sentences,\n",
    "                             total_examples = len(sentences),\n",
    "                             epochs = self._epochs)\n",
    "        else:\n",
    "            self._model = Word2Vec(sentences,\n",
    "                                   vector_size=self._size,\n",
    "                                   window=self._window,\n",
    "                                   min_count=self._min_count,\n",
    "                                   workers=self._workers)\n",
    "    def save(self, model_path = None):\n",
    "        self._model.save(model_path)\n",
    "    \n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87cf505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新训练word2vec模型时打开\n",
    "######## 根据全量数据训练word2vec模型(可选) START ########\n",
    "# sentences = [[item[0] for item in item_list] for item_list in df.segments.values]\n",
    "# wv_emb = Embedding(size=50,\n",
    "#                      window=5, \n",
    "#                      min_count=5, \n",
    "#                      workers=5,\n",
    "#                      epochs=5, \n",
    "#                      pretrained_model=None)\n",
    "# wv_emb.train(sentences)\n",
    "# wv_emb.save(wv_model_path)\n",
    "######## 根据全量数据训练word2vec模型 END ########\n",
    "\n",
    "# 加载训练好的word2vec模型\n",
    "wv_emb = Embedding(size=50,\n",
    "                     window=5, \n",
    "                     min_count=5, \n",
    "                     workers=5,\n",
    "                     epochs=5, \n",
    "                     pretrained_model=wv_model_path)\n",
    "wv_model = wv_emb.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef13b4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not_in_dict_set cnt: 290\n",
      "--> RUN TIME: <get_doc2vec> : 0.04487752914428711\n"
     ]
    }
   ],
   "source": [
    "@timmer\n",
    "def get_doc2vec(docs, df):\n",
    "    \"\"\" 根据Tfidf关键词计算平均embedding \"\"\"\n",
    "    sentences = [list(set([item[0] for item in doc])) for doc in docs]\n",
    "    not_in_dict_set = set()\n",
    "    avg_wv_arr = []\n",
    "    for sentence in sentences:\n",
    "        wv_arr = np.array([0.0] * 50)\n",
    "        cnt = 0\n",
    "        for i in range(len(sentence)):\n",
    "            try:\n",
    "                wv_arr = wv_arr + wv_model.wv[sentence[i]]\n",
    "                cnt += 1\n",
    "            except Exception as e:\n",
    "                not_in_dict_set.add(sentence[i])\n",
    "        if cnt == 0:\n",
    "            avg_wv_arr.append(wv_arr)\n",
    "        else:\n",
    "            avg_wv_arr.append(wv_arr / cnt)\n",
    "\n",
    "    print('not_in_dict_set cnt: {}'.format(len(not_in_dict_set)))\n",
    "    df['doc2vec'] = avg_wv_arr\n",
    "\n",
    "get_doc2vec(df.tfidf.values, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff03806",
   "metadata": {},
   "source": [
    "##### TFIDF与TextRank共现词作为兴趣标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "154fdf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> RUN TIME: <get_tags> : 2.060326099395752\n"
     ]
    }
   ],
   "source": [
    "def process_tags(row):\n",
    "    \"\"\" TFIDF与TextRank共现词作为兴趣标签 \"\"\"\n",
    "    tags = list(set([item[0] for item in row.tfidf]) & set([item[0] for item in row.textrank]))\n",
    "    return tags\n",
    "\n",
    "@timmer\n",
    "def get_tags(df):\n",
    "    df['tags'] = df.parallel_apply(process_tags, axis=1)\n",
    "\n",
    "get_tags(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab8e73a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>source</th>\n",
       "      <th>head_img</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>keyword</th>\n",
       "      <th>tag</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "      <th>segments</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>textrank</th>\n",
       "      <th>doc2vec</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021/4/29 20:02:38</td>\n",
       "      <td>对话张恩华：武磊的言行代表着中国足球 希望更多人留洋</td>\n",
       "      <td>tencent</td>\n",
       "      <td>https://inews.gtimg.com/newsapp_ls/0/905250541...</td>\n",
       "      <td>2021-04-29 18:36:10</td>\n",
       "      <td>http://new.qq.com/cmsn/20190522/20190522003669...</td>\n",
       "      <td>运动</td>\n",
       "      <td>张恩华,武磊,国足,腾讯体育,博斯克,中超</td>\n",
       "      <td>张恩华;武磊;国足;腾讯体育;博斯克;中超</td>\n",
       "      <td>对话张恩华：武磊的言行代表着中国足球 希望更多人留洋</td>\n",
       "      <td>对话张恩华：武磊的言行代表着中国足球 希望更多人留洋对话张恩华：武磊的言行代表着中国足球 希...</td>\n",
       "      <td>[(对话, n), (张恩华, nr), (武磊, nr), (言行, n), (代表, n...</td>\n",
       "      <td>[(张恩华, 0.3434719541875263), (足球, 0.18942164938...</td>\n",
       "      <td>[(足球, 1.0), (张恩华, 0.9718222586235503), (中国, 0....</td>\n",
       "      <td>[1.1075875863432885, 0.7062447622418404, -1.58...</td>\n",
       "      <td>[言行, 球员, 博斯克, 代表, 足球, 对话, 留洋, 武磊, 张恩华, 青训]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp                       title   source  \\\n",
       "0  2021/4/29 20:02:38  对话张恩华：武磊的言行代表着中国足球 希望更多人留洋  tencent   \n",
       "\n",
       "                                            head_img         publish_time  \\\n",
       "0  https://inews.gtimg.com/newsapp_ls/0/905250541...  2021-04-29 18:36:10   \n",
       "\n",
       "                                                 url category  \\\n",
       "0  http://new.qq.com/cmsn/20190522/20190522003669...       运动   \n",
       "\n",
       "                 keyword                    tag                 description  \\\n",
       "0  张恩华,武磊,国足,腾讯体育,博斯克,中超  张恩华;武磊;国足;腾讯体育;博斯克;中超  对话张恩华：武磊的言行代表着中国足球 希望更多人留洋   \n",
       "\n",
       "                                             content  \\\n",
       "0  对话张恩华：武磊的言行代表着中国足球 希望更多人留洋对话张恩华：武磊的言行代表着中国足球 希...   \n",
       "\n",
       "                                            segments  \\\n",
       "0  [(对话, n), (张恩华, nr), (武磊, nr), (言行, n), (代表, n...   \n",
       "\n",
       "                                               tfidf  \\\n",
       "0  [(张恩华, 0.3434719541875263), (足球, 0.18942164938...   \n",
       "\n",
       "                                            textrank  \\\n",
       "0  [(足球, 1.0), (张恩华, 0.9718222586235503), (中国, 0....   \n",
       "\n",
       "                                             doc2vec  \\\n",
       "0  [1.1075875863432885, 0.7062447622418404, -1.58...   \n",
       "\n",
       "                                         tags  \n",
       "0  [言行, 球员, 博斯克, 代表, 足球, 对话, 留洋, 武磊, 张恩华, 青训]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74592693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对话张恩华：武磊的言行代表着中国足球 希望更多人留洋\n",
      "['张恩华', '足球', '青训', '留洋', '武磊', '言行', '博斯克', '代表', '对话', '球员'] ['足球', '张恩华', '中国', '青训', '代表', '留洋', '武磊', '球员', '对话', '言行', '腾讯', '感觉', '西甲', '深圳', '西班牙', '博斯克', '总监', '踢球', '东西', '直观'] ['言行', '球员', '博斯克', '代表', '足球', '对话', '留洋', '武磊', '张恩华', '青训'] \n",
      "\n",
      "为了生个优质宝宝，孕前、孕期检查很重要，这些项目必须查！\n",
      "['胎儿', '孕前', '畸形儿', '孕期', '宝宝', '感染', '确宝', '弓形虫', '优质', '死胎'] ['胎儿', '宝宝', '优质', '建议', '畸形儿', '问题', '死胎', '医生', '顺产', '项目', '孩子', '医院', '激素', '糖尿病', '脊柱', '疾病', '代表', '厚度', '唐氏儿', '病毒'] ['优质', '宝宝', '死胎', '畸形儿', '胎儿'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_tag_info(df, index_list=[0,1]):\n",
    "    \"\"\" 打印tags信息 \"\"\" \n",
    "    for idx in index_list:\n",
    "        row = df.iloc[idx]\n",
    "        _tfidf = [i[0] for i in row.tfidf]\n",
    "        _textrank =  [i[0] for i in row.textrank]\n",
    "        _tags =  [i for i in row.tags]\n",
    "        print(row.title)\n",
    "        print(_tfidf, _textrank, _tags, '\\n')\n",
    "\n",
    "print_tag_info(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4b6fe7",
   "metadata": {},
   "source": [
    "##### 优化方向：\n",
    "\n",
    "1.根据文章所属行业类型整理出一份行业专属词典。\n",
    "\n",
    "2.尝试采用更优秀的命名实体识别模型替换掉jieba分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7f2a74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
