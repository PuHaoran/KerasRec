{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef95651",
   "metadata": {},
   "source": [
    "## MMOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4d3e787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import  MinMaxScaler, LabelEncoder\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import classification_report, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43226df5",
   "metadata": {},
   "source": [
    "##### 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74b85e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape: (6708846, 12), test_df.shape: (609036, 12)\n"
     ]
    }
   ],
   "source": [
    "def get_wechat_data():\n",
    "    \"\"\" 读取wechat数据集 \"\"\"\n",
    "    train_path = '../data/wechat/train_df.pkl'\n",
    "    test_path = '../data/wechat/test_df.pkl'\n",
    "    encoder_dict_path = '../data/wechat/encoder_dict.pkl'\n",
    "    train_df = pd.read_pickle(train_path)\n",
    "    test_df = pd.read_pickle(test_path)\n",
    "    encoder_dict = joblib.load(encoder_dict_path)\n",
    "    return train_df, test_df, encoder_dict\n",
    "\n",
    "train_df, test_df, encoder_dict = get_wechat_data()\n",
    "train_df = train_df.sample(frac=1.0)\n",
    "data = pd.concat([train_df, test_df], axis=0)\n",
    "print('train_df.shape: {}, test_df.shape: {}'.format(train_df.shape, test_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b4808c",
   "metadata": {},
   "source": [
    "##### 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb3416de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseFeature(name='userid', vocabulary_size=17390, embedding_size=8),\n",
       " SparseFeature(name='feedid', vocabulary_size=16448, embedding_size=8),\n",
       " SparseFeature(name='authorid', vocabulary_size=6966, embedding_size=8),\n",
       " SparseFeature(name='bgm_song_id', vocabulary_size=5773, embedding_size=8),\n",
       " SparseFeature(name='bgm_singer_id', vocabulary_size=4573, embedding_size=8),\n",
       " DenseFeature(name='videoplayseconds', dimension=1),\n",
       " VarLenSparseFeature(name='manual_keyword_list', vocabulary_size=21576, embedding_size=8, maxlen=18),\n",
       " VarLenSparseFeature(name='manual_tag_list', vocabulary_size=349, embedding_size=8, maxlen=11)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = ['read_comment', 'like', 'click_avatar', 'forward']\n",
    "\n",
    "# 稠密特征、稀疏特征、变长稀疏特征\n",
    "dense_column_names = ['videoplayseconds']\n",
    "sparse_column_names = ['userid', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id']\n",
    "varlen_sparse_column_names = ['manual_keyword_list', 'manual_tag_list']\n",
    "\n",
    "SparseFeature = namedtuple('SparseFeature', ['name', 'vocabulary_size', 'embedding_size'])\n",
    "DenseFeature = namedtuple('DenseFeature', ['name', 'dimension'])\n",
    "VarLenSparseFeature = namedtuple('VarLenSparseFeature', ['name', 'vocabulary_size', 'embedding_size', 'maxlen'])\n",
    "\n",
    "varlen_sparse_column_maxlen_dict = {\n",
    "    'manual_keyword_list': 18,\n",
    "    'manual_tag_list': 11\n",
    "}\n",
    "\n",
    "feature_columns = [SparseFeature(f, vocabulary_size=data[f].nunique(), embedding_size=8) for f in sparse_column_names] + \\\n",
    "[DenseFeature(f, 1) for f in dense_column_names] + \\\n",
    "[VarLenSparseFeature(f, len(encoder_dict[f])+1, embedding_size=8, maxlen=varlen_sparse_column_maxlen_dict[f]) for f in varlen_sparse_column_names]\n",
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab6ad472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "manual_keyword_list (InputLayer [(None, 18)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "manual_tag_list (InputLayer)    [(None, 11)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "userid (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "feedid (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "authorid (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bgm_song_id (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bgm_singer_id (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "var_emb_manual_keyword_list (Em (None, 18, 8)        172616      manual_keyword_list[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "var_emb_manual_tag_list (Embedd (None, 11, 8)        2800        manual_tag_list[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "emb_userid (Embedding)          (None, 1, 8)         139128      userid[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "emb_feedid (Embedding)          (None, 1, 8)         131592      feedid[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "emb_authorid (Embedding)        (None, 1, 8)         55736       authorid[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "emb_bgm_song_id (Embedding)     (None, 1, 8)         46192       bgm_song_id[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "emb_bgm_singer_id (Embedding)   (None, 1, 8)         36592       bgm_singer_id[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, 18, 8)        0           var_emb_manual_keyword_list[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 11, 8)        0           var_emb_manual_tag_list[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8)            0           emb_userid[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8)            0           emb_feedid[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8)            0           emb_authorid[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 8)            0           emb_bgm_song_id[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 8)            0           emb_bgm_singer_id[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "mean_pooling_layer (MeanPooling (None, 8)            0           masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mean_pooling_layer_1 (MeanPooli (None, 8)            0           masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "videoplayseconds (InputLayer)   [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 40)           0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 16)           0           mean_pooling_layer[0][0]         \n",
      "                                                                 mean_pooling_layer_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 57)           0           videoplayseconds[0][0]           \n",
      "                                                                 concatenate[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          7424        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           8256        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "m_mo_e_layer (MMoELayer)        [(None, 32), (None,  9360        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "read_comment (Dense)            (None, 1)            33          m_mo_e_layer[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "like (Dense)                    (None, 1)            33          m_mo_e_layer[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "click_avatar (Dense)            (None, 1)            33          m_mo_e_layer[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "forward (Dense)                 (None, 1)            33          m_mo_e_layer[0][3]               \n",
      "==================================================================================================\n",
      "Total params: 609,828\n",
      "Trainable params: 609,828\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def model_metric(prob, label, thr=0.5):\n",
    "    \"\"\" 模型评估 \"\"\"\n",
    "    # AUC\n",
    "    fpr, tpr, threshold = metrics.roc_curve(label, prob)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    score = metrics.accuracy_score(label, prob > thr)\n",
    "    # LogLoss\n",
    "    logloss = log_loss(label, prob)\n",
    "    print('模型准确率:{}, AUC得分:{}, LogLoss:{}'.format(score, round(auc, 4), logloss))\n",
    "    print(classification_report(label, prob > thr, digits=2))\n",
    "    print('==========================================================')\n",
    "\n",
    "def build_input_layers(feature_columns):\n",
    "    \"\"\" 构建输入层 \"\"\"\n",
    "    dense_input_dict, sparse_input_dict, varlen_sparse_input_dict = {}, {}, {}\n",
    "    for f in feature_columns:\n",
    "        if isinstance(f, DenseFeature):\n",
    "            dense_input_dict[f.name] = Input(shape=(f.dimension, ), name=f.name)\n",
    "        elif isinstance(f, SparseFeature):\n",
    "            sparse_input_dict[f.name] = Input(shape=(1, ), name=f.name)\n",
    "        elif isinstance(f, VarLenSparseFeature):\n",
    "            varlen_sparse_input_dict[f.name] = Input(shape=(f.maxlen, ), name=f.name)\n",
    "    return dense_input_dict, sparse_input_dict, varlen_sparse_input_dict\n",
    "\n",
    "class MMoELayer(Layer):\n",
    "    def __init__(self, expert_dim, expert_num, task_num):\n",
    "        super(MMoELayer, self).__init__()\n",
    "        self.task_num = task_num\n",
    "        # expert，将拼接的向量（sparse embedding+dense feature）输入到多个relu为激活函数的全连接层，表示多个Expert网络。\n",
    "        self.expert_layers = [Dense(expert_dim, activation='relu') for i in range(expert_num)]\n",
    "        # gate，将拼接的向量输入到一个softmax层，输出expert_num个权重，表示Expert网络的权重。\n",
    "        # 考虑将一层fc升级为三层fc，gate网络参数增加，expert网络参数减少\n",
    "        self.gate_layers = [Dense(expert_num, activation='softmax', name='gate{}'.format(i)) for i in range(task_num)]\n",
    "        \n",
    "    def call(self, x):\n",
    "        # 专家网络的输出张量\n",
    "        expert_out = [expert_layer(x) for expert_layer in self.expert_layers] # [(None, expert_dim)...]\n",
    "        # [(None, 1, expert_dim)] => (None, expert_num, expert_dim)\n",
    "        expert_out = Concatenate(axis=1)([e[:, tf.newaxis, :] for e in expert_out])\n",
    "        \n",
    "        # 门控网络的输出权重\n",
    "        gate_out = [gate_layer(x) for gate_layer in self.gate_layers] # [(None, expert_num)...]\n",
    "        \n",
    "        # 一个任务一个门网络，每个任务的门网络*所有专家网络，得到每个任务所需要的浅层网络信息\n",
    "        tower_inputs = []\n",
    "        for i in range(self.task_num):\n",
    "            _gate_out = tf.expand_dims(gate_out[i], axis=-1) # (None, expert_num, 1)\n",
    "            # (None, expert_dim, expert_num) * (None, expert_num, 1) => (None, expert_dim, 1)\n",
    "            _tower_input = tf.matmul(expert_out, _gate_out, transpose_a=True) \n",
    "            tower_inputs.append(Flatten()(_tower_input)) # (None, expert_dim)\n",
    "        return tower_inputs\n",
    "    \n",
    "class MoELayer(Layer):\n",
    "    def __init__(self, expert_dim, expert_num, task_num):\n",
    "        super(MoELayer, self).__init__()\n",
    "        self.task_num = task_num\n",
    "        # expert，将拼接的向量（sparse embedding+dense feature）输入到多个relu为激活函数的全连接层，表示多个Expert网络。\n",
    "        self.expert_layers = [Dense(expert_dim, activation='relu') for i in range(expert_num)]\n",
    "        # gate，将拼接的向量输入到一个softmax层，输出expert_num个权重，表示Expert网络的权重。\n",
    "        self.gate_layer = Dense(expert_num, activation='softmax')\n",
    "        \n",
    "    def call(self, x):\n",
    "        # 专家网络的输出张量\n",
    "        expert_out = [expert_layer(x) for expert_layer in self.expert_layers] # [(None, expert_dim)...]\n",
    "        # [(None, 1, expert_dim)] => (None, expert_num, expert_dim)\n",
    "        expert_out = Concatenate(axis=1)([e[:, tf.newaxis, :] for e in expert_out])\n",
    "        \n",
    "        # 门控网络的输出权重\n",
    "        gate_out = self.gate_layer(x) # [(None, expert_num)...]\n",
    "        \n",
    "        # 一个任务一个门网络，每个任务的门网络*所有专家网络，得到每个任务所需要的浅层网络信息\n",
    "        tower_inputs = []\n",
    "        for i in range(self.task_num):\n",
    "            _gate_out = tf.expand_dims(gate_out, axis=-1) # (None, expert_num, 1)\n",
    "            # (None, expert_dim, expert_num) * (None, expert_num, 1) => (None, expert_dim, 1)\n",
    "            _tower_input = tf.matmul(expert_out, _gate_out, transpose_a=True) \n",
    "            tower_inputs.append(Flatten()(_tower_input)) # (None, expert_dim)\n",
    "        return tower_inputs\n",
    "        \n",
    "def concat_input_list(input_list):\n",
    "    \"\"\" 合并input列表 \"\"\"\n",
    "    _num = len(input_list)\n",
    "    if _num > 1:\n",
    "        return Concatenate(axis=1)(input_list)\n",
    "    elif len(input_list) == 1:\n",
    "        return input_list[0]\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "def build_embedding_layers(feature_columns):\n",
    "    \"\"\" 构建embedding层 \"\"\"\n",
    "    embedding_layer_dict = {}\n",
    "    for f in feature_columns:\n",
    "        if isinstance(f, SparseFeature):\n",
    "            embedding_layer_dict[f.name] = Embedding(f.vocabulary_size+1, f.embedding_size, name='emb_' + f.name)\n",
    "        elif isinstance(f, VarLenSparseFeature):\n",
    "            embedding_layer_dict[f.name] = Embedding(f.vocabulary_size+1, f.embedding_size, name='var_emb_' + f.name, mask_zero=True)\n",
    "    return embedding_layer_dict\n",
    "        \n",
    "def embedding_lookup(columns, input_dict, embedding_layer_dict, flatten=False):\n",
    "    \"\"\" 根据feature_columns或column_names查表，得到对应embedding向量列表 \"\"\"\n",
    "    embedding_list = []\n",
    "    for f in columns:\n",
    "        if type(f) == str:\n",
    "            column_name = f\n",
    "        else:\n",
    "            column_name = f.name\n",
    "        _input = input_dict[column_name]\n",
    "        _embed = embedding_layer_dict[column_name]\n",
    "        embed_layer = _embed(_input)\n",
    "        if flatten:\n",
    "            embed_layer = Flatten()(embed_layer)\n",
    "        embedding_list.append(embed_layer)\n",
    "    return embedding_list\n",
    "    \n",
    "class MeanPoolingLayer(Layer):\n",
    "    def __init__(self, axis, **kwargs):\n",
    "        super(MeanPoolingLayer, self).__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "        self.supports_masking = True\n",
    "    \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # need not to pass the mask to next layers\n",
    "        return None\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        if x is not None:\n",
    "            mask = K.cast(mask, K.floatx()) # (None, 18)\n",
    "            mask = K.repeat(mask, x.shape[-1]) # (None, 4, 18)\n",
    "            mask = tf.transpose(mask, [0, 2, 1]) # (None, 18, 4)\n",
    "            x = x * mask # # (None, 18, 4) * (None, 18, 4)\n",
    "            return K.sum(x, axis=self.axis) / K.sum(mask, axis=self.axis)\n",
    "        else:\n",
    "            return K.mean(x, axis=self.axis)\n",
    "    \n",
    "def MMoE(feature_columns, target, dnn_hidden_units=[64, 64], expert_dim=32, expert_num=4, task_num=4, is_MoE=False):\n",
    "    dense_input_dict, sparse_input_dict, varlen_sparse_input_dict = build_input_layers(feature_columns)\n",
    "    \n",
    "    # Input\n",
    "    input_list = list(dense_input_dict.values()) + list(sparse_input_dict.values()) + list(varlen_sparse_input_dict.values())\n",
    "    \n",
    "    # dense feature (input->concat)\n",
    "    concat_dense_input_list = concat_input_list(list(dense_input_dict.values()))\n",
    "    \n",
    "    # sparse feature (input->embed->concat)\n",
    "    embedding_layer_dict = build_embedding_layers(feature_columns)\n",
    "    sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeature), feature_columns))\n",
    "    flatten_sparse_embed_list = embedding_lookup(sparse_feature_columns, sparse_input_dict, embedding_layer_dict, flatten=True)\n",
    "    concat_flatten_sparse_embed_list = concat_input_list(flatten_sparse_embed_list)\n",
    "    \n",
    "    # seq embeddings (input->embed->pooling->concat)\n",
    "    varlen_sparse_feature_columns = list(filter(lambda x: isinstance(x, VarLenSparseFeature), feature_columns))\n",
    "    varlen_sparse_embed_list = []\n",
    "    for f in varlen_sparse_feature_columns:\n",
    "        _input = varlen_sparse_input_dict[f.name] #  (None, 18)\n",
    "        _embed = embedding_layer_dict[f.name]\n",
    "        embed_layer = _embed(_input) # (None, 18, 4)\n",
    "        mask = Masking()(embed_layer) # (None, 18, 4)\n",
    "        mean_pooling_embed = MeanPoolingLayer(axis=1)(mask) # (None, 4)\n",
    "        varlen_sparse_embed_list.append(mean_pooling_embed)\n",
    "    concat_varlen_sparse_embed_list = concat_input_list(varlen_sparse_embed_list)\n",
    "    \n",
    "    # concat dense feature + concat sparse embeddings + concat seq embeddings\n",
    "    dnn_input = Concatenate(axis=1)([concat_dense_input_list, concat_flatten_sparse_embed_list, concat_varlen_sparse_embed_list]) \n",
    "    \n",
    "    # DNN\n",
    "    for dnn in dnn_hidden_units:\n",
    "        dnn_input = Dropout(0.1)(Dense(dnn, activation='relu')(dnn_input))\n",
    "    \n",
    "    # MMoE网络层\n",
    "    if is_MoE:\n",
    "        tower_inputs = MoELayer(expert_dim, expert_num, task_num)(dnn_input)\n",
    "    else:\n",
    "        tower_inputs = MMoELayer(expert_dim, expert_num, task_num)(dnn_input)\n",
    "    \n",
    "    # 通过各个任务对应的门网络的加权求和，每个塔都得到自己任务所需要的专家信息。\n",
    "    outputs = [Dense(1, activation='sigmoid', name=target_name)(tower_input) for tower_input, target_name in zip(tower_inputs, target)]\n",
    "    model = Model(input_list, outputs)\n",
    "    return model\n",
    "\n",
    "model = MMoE(feature_columns,\n",
    "             target, \n",
    "             dnn_hidden_units=[128, 64],\n",
    "             expert_dim=32,\n",
    "             expert_num=4, \n",
    "             task_num=4,\n",
    "             is_MoE=False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "608b9f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5242/5242 [==============================] - 47s 9ms/step - loss: 0.2707 - read_comment_loss: 0.1010 - like_loss: 0.1010 - click_avatar_loss: 0.0425 - forward_loss: 0.0261 - read_comment_auc: 0.9083 - like_auc: 0.8063 - click_avatar_auc: 0.7272 - forward_auc: 0.6938 - val_loss: 0.2401 - val_read_comment_loss: 0.0917 - val_like_loss: 0.0912 - val_click_avatar_loss: 0.0364 - val_forward_loss: 0.0208 - val_read_comment_auc: 0.9331 - val_like_auc: 0.8530 - val_click_avatar_auc: 0.8250 - val_forward_auc: 0.7909\n",
      "Epoch 2/100\n",
      "5242/5242 [==============================] - 47s 9ms/step - loss: 0.2348 - read_comment_loss: 0.0898 - like_loss: 0.0895 - click_avatar_loss: 0.0355 - forward_loss: 0.0200 - read_comment_auc: 0.9353 - like_auc: 0.8647 - click_avatar_auc: 0.8452 - forward_auc: 0.8356 - val_loss: 0.2386 - val_read_comment_loss: 0.0910 - val_like_loss: 0.0907 - val_click_avatar_loss: 0.0361 - val_forward_loss: 0.0207 - val_read_comment_auc: 0.9322 - val_like_auc: 0.8561 - val_click_avatar_auc: 0.8192 - val_forward_auc: 0.7863\n",
      "Epoch 3/100\n",
      "5242/5242 [==============================] - 47s 9ms/step - loss: 0.2292 - read_comment_loss: 0.0883 - like_loss: 0.0879 - click_avatar_loss: 0.0341 - forward_loss: 0.0189 - read_comment_auc: 0.9391 - like_auc: 0.8730 - click_avatar_auc: 0.8653 - forward_auc: 0.8675 - val_loss: 0.2372 - val_read_comment_loss: 0.0907 - val_like_loss: 0.0903 - val_click_avatar_loss: 0.0360 - val_forward_loss: 0.0202 - val_read_comment_auc: 0.9356 - val_like_auc: 0.8585 - val_click_avatar_auc: 0.8211 - val_forward_auc: 0.8354\n",
      "Epoch 4/100\n",
      "5242/5242 [==============================] - 48s 9ms/step - loss: 0.2252 - read_comment_loss: 0.0870 - like_loss: 0.0866 - click_avatar_loss: 0.0333 - forward_loss: 0.0183 - read_comment_auc: 0.9418 - like_auc: 0.8790 - click_avatar_auc: 0.8760 - forward_auc: 0.8865 - val_loss: 0.2380 - val_read_comment_loss: 0.0911 - val_like_loss: 0.0907 - val_click_avatar_loss: 0.0359 - val_forward_loss: 0.0204 - val_read_comment_auc: 0.9332 - val_like_auc: 0.8571 - val_click_avatar_auc: 0.8320 - val_forward_auc: 0.8315\n",
      "Epoch 5/100\n",
      "5242/5242 [==============================] - 47s 9ms/step - loss: 0.2218 - read_comment_loss: 0.0859 - like_loss: 0.0855 - click_avatar_loss: 0.0327 - forward_loss: 0.0178 - read_comment_auc: 0.9441 - like_auc: 0.8839 - click_avatar_auc: 0.8851 - forward_auc: 0.8978 - val_loss: 0.2390 - val_read_comment_loss: 0.0911 - val_like_loss: 0.0911 - val_click_avatar_loss: 0.0361 - val_forward_loss: 0.0206 - val_read_comment_auc: 0.9345 - val_like_auc: 0.8573 - val_click_avatar_auc: 0.8277 - val_forward_auc: 0.8418\n",
      "Epoch 6/100\n",
      "5242/5242 [==============================] - 49s 9ms/step - loss: 0.2190 - read_comment_loss: 0.0850 - like_loss: 0.0844 - click_avatar_loss: 0.0321 - forward_loss: 0.0175 - read_comment_auc: 0.9458 - like_auc: 0.8885 - click_avatar_auc: 0.8916 - forward_auc: 0.9036 - val_loss: 0.2401 - val_read_comment_loss: 0.0914 - val_like_loss: 0.0915 - val_click_avatar_loss: 0.0364 - val_forward_loss: 0.0207 - val_read_comment_auc: 0.9323 - val_like_auc: 0.8548 - val_click_avatar_auc: 0.8268 - val_forward_auc: 0.8323\n",
      "Epoch 7/100\n",
      "5242/5242 [==============================] - 50s 10ms/step - loss: 0.2167 - read_comment_loss: 0.0842 - like_loss: 0.0836 - click_avatar_loss: 0.0317 - forward_loss: 0.0173 - read_comment_auc: 0.9473 - like_auc: 0.8918 - click_avatar_auc: 0.8974 - forward_auc: 0.9095 - val_loss: 0.2425 - val_read_comment_loss: 0.0920 - val_like_loss: 0.0921 - val_click_avatar_loss: 0.0369 - val_forward_loss: 0.0214 - val_read_comment_auc: 0.9326 - val_like_auc: 0.8523 - val_click_avatar_auc: 0.8214 - val_forward_auc: 0.8339\n",
      "Epoch 8/100\n",
      "5242/5242 [==============================] - 49s 9ms/step - loss: 0.2146 - read_comment_loss: 0.0834 - like_loss: 0.0828 - click_avatar_loss: 0.0313 - forward_loss: 0.0171 - read_comment_auc: 0.9486 - like_auc: 0.8948 - click_avatar_auc: 0.9017 - forward_auc: 0.9132 - val_loss: 0.2435 - val_read_comment_loss: 0.0924 - val_like_loss: 0.0924 - val_click_avatar_loss: 0.0371 - val_forward_loss: 0.0217 - val_read_comment_auc: 0.9295 - val_like_auc: 0.8510 - val_click_avatar_auc: 0.8161 - val_forward_auc: 0.8328\n",
      "Epoch 9/100\n",
      "5242/5242 [==============================] - 47s 9ms/step - loss: 0.2127 - read_comment_loss: 0.0828 - like_loss: 0.0821 - click_avatar_loss: 0.0309 - forward_loss: 0.0169 - read_comment_auc: 0.9497 - like_auc: 0.8973 - click_avatar_auc: 0.9051 - forward_auc: 0.9170 - val_loss: 0.2458 - val_read_comment_loss: 0.0928 - val_like_loss: 0.0935 - val_click_avatar_loss: 0.0376 - val_forward_loss: 0.0219 - val_read_comment_auc: 0.9319 - val_like_auc: 0.8468 - val_click_avatar_auc: 0.8101 - val_forward_auc: 0.8212\n",
      "Epoch 10/100\n",
      "5242/5242 [==============================] - 48s 9ms/step - loss: 0.2111 - read_comment_loss: 0.0822 - like_loss: 0.0815 - click_avatar_loss: 0.0307 - forward_loss: 0.0167 - read_comment_auc: 0.9506 - like_auc: 0.8995 - click_avatar_auc: 0.9077 - forward_auc: 0.9179 - val_loss: 0.2463 - val_read_comment_loss: 0.0930 - val_like_loss: 0.0937 - val_click_avatar_loss: 0.0376 - val_forward_loss: 0.0220 - val_read_comment_auc: 0.9279 - val_like_auc: 0.8443 - val_click_avatar_auc: 0.8166 - val_forward_auc: 0.8135\n",
      "Epoch 11/100\n",
      "5242/5242 [==============================] - 51s 10ms/step - loss: 0.2096 - read_comment_loss: 0.0816 - like_loss: 0.0809 - click_avatar_loss: 0.0304 - forward_loss: 0.0166 - read_comment_auc: 0.9516 - like_auc: 0.9018 - click_avatar_auc: 0.9098 - forward_auc: 0.9196 - val_loss: 0.2487 - val_read_comment_loss: 0.0940 - val_like_loss: 0.0944 - val_click_avatar_loss: 0.0378 - val_forward_loss: 0.0224 - val_read_comment_auc: 0.9259 - val_like_auc: 0.8449 - val_click_avatar_auc: 0.8079 - val_forward_auc: 0.8056\n",
      "Epoch 12/100\n",
      "5242/5242 [==============================] - 48s 9ms/step - loss: 0.2082 - read_comment_loss: 0.0811 - like_loss: 0.0804 - click_avatar_loss: 0.0302 - forward_loss: 0.0165 - read_comment_auc: 0.9523 - like_auc: 0.9038 - click_avatar_auc: 0.9121 - forward_auc: 0.9224 - val_loss: 0.2504 - val_read_comment_loss: 0.0941 - val_like_loss: 0.0945 - val_click_avatar_loss: 0.0387 - val_forward_loss: 0.0231 - val_read_comment_auc: 0.9257 - val_like_auc: 0.8430 - val_click_avatar_auc: 0.8079 - val_forward_auc: 0.7956\n",
      "Epoch 13/100\n",
      "5242/5242 [==============================] - 49s 9ms/step - loss: 0.2069 - read_comment_loss: 0.0807 - like_loss: 0.0799 - click_avatar_loss: 0.0300 - forward_loss: 0.0163 - read_comment_auc: 0.9531 - like_auc: 0.9053 - click_avatar_auc: 0.9136 - forward_auc: 0.9242 - val_loss: 0.2505 - val_read_comment_loss: 0.0943 - val_like_loss: 0.0942 - val_click_avatar_loss: 0.0390 - val_forward_loss: 0.0230 - val_read_comment_auc: 0.9268 - val_like_auc: 0.8422 - val_click_avatar_auc: 0.8023 - val_forward_auc: 0.8124\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d9aa5cc5f8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型训练\n",
    "train_input = {f: np.array([row for row in train_df[f]]) for f in dense_column_names + sparse_column_names + varlen_sparse_column_names}\n",
    "test_input = {f: np.array([row for row in test_df[f]]) for f in dense_column_names + sparse_column_names + varlen_sparse_column_names}\n",
    "\n",
    "my_callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, verbose=2, mode='auto')\n",
    "]\n",
    "\n",
    "loss = tf.keras.losses.binary_crossentropy\n",
    "model.compile('adam',\n",
    "              loss={'read_comment': loss, 'like': loss, 'click_avatar': loss, 'forward': loss},\n",
    "              metrics=tf.keras.metrics.AUC(name='auc')) # [\"binary_crossentropy\", tf.keras.metrics.AUC(name='auc')]\n",
    "\n",
    "# 多个任务\n",
    "y_list = [train_df[i].values for i in target]\n",
    "model.fit(x=train_input,\n",
    "          y=y_list,\n",
    "          batch_size=1024,\n",
    "          epochs=100,\n",
    "          validation_split=0.2,\n",
    "          callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81dbba40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 read_comment\n",
      "模型准确率:0.9657754221425334, AUC得分:0.9114, LogLoss:0.10193397312271069\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98    588439\n",
      "           1       0.49      0.23      0.32     20597\n",
      "\n",
      "    accuracy                           0.97    609036\n",
      "   macro avg       0.73      0.61      0.65    609036\n",
      "weighted avg       0.96      0.97      0.96    609036\n",
      "\n",
      "==========================================================\n",
      "1 like\n",
      "模型准确率:0.9754579368050493, AUC得分:0.809, LogLoss:0.09780480847999261\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99    594422\n",
      "           1       0.45      0.10      0.17     14614\n",
      "\n",
      "    accuracy                           0.98    609036\n",
      "   macro avg       0.71      0.55      0.58    609036\n",
      "weighted avg       0.97      0.98      0.97    609036\n",
      "\n",
      "==========================================================\n",
      "2 click_avatar\n",
      "模型准确率:0.9924881287805647, AUC得分:0.8024, LogLoss:0.04206241908966137\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    604496\n",
      "           1       0.37      0.01      0.02      4540\n",
      "\n",
      "    accuracy                           0.99    609036\n",
      "   macro avg       0.68      0.51      0.51    609036\n",
      "weighted avg       0.99      0.99      0.99    609036\n",
      "\n",
      "==========================================================\n",
      "3 forward\n",
      "模型准确率:0.9965765570508147, AUC得分:0.8115, LogLoss:0.024243370304032903\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    606975\n",
      "           1       0.26      0.01      0.01      2061\n",
      "\n",
      "    accuracy                           1.00    609036\n",
      "   macro avg       0.63      0.50      0.51    609036\n",
      "weighted avg       0.99      1.00      0.99    609036\n",
      "\n",
      "==========================================================\n"
     ]
    }
   ],
   "source": [
    "# 模型预测与评估\n",
    "result = model.predict(test_input)\n",
    "\n",
    "for idx, target_name in enumerate(target):\n",
    "    print(idx, target_name)\n",
    "    model_metric(np.array([i[0] for i in result[idx]]), test_df[target_name].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdff4dff",
   "metadata": {},
   "source": [
    "##### 不同行为加权gAUC分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb58bf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\SoftwareInstall\\Anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:  read_comment\n",
      "action:  like\n",
      "action:  click_avatar\n",
      "action:  forward\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.6121,\n",
       " 'score_detail': {'read_comment': 0.6048101425505199,\n",
       "  'like': 0.575027553968035,\n",
       "  'click_avatar': 0.6731788167355597,\n",
       "  'forward': 0.630115412144834}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import rankdata\n",
    "from collections import defaultdict\n",
    "\n",
    "def fast_auc(actual, predicted):\n",
    "    # https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/208031    \n",
    "    pred_ranks = rankdata(predicted)\n",
    "    n_pos = np.sum(actual)\n",
    "    n_neg = len(actual) - n_pos\n",
    "    return (np.sum(pred_ranks[actual == 1]) - n_pos*(n_pos+1)/2) / (n_pos*n_neg)\n",
    "\n",
    "def uAUC(labels, preds, users):\n",
    "    \"\"\" 计算uAUC \"\"\"\n",
    "    label_dict, pred_dict, user_flag_dict = defaultdict(lambda: []), defaultdict(lambda: []), defaultdict(lambda: False)\n",
    "    for idx, label in enumerate(labels):\n",
    "        user = users[idx]\n",
    "        pred = preds[idx]\n",
    "        label = labels[idx]\n",
    "        label_dict[user].append(label)\n",
    "        pred_dict[user].append(pred)\n",
    "    \n",
    "    # 当前用户是否全为正/负样本\n",
    "    for user in set(users):\n",
    "        _labels = label_dict[user]\n",
    "        flag = False\n",
    "        for i in range(len(_labels)-1):\n",
    "            if _labels[i] != _labels[i+1]:\n",
    "                flag = True\n",
    "                break\n",
    "        user_flag_dict[user] = flag\n",
    "    \n",
    "    auc_sum = 0.0\n",
    "    auc_cnt = 0.0\n",
    "    for user in user_flag_dict:\n",
    "        if user_flag_dict[user]:\n",
    "            auc = fast_auc(np.asarray(label_dict[user]), np.asarray(pred_dict[user]))\n",
    "            auc_sum += auc\n",
    "            auc_cnt += 1.0\n",
    "    return auc_sum * 1.0 / auc_cnt\n",
    "\n",
    "def score(result_df, action_list):\n",
    "    \"\"\" 计算多个行为的加权gAUC分数 \"\"\"\n",
    "    weight_dict = {\n",
    "        \"read_comment\": 4.0,  # 是否查看评论\n",
    "        \"like\": 3.0,  # 是否点赞\n",
    "        \"click_avatar\": 2.0,  # 是否点击头像\n",
    "        \"forward\": 1.0,  # 是否转发\n",
    "        \"favorite\": 1.0,  # 是否收藏\n",
    "        \"comment\": 1.0,  # 是否发表评论\n",
    "        \"follow\": 1.0  # 是否关注\n",
    "    }\n",
    "    \n",
    "    score = 0.0\n",
    "    score_dict = {}\n",
    "    weight_sum = 0.0\n",
    "    for action in action_list:\n",
    "        print('action: ', action)\n",
    "        labels = result_df[action].values\n",
    "        preds = result_df['p'+action].values\n",
    "        users = result_df['userid'].values\n",
    "        weight = weight_dict[action]\n",
    "        gauc = uAUC(labels, preds, users)\n",
    "        score_dict[action] = gauc\n",
    "        score += weight * gauc\n",
    "        weight_sum += weight\n",
    "    \n",
    "    score /= weight_sum\n",
    "    score = round(score, 4)\n",
    "    return {\n",
    "        'score': score,\n",
    "        'score_detail': score_dict\n",
    "    }\n",
    "\n",
    "result_df = test_df[['userid', 'feedid'] + target]\n",
    "for idx, target_name in enumerate(target):\n",
    "    result_df['p'+target_name] = [i[0] for i in result[idx]]\n",
    "\n",
    "score(result_df, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6741c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
