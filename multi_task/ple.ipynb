{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d666e7e2",
   "metadata": {},
   "source": [
    "## PLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86642f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import  MinMaxScaler, LabelEncoder\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import classification_report, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cda04f",
   "metadata": {},
   "source": [
    "##### 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29e78855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape: (6708846, 12), test_df.shape: (609036, 12)\n"
     ]
    }
   ],
   "source": [
    "def get_wechat_data():\n",
    "    \"\"\" 读取wechat数据集 \"\"\"\n",
    "    train_path = '../data/wechat/train_df.pkl'\n",
    "    test_path = '../data/wechat/test_df.pkl'\n",
    "    encoder_dict_path = '../data/wechat/encoder_dict.pkl'\n",
    "    train_df = pd.read_pickle(train_path)\n",
    "    test_df = pd.read_pickle(test_path)\n",
    "    encoder_dict = joblib.load(encoder_dict_path)\n",
    "    return train_df, test_df, encoder_dict\n",
    "\n",
    "train_df, test_df, encoder_dict = get_wechat_data()\n",
    "train_df = train_df.sample(frac=1.0)\n",
    "data = pd.concat([train_df, test_df], axis=0)\n",
    "print('train_df.shape: {}, test_df.shape: {}'.format(train_df.shape, test_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1914ea57",
   "metadata": {},
   "source": [
    "##### 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94b61989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseFeature(name='userid', vocabulary_size=17390, embedding_size=8),\n",
       " SparseFeature(name='feedid', vocabulary_size=16448, embedding_size=8),\n",
       " SparseFeature(name='authorid', vocabulary_size=6966, embedding_size=8),\n",
       " SparseFeature(name='bgm_song_id', vocabulary_size=5773, embedding_size=8),\n",
       " SparseFeature(name='bgm_singer_id', vocabulary_size=4573, embedding_size=8),\n",
       " DenseFeature(name='videoplayseconds', dimension=1),\n",
       " VarLenSparseFeature(name='manual_keyword_list', vocabulary_size=21576, embedding_size=8, maxlen=18),\n",
       " VarLenSparseFeature(name='manual_tag_list', vocabulary_size=349, embedding_size=8, maxlen=11)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = ['read_comment', 'like', 'click_avatar', 'forward']\n",
    "\n",
    "# 稠密特征、稀疏特征、变长稀疏特征\n",
    "dense_column_names = ['videoplayseconds']\n",
    "sparse_column_names = ['userid', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id']\n",
    "varlen_sparse_column_names = ['manual_keyword_list', 'manual_tag_list']\n",
    "\n",
    "SparseFeature = namedtuple('SparseFeature', ['name', 'vocabulary_size', 'embedding_size'])\n",
    "DenseFeature = namedtuple('DenseFeature', ['name', 'dimension'])\n",
    "VarLenSparseFeature = namedtuple('VarLenSparseFeature', ['name', 'vocabulary_size', 'embedding_size', 'maxlen'])\n",
    "\n",
    "varlen_sparse_column_maxlen_dict = {\n",
    "    'manual_keyword_list': 18,\n",
    "    'manual_tag_list': 11\n",
    "}\n",
    "\n",
    "feature_columns = [SparseFeature(f, vocabulary_size=data[f].nunique(), embedding_size=8) for f in sparse_column_names] + \\\n",
    "[DenseFeature(f, 1) for f in dense_column_names] + \\\n",
    "[VarLenSparseFeature(f, len(encoder_dict[f])+1, embedding_size=8, maxlen=varlen_sparse_column_maxlen_dict[f]) for f in varlen_sparse_column_names]\n",
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b04cbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "manual_keyword_list (InputLayer [(None, 18)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "manual_tag_list (InputLayer)    [(None, 11)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "userid (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "feedid (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "authorid (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bgm_song_id (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bgm_singer_id (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "var_emb_manual_keyword_list (Em (None, 18, 8)        172616      manual_keyword_list[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "var_emb_manual_tag_list (Embedd (None, 11, 8)        2800        manual_tag_list[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "emb_userid (Embedding)          (None, 1, 8)         139128      userid[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "emb_feedid (Embedding)          (None, 1, 8)         131592      feedid[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "emb_authorid (Embedding)        (None, 1, 8)         55736       authorid[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "emb_bgm_song_id (Embedding)     (None, 1, 8)         46192       bgm_song_id[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "emb_bgm_singer_id (Embedding)   (None, 1, 8)         36592       bgm_singer_id[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, 18, 8)        0           var_emb_manual_keyword_list[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 11, 8)        0           var_emb_manual_tag_list[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8)            0           emb_userid[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8)            0           emb_feedid[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8)            0           emb_authorid[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 8)            0           emb_bgm_song_id[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 8)            0           emb_bgm_singer_id[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "mean_pooling_layer (MeanPooling (None, 8)            0           masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mean_pooling_layer_1 (MeanPooli (None, 8)            0           masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "videoplayseconds (InputLayer)   [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 40)           0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 16)           0           mean_pooling_layer[0][0]         \n",
      "                                                                 mean_pooling_layer_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 57)           0           videoplayseconds[0][0]           \n",
      "                                                                 concatenate[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           3712        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 64)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           4160        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cgc_layer (CGCLayer)            [(None, 32), (None,  39000       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "read_comment (Dense)            (None, 1)            33          cgc_layer[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "like (Dense)                    (None, 1)            33          cgc_layer[0][1]                  \n",
      "__________________________________________________________________________________________________\n",
      "click_avatar (Dense)            (None, 1)            33          cgc_layer[0][2]                  \n",
      "__________________________________________________________________________________________________\n",
      "forward (Dense)                 (None, 1)            33          cgc_layer[0][3]                  \n",
      "==================================================================================================\n",
      "Total params: 631,660\n",
      "Trainable params: 631,660\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def model_metric(prob, label, thr=0.5):\n",
    "    \"\"\" 模型评估 \"\"\"\n",
    "    # AUC\n",
    "    fpr, tpr, threshold = metrics.roc_curve(label, prob)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    score = metrics.accuracy_score(label, prob > thr)\n",
    "    # LogLoss\n",
    "    logloss = log_loss(label, prob)\n",
    "    print('模型准确率:{}, AUC得分:{}, LogLoss:{}'.format(score, round(auc, 4), logloss))\n",
    "    print(classification_report(label, prob > thr, digits=2))\n",
    "    print('==========================================================')\n",
    "\n",
    "def build_input_layers(feature_columns):\n",
    "    \"\"\" 构建输入层 \"\"\"\n",
    "    dense_input_dict, sparse_input_dict, varlen_sparse_input_dict = {}, {}, {}\n",
    "    for f in feature_columns:\n",
    "        if isinstance(f, DenseFeature):\n",
    "            dense_input_dict[f.name] = Input(shape=(f.dimension, ), name=f.name)\n",
    "        elif isinstance(f, SparseFeature):\n",
    "            sparse_input_dict[f.name] = Input(shape=(1, ), name=f.name)\n",
    "        elif isinstance(f, VarLenSparseFeature):\n",
    "            varlen_sparse_input_dict[f.name] = Input(shape=(f.maxlen, ), name=f.name)\n",
    "    return dense_input_dict, sparse_input_dict, varlen_sparse_input_dict\n",
    "\n",
    "class MMoELayer(Layer):\n",
    "    def __init__(self, expert_dim, expert_num, task_num):\n",
    "        super(MMoELayer, self).__init__()\n",
    "        self.task_num = task_num\n",
    "        # expert，将拼接的向量（sparse embedding+dense feature）输入到多个relu为激活函数的全连接层，表示多个Expert网络。\n",
    "        self.expert_layers = [Dense(expert_dim, activation='relu') for i in range(expert_num)]\n",
    "        # gate，将拼接的向量输入到一个softmax层，输出expert_num个权重，表示Expert网络的权重。\n",
    "        self.gate_layers = [Dense(expert_num, activation='softmax') for i in range(task_num)]\n",
    "        \n",
    "    def call(self, x):\n",
    "        # 专家网络的输出张量\n",
    "        expert_out = [expert_layer(x) for expert_layer in self.expert_layers] # [(None, expert_dim)...]\n",
    "        # [(None, 1, expert_dim)] => (None, expert_num, expert_dim)\n",
    "        expert_out = Concatenate(axis=1)([e[:, tf.newaxis, :] for e in expert_out])\n",
    "        \n",
    "        # 门控网络的输出权重\n",
    "        gate_out = [gate_layer(x) for gate_layer in self.gate_layers] # [(None, expert_num)...]\n",
    "        \n",
    "        # 一个任务一个门网络，每个任务的门网络*所有专家网络，得到每个任务所需要的浅层网络信息\n",
    "        tower_inputs = []\n",
    "        for i in range(self.task_num):\n",
    "            _gate_out = tf.expand_dims(gate_out, axis=-1) # (None, expert_num, 1)\n",
    "            # (None, expert_dim, expert_num) * (None, expert_num, 1) => (None, expert_dim, 1)\n",
    "            _tower_input = tf.matmul(expert_out, _gate_out, transpose_a=True) \n",
    "            tower_inputs.append(Flatten()(_tower_input)) # (None, expert_dim)\n",
    "        return tower_inputs\n",
    "    \n",
    "class CGCLayer(Layer):\n",
    "    def __init__(self, expert_dim, sp_expert_nums, share_expert_num, task_num):\n",
    "        \"\"\"\n",
    "        :expert_dim 专家层维度\n",
    "        :sp_expert_nums 每个任务使用的特定专家层数量\n",
    "        :task_num 任务数量\n",
    "        :share_expert_num 共享的专家层数量\n",
    "        \"\"\"\n",
    "        super(CGCLayer, self).__init__()\n",
    "        self.task_num = task_num\n",
    "        \n",
    "        # 每个任务生成expert_num个特定专家层\n",
    "        self.sp_expert_layers = []\n",
    "        for i in range(task_num):\n",
    "            _sp_expert_layers = [Dense(expert_dim, activation='relu') for i in range(sp_expert_nums[i])]\n",
    "            self.sp_expert_layers.append(_sp_expert_layers)\n",
    "        \n",
    "        # 共享专家层\n",
    "        self.share_expert_layers = [Dense(expert_dim, activation='relu') for i in range(share_expert_num)]\n",
    "        \n",
    "        # 每个任务一个门层，门层输出维度=共享专家层数 + 特定专家层\n",
    "        self.gate_layers = [Dense(share_expert_num+sp_expert_nums[i], activation='softmax') for i in range(task_num)]\n",
    "        \n",
    "    def call(self, x):\n",
    "        # 专家网络的输出张量\n",
    "        sp_expert_out = [[expert_layer(x) for expert_layer in _sp_expert_layers] for _sp_expert_layers in self.sp_expert_layers] # [[(None, expert_dim)...](任务一)]\n",
    "        share_expert_out = [expert_layer(x) for expert_layer in self.share_expert_layers] \n",
    "        \n",
    "        tower_inputs = []\n",
    "        for i in range(self.task_num):\n",
    "            gate_out = self.gate_layers[i](x)\n",
    "            _gate_out = tf.expand_dims(gate_out, axis=-1)\n",
    "            expert_out = share_expert_out + sp_expert_out[i]\n",
    "            expert_out = Concatenate(axis=1)([e[:, tf.newaxis, :] for e in expert_out])\n",
    "            _tower_input = tf.matmul(expert_out, _gate_out, transpose_a=True) \n",
    "            tower_inputs.append(Flatten()(_tower_input)) # (None, expert_dim)\n",
    "        return tower_inputs\n",
    "        \n",
    "def concat_input_list(input_list):\n",
    "    \"\"\" 合并input列表 \"\"\"\n",
    "    _num = len(input_list)\n",
    "    if _num > 1:\n",
    "        return Concatenate(axis=1)(input_list)\n",
    "    elif len(input_list) == 1:\n",
    "        return input_list[0]\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "def build_embedding_layers(feature_columns):\n",
    "    \"\"\" 构建embedding层 \"\"\"\n",
    "    embedding_layer_dict = {}\n",
    "    for f in feature_columns:\n",
    "        if isinstance(f, SparseFeature):\n",
    "            embedding_layer_dict[f.name] = Embedding(f.vocabulary_size+1, f.embedding_size, name='emb_' + f.name)\n",
    "        elif isinstance(f, VarLenSparseFeature):\n",
    "            embedding_layer_dict[f.name] = Embedding(f.vocabulary_size+1, f.embedding_size, name='var_emb_' + f.name, mask_zero=True)\n",
    "    return embedding_layer_dict\n",
    "        \n",
    "def embedding_lookup(columns, input_dict, embedding_layer_dict, flatten=False):\n",
    "    \"\"\" 根据feature_columns或column_names查表，得到对应embedding向量列表 \"\"\"\n",
    "    embedding_list = []\n",
    "    for f in columns:\n",
    "        if type(f) == str:\n",
    "            column_name = f\n",
    "        else:\n",
    "            column_name = f.name\n",
    "        _input = input_dict[column_name]\n",
    "        _embed = embedding_layer_dict[column_name]\n",
    "        embed_layer = _embed(_input)\n",
    "        if flatten:\n",
    "            embed_layer = Flatten()(embed_layer)\n",
    "        embedding_list.append(embed_layer)\n",
    "    return embedding_list\n",
    "    \n",
    "class MeanPoolingLayer(Layer):\n",
    "    def __init__(self, axis, **kwargs):\n",
    "        super(MeanPoolingLayer, self).__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "        self.supports_masking = True\n",
    "    \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # need not to pass the mask to next layers\n",
    "        return None\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        if x is not None:\n",
    "            mask = K.cast(mask, K.floatx()) # (None, 18)\n",
    "            mask = K.repeat(mask, x.shape[-1]) # (None, 4, 18)\n",
    "            mask = tf.transpose(mask, [0, 2, 1]) # (None, 18, 4)\n",
    "            x = x * mask # # (None, 18, 4) * (None, 18, 4)\n",
    "            return K.sum(x, axis=self.axis) / K.sum(mask, axis=self.axis)\n",
    "        else:\n",
    "            return K.mean(x, axis=self.axis)\n",
    "    \n",
    "def PLE(feature_columns, target, dnn_hidden_units=[64, 64], expert_dim=32, sp_expert_nums=[4, 4, 4, 4], share_expert_num=2, task_num=4, is_cgc=False):\n",
    "    \n",
    "    dense_input_dict, sparse_input_dict, varlen_sparse_input_dict = build_input_layers(feature_columns)\n",
    "    \n",
    "    # Input\n",
    "    input_list = list(dense_input_dict.values()) + list(sparse_input_dict.values()) + list(varlen_sparse_input_dict.values())\n",
    "    \n",
    "    # dense feature (input->concat)\n",
    "    concat_dense_input_list = concat_input_list(list(dense_input_dict.values()))\n",
    "    \n",
    "    # sparse feature (input->embed->concat)\n",
    "    embedding_layer_dict = build_embedding_layers(feature_columns)\n",
    "    sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeature), feature_columns))\n",
    "    flatten_sparse_embed_list = embedding_lookup(sparse_feature_columns, sparse_input_dict, embedding_layer_dict, flatten=True)\n",
    "    concat_flatten_sparse_embed_list = concat_input_list(flatten_sparse_embed_list)\n",
    "    \n",
    "    # seq embeddings (input->embed->pooling->concat)\n",
    "    varlen_sparse_feature_columns = list(filter(lambda x: isinstance(x, VarLenSparseFeature), feature_columns))\n",
    "    varlen_sparse_embed_list = []\n",
    "    for f in varlen_sparse_feature_columns:\n",
    "        _input = varlen_sparse_input_dict[f.name] #  (None, 18)\n",
    "        _embed = embedding_layer_dict[f.name]\n",
    "        embed_layer = _embed(_input) # (None, 18, 4)\n",
    "        mask = Masking()(embed_layer) # (None, 18, 4)\n",
    "        mean_pooling_embed = MeanPoolingLayer(axis=1)(mask) # (None, 4)\n",
    "        varlen_sparse_embed_list.append(mean_pooling_embed)\n",
    "    concat_varlen_sparse_embed_list = concat_input_list(varlen_sparse_embed_list)\n",
    "    \n",
    "    # concat dense feature + concat sparse embeddings + concat seq embeddings\n",
    "    dnn_input = Concatenate(axis=1)([concat_dense_input_list, concat_flatten_sparse_embed_list, concat_varlen_sparse_embed_list]) \n",
    "    \n",
    "    # DNN\n",
    "    for dnn in dnn_hidden_units:\n",
    "        dnn_input = Dropout(0.1)(Dense(dnn, activation='relu')(dnn_input))\n",
    "    \n",
    "    # MMoE网络层\n",
    "    if is_cgc:\n",
    "        tower_inputs = CGCLayer(expert_dim, sp_expert_nums, share_expert_num, task_num)(dnn_input)\n",
    "    else:\n",
    "        tower_inputs = CGCLayer(expert_dim, sp_expert_nums, share_expert_num, task_num)(dnn_input)\n",
    "        concat_tower_inputs = Concatenate(axis=1)(tower_inputs)\n",
    "        print('concat_tower_inputs.shape: ', concat_tower_inputs.shape)\n",
    "        # 2层CGC网络\n",
    "        tower_inputs = CGCLayer(expert_dim, sp_expert_nums, share_expert_num, task_num)(concat_tower_inputs)\n",
    "    \n",
    "    # 通过各个任务对应的门网络的加权求和，每个塔都得到自己任务所需要的专家信息。\n",
    "    outputs = [Dense(1, activation='sigmoid', name=target_name)(tower_input) for tower_input, target_name in zip(tower_inputs, target)]\n",
    "    model = Model(input_list, outputs)\n",
    "    return model\n",
    "\n",
    "model = PLE(feature_columns,\n",
    "             target, \n",
    "             dnn_hidden_units=[64, 64],\n",
    "             expert_dim=32, \n",
    "             sp_expert_nums=[4, 4, 4, 4],\n",
    "             share_expert_num=2,\n",
    "             task_num=4,\n",
    "             is_cgc=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e33c5779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5242/5242 [==============================] - 63s 11ms/step - loss: 0.2696 - read_comment_loss: 0.1011 - like_loss: 0.0996 - click_avatar_loss: 0.0426 - forward_loss: 0.0263 - read_comment_auc: 0.9087 - like_auc: 0.8101 - click_avatar_auc: 0.7293 - forward_auc: 0.6959 - val_loss: 0.2415 - val_read_comment_loss: 0.0918 - val_like_loss: 0.0917 - val_click_avatar_loss: 0.0366 - val_forward_loss: 0.0214 - val_read_comment_auc: 0.9295 - val_like_auc: 0.8515 - val_click_avatar_auc: 0.8331 - val_forward_auc: 0.7912\n",
      "Epoch 2/100\n",
      "5242/5242 [==============================] - 56s 11ms/step - loss: 0.2347 - read_comment_loss: 0.0899 - like_loss: 0.0892 - click_avatar_loss: 0.0354 - forward_loss: 0.0201 - read_comment_auc: 0.9351 - like_auc: 0.8655 - click_avatar_auc: 0.8454 - forward_auc: 0.8271 - val_loss: 0.2383 - val_read_comment_loss: 0.0908 - val_like_loss: 0.0912 - val_click_avatar_loss: 0.0357 - val_forward_loss: 0.0207 - val_read_comment_auc: 0.9339 - val_like_auc: 0.8555 - val_click_avatar_auc: 0.8356 - val_forward_auc: 0.8218\n",
      "Epoch 3/100\n",
      "5242/5242 [==============================] - 57s 11ms/step - loss: 0.2294 - read_comment_loss: 0.0884 - like_loss: 0.0877 - click_avatar_loss: 0.0342 - forward_loss: 0.0191 - read_comment_auc: 0.9388 - like_auc: 0.8737 - click_avatar_auc: 0.8639 - forward_auc: 0.8587 - val_loss: 0.2384 - val_read_comment_loss: 0.0909 - val_like_loss: 0.0912 - val_click_avatar_loss: 0.0357 - val_forward_loss: 0.0207 - val_read_comment_auc: 0.9339 - val_like_auc: 0.8552 - val_click_avatar_auc: 0.8427 - val_forward_auc: 0.8386\n",
      "Epoch 4/100\n",
      "5242/5242 [==============================] - 60s 11ms/step - loss: 0.2257 - read_comment_loss: 0.0872 - like_loss: 0.0865 - click_avatar_loss: 0.0334 - forward_loss: 0.0185 - read_comment_auc: 0.9413 - like_auc: 0.8792 - click_avatar_auc: 0.8750 - forward_auc: 0.8760 - val_loss: 0.2386 - val_read_comment_loss: 0.0907 - val_like_loss: 0.0912 - val_click_avatar_loss: 0.0359 - val_forward_loss: 0.0207 - val_read_comment_auc: 0.9345 - val_like_auc: 0.8552 - val_click_avatar_auc: 0.8449 - val_forward_auc: 0.8263\n",
      "Epoch 5/100\n",
      "5242/5242 [==============================] - 62s 12ms/step - loss: 0.2226 - read_comment_loss: 0.0862 - like_loss: 0.0854 - click_avatar_loss: 0.0329 - forward_loss: 0.0180 - read_comment_auc: 0.9434 - like_auc: 0.8841 - click_avatar_auc: 0.8819 - forward_auc: 0.8884 - val_loss: 0.2400 - val_read_comment_loss: 0.0909 - val_like_loss: 0.0920 - val_click_avatar_loss: 0.0361 - val_forward_loss: 0.0209 - val_read_comment_auc: 0.9339 - val_like_auc: 0.8523 - val_click_avatar_auc: 0.8395 - val_forward_auc: 0.8426\n",
      "Epoch 6/100\n",
      "5242/5242 [==============================] - 67s 13ms/step - loss: 0.2199 - read_comment_loss: 0.0853 - like_loss: 0.0844 - click_avatar_loss: 0.0324 - forward_loss: 0.0177 - read_comment_auc: 0.9451 - like_auc: 0.8879 - click_avatar_auc: 0.8880 - forward_auc: 0.8966 - val_loss: 0.2406 - val_read_comment_loss: 0.0913 - val_like_loss: 0.0920 - val_click_avatar_loss: 0.0361 - val_forward_loss: 0.0211 - val_read_comment_auc: 0.9317 - val_like_auc: 0.8519 - val_click_avatar_auc: 0.8340 - val_forward_auc: 0.8238\n",
      "Epoch 7/100\n",
      "5242/5242 [==============================] - 66s 12ms/step - loss: 0.2175 - read_comment_loss: 0.0845 - like_loss: 0.0835 - click_avatar_loss: 0.0320 - forward_loss: 0.0175 - read_comment_auc: 0.9466 - like_auc: 0.8919 - click_avatar_auc: 0.8933 - forward_auc: 0.9030 - val_loss: 0.2428 - val_read_comment_loss: 0.0921 - val_like_loss: 0.0927 - val_click_avatar_loss: 0.0365 - val_forward_loss: 0.0214 - val_read_comment_auc: 0.9299 - val_like_auc: 0.8480 - val_click_avatar_auc: 0.8238 - val_forward_auc: 0.8222\n",
      "Epoch 8/100\n",
      "5242/5242 [==============================] - 65s 12ms/step - loss: 0.2155 - read_comment_loss: 0.0837 - like_loss: 0.0828 - click_avatar_loss: 0.0317 - forward_loss: 0.0173 - read_comment_auc: 0.9480 - like_auc: 0.8951 - click_avatar_auc: 0.8954 - forward_auc: 0.9068 - val_loss: 0.2439 - val_read_comment_loss: 0.0925 - val_like_loss: 0.0930 - val_click_avatar_loss: 0.0367 - val_forward_loss: 0.0216 - val_read_comment_auc: 0.9294 - val_like_auc: 0.8479 - val_click_avatar_auc: 0.8235 - val_forward_auc: 0.8195\n",
      "Epoch 9/100\n",
      "5242/5242 [==============================] - 68s 13ms/step - loss: 0.2137 - read_comment_loss: 0.0831 - like_loss: 0.0821 - click_avatar_loss: 0.0315 - forward_loss: 0.0171 - read_comment_auc: 0.9489 - like_auc: 0.8976 - click_avatar_auc: 0.8983 - forward_auc: 0.9087 - val_loss: 0.2451 - val_read_comment_loss: 0.0925 - val_like_loss: 0.0933 - val_click_avatar_loss: 0.0371 - val_forward_loss: 0.0221 - val_read_comment_auc: 0.9293 - val_like_auc: 0.8462 - val_click_avatar_auc: 0.8235 - val_forward_auc: 0.8179\n",
      "Epoch 10/100\n",
      "5242/5242 [==============================] - 67s 13ms/step - loss: 0.2122 - read_comment_loss: 0.0825 - like_loss: 0.0815 - click_avatar_loss: 0.0312 - forward_loss: 0.0170 - read_comment_auc: 0.9499 - like_auc: 0.8998 - click_avatar_auc: 0.9008 - forward_auc: 0.9127 - val_loss: 0.2472 - val_read_comment_loss: 0.0931 - val_like_loss: 0.0939 - val_click_avatar_loss: 0.0376 - val_forward_loss: 0.0225 - val_read_comment_auc: 0.9285 - val_like_auc: 0.8425 - val_click_avatar_auc: 0.8111 - val_forward_auc: 0.8039\n",
      "Epoch 11/100\n",
      "5242/5242 [==============================] - 68s 13ms/step - loss: 0.2107 - read_comment_loss: 0.0820 - like_loss: 0.0809 - click_avatar_loss: 0.0310 - forward_loss: 0.0168 - read_comment_auc: 0.9507 - like_auc: 0.9021 - click_avatar_auc: 0.9024 - forward_auc: 0.9156 - val_loss: 0.2489 - val_read_comment_loss: 0.0936 - val_like_loss: 0.0948 - val_click_avatar_loss: 0.0379 - val_forward_loss: 0.0226 - val_read_comment_auc: 0.9271 - val_like_auc: 0.8413 - val_click_avatar_auc: 0.8071 - val_forward_auc: 0.8076\n",
      "Epoch 12/100\n",
      "5242/5242 [==============================] - 65s 12ms/step - loss: 0.2094 - read_comment_loss: 0.0814 - like_loss: 0.0804 - click_avatar_loss: 0.0308 - forward_loss: 0.0167 - read_comment_auc: 0.9514 - like_auc: 0.9040 - click_avatar_auc: 0.9051 - forward_auc: 0.9169 - val_loss: 0.2521 - val_read_comment_loss: 0.0947 - val_like_loss: 0.0961 - val_click_avatar_loss: 0.0375 - val_forward_loss: 0.0238 - val_read_comment_auc: 0.9248 - val_like_auc: 0.8377 - val_click_avatar_auc: 0.8174 - val_forward_auc: 0.7847\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2002a14c438>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型训练\n",
    "train_input = {f: np.array([row for row in train_df[f]]) for f in dense_column_names + sparse_column_names + varlen_sparse_column_names}\n",
    "test_input = {f: np.array([row for row in test_df[f]]) for f in dense_column_names + sparse_column_names + varlen_sparse_column_names}\n",
    "\n",
    "my_callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, verbose=2, mode='auto')\n",
    "]\n",
    "\n",
    "loss = tf.keras.losses.binary_crossentropy\n",
    "model.compile('adam',\n",
    "              loss={'read_comment': loss, 'like': loss, 'click_avatar': loss, 'forward': loss},\n",
    "              metrics=tf.keras.metrics.AUC(name='auc')) # [\"binary_crossentropy\", tf.keras.metrics.AUC(name='auc')]\n",
    "\n",
    "# 多个任务\n",
    "y_list = [train_df[i].values for i in target]\n",
    "model.fit(x=train_input,\n",
    "          y=y_list,\n",
    "          batch_size=1024,\n",
    "          epochs=100,\n",
    "          validation_split=0.2,\n",
    "          callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7af43aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 read_comment\n",
      "模型准确率:0.9667277467998607, AUC得分:0.9118, LogLoss:0.10097670342414897\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98    588439\n",
      "           1       0.53      0.15      0.24     20597\n",
      "\n",
      "    accuracy                           0.97    609036\n",
      "   macro avg       0.75      0.57      0.61    609036\n",
      "weighted avg       0.96      0.97      0.96    609036\n",
      "\n",
      "==========================================================\n",
      "1 like\n",
      "模型准确率:0.9759176797430694, AUC得分:0.8084, LogLoss:0.09929165239177191\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99    594422\n",
      "           1       0.49      0.09      0.15     14614\n",
      "\n",
      "    accuracy                           0.98    609036\n",
      "   macro avg       0.73      0.54      0.57    609036\n",
      "weighted avg       0.97      0.98      0.97    609036\n",
      "\n",
      "==========================================================\n",
      "2 click_avatar\n",
      "模型准确率:0.9925406708306241, AUC得分:0.8092, LogLoss:0.04046633934532532\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    604496\n",
      "           1       0.45      0.00      0.01      4540\n",
      "\n",
      "    accuracy                           0.99    609036\n",
      "   macro avg       0.72      0.50      0.50    609036\n",
      "weighted avg       0.99      0.99      0.99    609036\n",
      "\n",
      "==========================================================\n",
      "3 forward\n",
      "模型准确率:0.9966028280758444, AUC得分:0.8196, LogLoss:0.023672397650630422\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    606975\n",
      "           1       0.35      0.00      0.01      2061\n",
      "\n",
      "    accuracy                           1.00    609036\n",
      "   macro avg       0.67      0.50      0.50    609036\n",
      "weighted avg       0.99      1.00      0.99    609036\n",
      "\n",
      "==========================================================\n"
     ]
    }
   ],
   "source": [
    "# 模型预测与评估\n",
    "result = model.predict(test_input)\n",
    "\n",
    "for idx, target_name in enumerate(target):\n",
    "    print(idx, target_name)\n",
    "    model_metric(np.array([i[0] for i in result[idx]]), test_df[target_name].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b2b810",
   "metadata": {},
   "source": [
    "##### 不同行为加权gAUC分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2df4d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\SoftwareInstall\\Anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:  read_comment\n",
      "action:  like\n",
      "action:  click_avatar\n",
      "action:  forward\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.6176,\n",
       " 'score_detail': {'read_comment': 0.6043690226531183,\n",
       "  'like': 0.5842259667486817,\n",
       "  'click_avatar': 0.6747976432374228,\n",
       "  'forward': 0.6557983203994906}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import rankdata\n",
    "from collections import defaultdict\n",
    "\n",
    "def fast_auc(actual, predicted):\n",
    "    # https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/208031    \n",
    "    pred_ranks = rankdata(predicted)\n",
    "    n_pos = np.sum(actual)\n",
    "    n_neg = len(actual) - n_pos\n",
    "    return (np.sum(pred_ranks[actual == 1]) - n_pos*(n_pos+1)/2) / (n_pos*n_neg)\n",
    "\n",
    "def uAUC(labels, preds, users):\n",
    "    \"\"\" 计算uAUC \"\"\"\n",
    "    label_dict, pred_dict, user_flag_dict = defaultdict(lambda: []), defaultdict(lambda: []), defaultdict(lambda: False)\n",
    "    for idx, label in enumerate(labels):\n",
    "        user = users[idx]\n",
    "        pred = preds[idx]\n",
    "        label = labels[idx]\n",
    "        label_dict[user].append(label)\n",
    "        pred_dict[user].append(pred)\n",
    "    \n",
    "    # 当前用户是否全为正/负样本\n",
    "    for user in set(users):\n",
    "        _labels = label_dict[user]\n",
    "        flag = False\n",
    "        for i in range(len(_labels)-1):\n",
    "            if _labels[i] != _labels[i+1]:\n",
    "                flag = True\n",
    "                break\n",
    "        user_flag_dict[user] = flag\n",
    "    \n",
    "    auc_sum = 0.0\n",
    "    auc_cnt = 0.0\n",
    "    for user in user_flag_dict:\n",
    "        if user_flag_dict[user]:\n",
    "            auc = fast_auc(np.asarray(label_dict[user]), np.asarray(pred_dict[user]))\n",
    "            auc_sum += auc\n",
    "            auc_cnt += 1.0\n",
    "    return auc_sum * 1.0 / auc_cnt\n",
    "\n",
    "def score(result_df, action_list):\n",
    "    \"\"\" 计算多个行为的加权gAUC分数 \"\"\"\n",
    "    weight_dict = {\n",
    "        \"read_comment\": 4.0,  # 是否查看评论\n",
    "        \"like\": 3.0,  # 是否点赞\n",
    "        \"click_avatar\": 2.0,  # 是否点击头像\n",
    "        \"forward\": 1.0,  # 是否转发\n",
    "        \"favorite\": 1.0,  # 是否收藏\n",
    "        \"comment\": 1.0,  # 是否发表评论\n",
    "        \"follow\": 1.0  # 是否关注\n",
    "    }\n",
    "    \n",
    "    score = 0.0\n",
    "    score_dict = {}\n",
    "    weight_sum = 0.0\n",
    "    for action in action_list:\n",
    "        print('action: ', action)\n",
    "        labels = result_df[action].values\n",
    "        preds = result_df['p'+action].values\n",
    "        users = result_df['userid'].values\n",
    "        weight = weight_dict[action]\n",
    "        gauc = uAUC(labels, preds, users)\n",
    "        score_dict[action] = gauc\n",
    "        score += weight * gauc\n",
    "        weight_sum += weight\n",
    "    \n",
    "    score /= weight_sum\n",
    "    score = round(score, 4)\n",
    "    return {\n",
    "        'score': score,\n",
    "        'score_detail': score_dict\n",
    "    }\n",
    "\n",
    "result_df = test_df[['userid', 'feedid'] + target]\n",
    "for idx, target_name in enumerate(target):\n",
    "    result_df['p'+target_name] = [i[0] for i in result[idx]]\n",
    "\n",
    "score(result_df, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac963c91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
