{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f66e92bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import  MinMaxScaler, LabelEncoder\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import classification_report, roc_curve\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.initializers import Zeros, glorot_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b261809",
   "metadata": {},
   "source": [
    "##### 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5561a08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (1956191, 14), test.shape: (12078, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>hist_item_id</th>\n",
       "      <th>hist_s1</th>\n",
       "      <th>hist_s2</th>\n",
       "      <th>item_id</th>\n",
       "      <th>label</th>\n",
       "      <th>rating</th>\n",
       "      <th>click_timestamp</th>\n",
       "      <th>hist_len</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>item_date</th>\n",
       "      <th>item_title</th>\n",
       "      <th>item_cate_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32134</th>\n",
       "      <td>5982</td>\n",
       "      <td>1814,1518,353,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>1814,1518,353,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>2993</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>1246,3239,0,0,0,0,0,0,0,0,0,0,0,0,0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856016</th>\n",
       "      <td>3824</td>\n",
       "      <td>2974,2383,2392,2547,2667,3326,2191,2405,3364,2...</td>\n",
       "      <td>2382,2051,3808,1910,1912,2722,2882,2397,2409,3...</td>\n",
       "      <td>1913,3774,2312,2331,2680,3698,2810,2305,1369,2...</td>\n",
       "      <td>1361</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>966541021</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>63</td>\n",
       "      <td>2338,1193,0,0,0,0,0,0,0,0,0,0,0,0,0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492110</th>\n",
       "      <td>182</td>\n",
       "      <td>3683,2046,504,524,1673,3510,643,3685,3687,3416...</td>\n",
       "      <td>1047,3184,1544,3121,3233,2368,1,3046,2328,353,...</td>\n",
       "      <td>1657,316,334,508,1196,1960,1727,109,160,3187,0...</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>977085349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>76</td>\n",
       "      <td>85,0,0,0,0,0,0,0,0,0,0,0,0,0,0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622647</th>\n",
       "      <td>4269</td>\n",
       "      <td>2301,2345,3382,2077,2471,1120,1357,2030,465,23...</td>\n",
       "      <td>2461,2503,1221,1255,2596,1282,957,1354,3635,12...</td>\n",
       "      <td>2919,3638,1182,741,258,32,913,1197,1189,2900,0...</td>\n",
       "      <td>2436</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>3571,3572,0,0,0,0,0,0,0,0,0,0,0,0,0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80387</th>\n",
       "      <td>1087</td>\n",
       "      <td>3459,2339,2589,1336,1376,1608,2572,1114,1540,1...</td>\n",
       "      <td>1059,587,589,1112,2048,2047,2968,1899,2346,135...</td>\n",
       "      <td>1240,1268,958,2803,1180,2298,3393,3036,1272,28...</td>\n",
       "      <td>497</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>74</td>\n",
       "      <td>748,960,0,0,0,0,0,0,0,0,0,0,0,0,0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id                                       hist_item_id  \\\n",
       "32134       5982  1814,1518,353,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...   \n",
       "856016      3824  2974,2383,2392,2547,2667,3326,2191,2405,3364,2...   \n",
       "492110       182  3683,2046,504,524,1673,3510,643,3685,3687,3416...   \n",
       "1622647     4269  2301,2345,3382,2077,2471,1120,1357,2030,465,23...   \n",
       "80387       1087  3459,2339,2589,1336,1376,1608,2572,1114,1540,1...   \n",
       "\n",
       "                                                   hist_s1  \\\n",
       "32134    1814,1518,353,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...   \n",
       "856016   2382,2051,3808,1910,1912,2722,2882,2397,2409,3...   \n",
       "492110   1047,3184,1544,3121,3233,2368,1,3046,2328,353,...   \n",
       "1622647  2461,2503,1221,1255,2596,1282,957,1354,3635,12...   \n",
       "80387    1059,587,589,1112,2048,2047,2968,1899,2346,135...   \n",
       "\n",
       "                                                   hist_s2  item_id  label  \\\n",
       "32134    0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...     2993      0   \n",
       "856016   1913,3774,2312,2331,2680,3698,2810,2305,1369,2...     1361      1   \n",
       "492110   1657,316,334,508,1196,1960,1727,109,160,3187,0...       39      1   \n",
       "1622647  2919,3638,1182,741,258,32,913,1197,1189,2900,0...     2436      0   \n",
       "80387    1240,1268,958,2803,1180,2298,3393,3036,1272,28...      497      0   \n",
       "\n",
       "         rating  click_timestamp  hist_len  gender  age  item_date  \\\n",
       "32134         0                0         3       1    4         23   \n",
       "856016        2        966541021        50       1    3         63   \n",
       "492110        4        977085349        47       1    2         76   \n",
       "1622647       0                0        50       1    4         80   \n",
       "80387         0                0        50       1    3         74   \n",
       "\n",
       "                                  item_title  item_cate_id  \n",
       "32134    1246,3239,0,0,0,0,0,0,0,0,0,0,0,0,0             5  \n",
       "856016   2338,1193,0,0,0,0,0,0,0,0,0,0,0,0,0             5  \n",
       "492110        85,0,0,0,0,0,0,0,0,0,0,0,0,0,0             5  \n",
       "1622647  3571,3572,0,0,0,0,0,0,0,0,0,0,0,0,0             5  \n",
       "80387      748,960,0,0,0,0,0,0,0,0,0,0,0,0,0             5  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/ml-1m/train_df.csv')\n",
    "test_df = pd.read_csv('../data/ml-1m/test_df.csv')\n",
    "train_df = train_df.sample(frac=1.0)\n",
    "data = train_df.append(test_df)\n",
    "print('train.shape: {}, test.shape: {}'.format(train_df.shape, test_df.shape))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e5c00e",
   "metadata": {},
   "source": [
    "##### 模型构建\n",
    "\n",
    "用户侧\n",
    "\n",
    "- 用户ID\n",
    "- 性别\n",
    "- 年龄\n",
    "- 序列长度\n",
    "- 序列物品ID\n",
    "\n",
    "物品侧\n",
    "\n",
    "- 物品ID\n",
    "- 物品类别ID\n",
    "\n",
    "提取预训练embedding向量，例如提取user侧embedding向量。\n",
    "\n",
    "1. 构建一个完整的模型结构。\n",
    "2. 将用户/物品侧输入和输出张量作为模型属性。\n",
    "3. 实例化一个新Model，选取用户输入输出，输入为用户Input层特征，输出为用户embedding向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aaa15a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([SparseFeature(name='user_id', vocabulary_size=6041, embedding_size=4),\n",
       "  SparseFeature(name='gender', vocabulary_size=3, embedding_size=4),\n",
       "  SparseFeature(name='age', vocabulary_size=8, embedding_size=4),\n",
       "  DenseFeature(name='hist_len', dimension=1),\n",
       "  VarLenSparseFeature(name='hist_item_id', vocabulary_size=3884, embedding_size=4, maxlen=50)],\n",
       " [SparseFeature(name='item_id', vocabulary_size=3884, embedding_size=4),\n",
       "  SparseFeature(name='item_cate_id', vocabulary_size=19, embedding_size=4)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparseFeature = namedtuple('SparseFeature', ['name', 'vocabulary_size', 'embedding_size'])\n",
    "DenseFeature = namedtuple('DenseFeature', ['name', 'dimension'])\n",
    "VarLenSparseFeature = namedtuple('VarLenSparseFeature', ['name', 'vocabulary_size', 'embedding_size', 'maxlen'])\n",
    "\n",
    "user_feature_columns = [\n",
    "    SparseFeature('user_id', data.user_id.max()+1, embedding_size=4),\n",
    "    SparseFeature('gender', data.gender.max()+1, embedding_size=4),\n",
    "    SparseFeature('age', data.age.max()+1, embedding_size=4),\n",
    "    DenseFeature('hist_len', 1),\n",
    "    VarLenSparseFeature('hist_item_id', data.item_id.max()+1, embedding_size=4, maxlen=50)\n",
    "]\n",
    "item_feature_columns = [\n",
    "    SparseFeature('item_id', data.item_id.max()+1, embedding_size=4),\n",
    "    SparseFeature('item_cate_id', data.item_cate_id.max()+1, embedding_size=4),\n",
    "]\n",
    "user_feature_columns, item_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10c41133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "hist_item_id (InputLayer)       [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hist_len (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_id (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gender (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "age (InputLayer)                [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "var_emb_hist_item_id (Embedding (None, 50, 4)        15540       hist_item_id[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "emb_user_id (Embedding)         (None, 1, 4)         24168       user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "emb_gender (Embedding)          (None, 1, 4)         16          gender[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "emb_age (Embedding)             (None, 1, 4)         36          age[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "sequence_pooling_layer_3 (Seque (None, 1, 4)         0           var_emb_hist_item_id[0][0]       \n",
      "                                                                 hist_len[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 4)            0           emb_user_id[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 4)            0           emb_gender[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 4)            0           emb_age[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 4)            0           sequence_pooling_layer_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 17)           0           hist_len[0][0]                   \n",
      "                                                                 flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "                                                                 flatten_14[0][0]                 \n",
      "                                                                 flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 8)            144         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "item_id (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 8)            0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_index_3 (EmbeddingInd (3884,)              0           item_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 4)            36          dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "emb_item_id (Embedding)         (3884, 4)            15540       embedding_index_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 4)            0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "item_cate_id (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sample_softmax_layer_3 (SampleS (None, 1)            3884        emb_item_id[0][0]                \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 item_id[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 59,364\n",
      "Trainable params: 55,480\n",
      "Non-trainable params: 3,884\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def model_metric(prob, label, thr=0.5):\n",
    "    \"\"\" 模型评估 \"\"\"\n",
    "    # AUC\n",
    "    fpr, tpr, threshold = metrics.roc_curve(label, prob)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    score = metrics.accuracy_score(label, prob > thr)\n",
    "    # LogLoss\n",
    "    logloss = log_loss(label, prob)\n",
    "    print('模型准确率:{}, AUC得分:{}, LogLoss:{}'.format(score, auc, logloss))\n",
    "    print(classification_report(label, prob > thr, digits=2))\n",
    "    print('==========================================================')\n",
    "\n",
    "def build_input_layers(feature_columns):\n",
    "    \"\"\" 构建输入层 \"\"\"\n",
    "    dense_input_dict, sparse_input_dict, varlen_sparse_input_dict = {}, {}, {}\n",
    "    for f in feature_columns:\n",
    "        if isinstance(f, DenseFeature):\n",
    "            dense_input_dict[f.name] = Input(shape=(f.dimension, ), name=f.name)\n",
    "        elif isinstance(f, SparseFeature):\n",
    "            sparse_input_dict[f.name] = Input(shape=(1, ), name=f.name)\n",
    "        elif isinstance(f, VarLenSparseFeature):\n",
    "            varlen_sparse_input_dict[f.name] = Input(shape=(f.maxlen, ), name=f.name)\n",
    "    return dense_input_dict, sparse_input_dict, varlen_sparse_input_dict\n",
    "\n",
    "def build_embedding_layers(feature_columns):\n",
    "    \"\"\" 构建embedding层 \"\"\"\n",
    "    embedding_layer_dict = {}\n",
    "    for f in feature_columns:\n",
    "        if isinstance(f, SparseFeature):\n",
    "            embedding_layer_dict[f.name] = Embedding(f.vocabulary_size+1, f.embedding_size, name='emb_' + f.name)\n",
    "        elif isinstance(f, VarLenSparseFeature):\n",
    "            embedding_layer_dict[f.name] = Embedding(f.vocabulary_size+1, f.embedding_size, name='var_emb_' + f.name, mask_zero=True)\n",
    "    return embedding_layer_dict\n",
    "\n",
    "def embedding_lookup(columns, input_dict, embedding_layer_dict, flatten=False):\n",
    "    \"\"\" 根据feature_columns或column_names查表，得到对应embedding向量列表 \"\"\"\n",
    "    embedding_list = []\n",
    "    for f in columns:\n",
    "        if type(f) == str:\n",
    "            column_name = f\n",
    "        else:\n",
    "            column_name = f.name\n",
    "        _input = input_dict[column_name]\n",
    "        _embed = embedding_layer_dict[column_name]\n",
    "        embed_layer = _embed(_input)\n",
    "        if flatten:\n",
    "            embed_layer = Flatten()(embed_layer)\n",
    "        embedding_list.append(embed_layer)\n",
    "    return embedding_list\n",
    "\n",
    "def concat_input_list(input_list):\n",
    "    \"\"\" 合并input列表 \"\"\"\n",
    "    _num = len(input_list)\n",
    "    if _num > 1:\n",
    "        return Concatenate(axis=1)(input_list)\n",
    "    elif len(input_list) == 1:\n",
    "        return input_list[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "class SequencePoolingLayer(Layer):\n",
    "    \"\"\" 根据变长序列embeddings和序列长度input，求pooling embedding \"\"\"\n",
    "    def __init__(self, mode='mean'):\n",
    "        super(SequencePoolingLayer, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.epsilon = tf.constant(1e-8, tf.float32)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.max_seq_len = int(input_shape[0][1])\n",
    "        self.embedding_size = int(input_shape[0][2])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        seq_embed, seq_len = inputs # (None, 50, 4) (None, 1)\n",
    "        mask = tf.sequence_mask(seq_len, self.max_seq_len, dtype=tf.float32) # 构造mask张量 (None, 1, 50)\n",
    "        mask = tf.transpose(mask, (0, 2, 1)) # (None, 50, 1)\n",
    "        mask = tf.tile(mask, [1, 1, self.embedding_size]) # (None, 50, 4)\n",
    "        masked_seq_embed = seq_embed * mask # (None, 50, 4) 序列中补0部分的embedding为0\n",
    "        \n",
    "        # max pooling\n",
    "        if self.mode == 'max':\n",
    "            return tf.reduce_max(masked_seq_embed, 1, keepdims=True) # (None, 1, 4)\n",
    "        elif self.mode == 'mean':\n",
    "            _sum = tf.reduce_sum(masked_seq_embed, 1, keepdims=False)\n",
    "            _mean = tf.divide(_sum, tf.cast(seq_len, tf.float32) + self.epsilon)\n",
    "            return tf.expand_dims(_mean, axis=1) # (None, 1, 4)\n",
    "        elif self.mode == 'sum':\n",
    "            return tf.reduce_sum(masked_seq_embed, 1, keepdims=True) #(None, 1, 4)\n",
    "        else:\n",
    "            raise Exception(\"seq pooling mode error\")\n",
    "\n",
    "class EmbeddingIndex(Layer):\n",
    "    def __init__(self, index):\n",
    "        super(EmbeddingIndex, self).__init__()\n",
    "        self.index = index\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super(EmbeddingIndex, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        return tf.constant(self.index)\n",
    "\n",
    "class SampleSoftmaxLayer(Layer):\n",
    "    def __init__(self, num_sampled=5):\n",
    "        super(SampleSoftmaxLayer, self).__init__()\n",
    "        self.num_sampled = num_sampled\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        super(SampleSoftmaxLayer, self).build(input_shape)\n",
    "        self.item_size = input_shape[0][0] # item个数\n",
    "        self.zero_bias = self.add_weight(shape=[self.item_size],\n",
    "                                         initializer=Zeros(),\n",
    "                                         dtype=tf.float32,\n",
    "                                         trainable=False,\n",
    "                                         name='bias')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        item_embedding_matrix, user_out, item_input = inputs\n",
    "        loss = tf.nn.sampled_softmax_loss(weights=item_embedding_matrix,\n",
    "                                          biases=self.zero_bias,\n",
    "                                          labels=item_input,\n",
    "                                          inputs=user_out,\n",
    "                                          num_sampled=self.num_sampled,\n",
    "                                          num_classes=self.item_size,\n",
    "                                          num_true=1,\n",
    "                                         )\n",
    "        loss = tf.expand_dims(loss, axis=1)\n",
    "        return loss\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'num_sampled': self.num_sampled}\n",
    "        base_config = super(SampleSoftmaxLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class GetItemEmbeddingLayer(Layer):\n",
    "    \"\"\" 根据input从embedding矩阵中获取相应的embedding向量 \"\"\"\n",
    "    def __init__(self):\n",
    "        super(GetItemEmbeddingLayer, self).__init__()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        item_embedding_matrix, item_input = inputs\n",
    "        item_input = tf.cast(item_input, dtype=tf.int32)\n",
    "        _gather = tf.gather(item_embedding_matrix, item_input)\n",
    "        return tf.squeeze(_gather, axis=1)\n",
    "    \n",
    "def get_dnn(dnn_input, hidden_units=[64, 32], activation='relu', l2=0.01):\n",
    "    dnn_list = [Dense(unit, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(l2=l2)) for unit in hidden_units]\n",
    "    dnn_out = dnn_input\n",
    "    for dnn in dnn_list:\n",
    "        dnn_out = Dropout(0.5)(dnn(dnn_out)) # dnn(dnn_out) \n",
    "    return dnn_out\n",
    "    \n",
    "def YouTubeNet(user_feature_columns,\n",
    "               item_feature_columns,\n",
    "               num_sampled=5,\n",
    "               user_dnn_hidden_units=[64, 32]):\n",
    "    user_dense_input_dict, user_sparse_input_dict, user_varlen_sparse_input_dict = build_input_layers(user_feature_columns)\n",
    "    item_dense_input_dict, item_sparse_input_dict, item_varlen_sparse_input_dict = build_input_layers(item_feature_columns)\n",
    "    \n",
    "    # user/item Input\n",
    "    user_input_list = list(user_dense_input_dict.values()) + list(user_sparse_input_dict.values()) + list(user_varlen_sparse_input_dict.values())\n",
    "    item_input_list = list(item_dense_input_dict.values()) + list(item_sparse_input_dict.values()) + list(item_varlen_sparse_input_dict.values())\n",
    "    \n",
    "    # 用户侧 concat(dense feature: input + sparse feature: input->flatten embed->concat + varlen sparse feature:input->pooling embed)\n",
    "    user_dense_input_list = list(user_dense_input_dict.values())\n",
    "    user_embedding_layer_dict = build_embedding_layers(user_feature_columns)\n",
    "    \n",
    "    user_sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeature), user_feature_columns))\n",
    "    user_varlen_sparse_feature_columns = list(filter(lambda x: isinstance(x, VarLenSparseFeature), user_feature_columns))\n",
    "\n",
    "    flatten_user_sparse_embed_list = embedding_lookup(user_sparse_feature_columns, user_sparse_input_dict, user_embedding_layer_dict, flatten=True)\n",
    "    user_varlen_sparse_embed_list = embedding_lookup(user_varlen_sparse_feature_columns, user_varlen_sparse_input_dict, user_embedding_layer_dict)\n",
    "    \n",
    "    # 行为序列池化\n",
    "    _varlen_embed = user_varlen_sparse_embed_list[0]\n",
    "    _varlen_len_input = user_dense_input_dict['hist_len']\n",
    "    \n",
    "    pooling_user_varlen_sparse_embed = SequencePoolingLayer(mode='mean')([_varlen_embed, _varlen_len_input])\n",
    "    flatten_pooling_user_varlen_sparse_embed = Flatten()(pooling_user_varlen_sparse_embed)\n",
    "    concat_flatten_user_sparse_embed_list = concat_input_list(user_dense_input_list + flatten_user_sparse_embed_list + [flatten_pooling_user_varlen_sparse_embed])\n",
    "    user_dnn_out = get_dnn(concat_flatten_user_sparse_embed_list, hidden_units=user_dnn_hidden_units) # (None, 4)\n",
    "\n",
    "    # 物品侧\n",
    "    item_embedding_layer_dict = build_embedding_layers(item_feature_columns)\n",
    "    item_vocabulary_size = item_feature_columns[0].vocabulary_size\n",
    "    item_column_name = item_feature_columns[0].name\n",
    "    # 获取嵌入矩阵\n",
    "    item_index = EmbeddingIndex(list(range(item_vocabulary_size)))(item_sparse_input_dict[item_column_name])\n",
    "    item_embedding_matrix = item_embedding_layer_dict[item_column_name](item_index)\n",
    "    \n",
    "    # concat user and item\n",
    "    #item_emb = item_embedding_layer_dict[item_column_name](item_sparse_input_dict[item_column_name])\n",
    "    #user_dnn_out = Concatenate(axis=1)([user_dnn_out, Flatten()(item_emb)])\n",
    "    #output = Dense(1, activation='sigmoid')(user_dnn_out)\n",
    "\n",
    "    # softmax layer\n",
    "    output = SampleSoftmaxLayer(num_sampled=num_sampled)([\n",
    "        item_embedding_matrix, # 物品权重矩阵\n",
    "        user_dnn_out, # 用户全连接的隐层\n",
    "        item_sparse_input_dict[item_column_name] # 输出层标签\n",
    "    ])\n",
    "    model = Model(inputs=user_input_list + item_input_list, outputs=output)\n",
    "    \n",
    "    # 获取embedding向量\n",
    "    _item_embedding = GetItemEmbeddingLayer()([item_embedding_matrix, item_sparse_input_dict[item_column_name]])\n",
    "    model.__setattr__('user_input', user_input_list)\n",
    "    model.__setattr__('user_embedding', user_dnn_out)\n",
    "    model.__setattr__('item_input', item_input_list)\n",
    "    model.__setattr__('item_embedding', _item_embedding)\n",
    "    \n",
    "    user_embed_model = Model(inputs=model.user_input, outputs=model.user_embedding)\n",
    "    item_embed_model = Model(inputs=model.item_input, outputs=model.item_embedding)\n",
    "    \n",
    "    return model, user_embed_model, item_embed_model\n",
    "\n",
    "model, user_embed_model, item_embed_model = YouTubeNet(user_feature_columns,\n",
    "                                                       item_feature_columns,\n",
    "                                                       num_sampled=2,\n",
    "                                                       user_dnn_hidden_units=[8, 4])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abfd5178",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = {\n",
    "    'user_id': np.array(train_df['user_id']),\n",
    "    'gender': np.array(train_df['gender']),\n",
    "    'age': np.array(train_df['age']),\n",
    "    'item_id': np.array(train_df['item_id']),\n",
    "    'item_cate_id': np.array(train_df['item_cate_id']),\n",
    "    'item_date': np.array(train_df['item_date']),\n",
    "    'hist_item_id': np.array([[int(i) for i in s.split(',')] for s in train_df['hist_item_id']]),\n",
    "    'hist_len': np.array(train_df['hist_len']),\n",
    "}\n",
    "test_input = {\n",
    "    'user_id': np.array(test_df['user_id']),\n",
    "    'gender': np.array(test_df['gender']),\n",
    "    'age': np.array(test_df['age']),\n",
    "    'item_id': np.array(test_df['item_id']),\n",
    "    'item_cate_id': np.array(test_df['item_cate_id']),\n",
    "    'item_date': np.array(test_df['item_date']),\n",
    "    'hist_item_id': np.array([[int(i) for i in s.split(',')] for s in test_df['hist_item_id']]),\n",
    "    'hist_len': np.array(test_df['hist_len']),\n",
    "}\n",
    "# 用户表\n",
    "user_input = {\n",
    "    'user_id': np.array(test_df[test_df.label == 1]['user_id']),\n",
    "    'gender': np.array(test_df[test_df.label == 1]['gender']),\n",
    "    'age': np.array(test_df[test_df.label == 1]['age']),\n",
    "    'hist_item_id': np.array([[int(i) for i in s.split(',')] for s in test_df[test_df.label == 1]['hist_item_id']]),\n",
    "    'hist_len': np.array(test_df[test_df.label == 1]['hist_len']),\n",
    "}\n",
    "# 物品表\n",
    "item_df = data[['item_id', 'item_cate_id', 'item_date']].drop_duplicates(['item_id'])\n",
    "item_input = {\n",
    "    'item_id': np.array(item_df['item_id']),\n",
    "    'item_cate_id': np.array(item_df['item_cate_id']),\n",
    "    'item_date': np.array(item_df['item_date']),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc07ac6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\SoftwareInstall\\Anaconda\\envs\\py36\\lib\\site-packages\\keras\\engine\\functional.py:585: UserWarning: Input dict contained keys ['item_date'] which did not match any model input. They will be ignored by the model.\n",
      "  [n for n in tensors.keys() if n not in ref_input_names])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1529/1529 [==============================] - 19s 11ms/step - loss: 0.4747 - val_loss: 0.4915\n",
      "Epoch 2/100\n",
      "1529/1529 [==============================] - 13s 8ms/step - loss: 0.4752 - val_loss: 0.4823\n",
      "Epoch 3/100\n",
      "1529/1529 [==============================] - 13s 9ms/step - loss: 0.4642 - val_loss: 0.4624\n",
      "Epoch 4/100\n",
      "1529/1529 [==============================] - 13s 8ms/step - loss: 0.4854 - val_loss: 0.4773\n",
      "Epoch 5/100\n",
      "1529/1529 [==============================] - 13s 8ms/step - loss: 0.4923 - val_loss: 0.5315\n",
      "Epoch 6/100\n",
      "1529/1529 [==============================] - 13s 9ms/step - loss: 0.4848 - val_loss: 0.4679\n",
      "Epoch 7/100\n",
      "1529/1529 [==============================] - 13s 8ms/step - loss: 0.4832 - val_loss: 0.4956\n",
      "Epoch 8/100\n",
      "1529/1529 [==============================] - 12s 8ms/step - loss: 0.4634 - val_loss: 0.5003\n",
      "Epoch 9/100\n",
      "1529/1529 [==============================] - 13s 8ms/step - loss: 0.4996 - val_loss: 0.4912\n",
      "Epoch 10/100\n",
      "1529/1529 [==============================] - 13s 8ms/step - loss: 0.4844 - val_loss: 0.4102\n",
      "Epoch 11/100\n",
      "1529/1529 [==============================] - 12s 8ms/step - loss: 0.5043 - val_loss: 0.4988\n",
      "Epoch 12/100\n",
      "1529/1529 [==============================] - 13s 8ms/step - loss: 0.4887 - val_loss: 0.4912\n",
      "Epoch 13/100\n",
      "1529/1529 [==============================] - 13s 8ms/step - loss: 0.4637 - val_loss: 0.4452\n",
      "Epoch 14/100\n",
      "1529/1529 [==============================] - 13s 8ms/step - loss: 0.4757 - val_loss: 0.4607\n",
      "Epoch 15/100\n",
      "1529/1529 [==============================] - 12s 8ms/step - loss: 0.5043 - val_loss: 0.4587\n",
      "Epoch 16/100\n",
      "1529/1529 [==============================] - 13s 8ms/step - loss: 0.4792 - val_loss: 0.5183\n",
      "Epoch 17/100\n",
      "1529/1529 [==============================] - 13s 8ms/step - loss: 0.5026 - val_loss: 0.4709\n",
      "Epoch 18/100\n",
      "1529/1529 [==============================] - 13s 8ms/step - loss: 0.4878 - val_loss: 0.4736\n",
      "Epoch 19/100\n",
      "1529/1529 [==============================] - 13s 9ms/step - loss: 0.4918 - val_loss: 0.4971\n",
      "Epoch 20/100\n",
      "1529/1529 [==============================] - 13s 9ms/step - loss: 0.4912 - val_loss: 0.4771\n",
      "Epoch 00020: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x257499d04a8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型训练\n",
    "my_callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, verbose=2, mode='auto')\n",
    "]\n",
    "\n",
    "def sampled_softmax_loss(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "\n",
    "model.compile('adam',\n",
    "              loss=sampled_softmax_loss)\n",
    "\n",
    "model.fit(train_input,\n",
    "          train_df['label'].values,\n",
    "          batch_size=1024,\n",
    "          epochs=100,\n",
    "          validation_split=0.2,\n",
    "          callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1dcd15eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型准确率:0.4611690677264448, AUC得分:0.4539537125797104, LogLoss:4.179249416828256\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.69      0.56      6038\n",
      "           1       0.43      0.24      0.30      6040\n",
      "\n",
      "    accuracy                           0.46     12078\n",
      "   macro avg       0.45      0.46      0.43     12078\n",
      "weighted avg       0.45      0.46      0.43     12078\n",
      "\n",
      "==========================================================\n"
     ]
    }
   ],
   "source": [
    "# 模型预测与评估\n",
    "result = model.predict(test_input)\n",
    "model_metric(np.array([i[0] for i in result]).astype(np.float64), test_df['label'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2f2283",
   "metadata": {},
   "source": [
    "##### Embedding召回\n",
    "\n",
    "① 提取user和item embedding向量。\n",
    "\n",
    "② 构建faiss索引求用户TopN相似物品。\n",
    "\n",
    "③ 评估召回率和hit rate。\n",
    "\n",
    "④ 保存用户、物品向量到本地。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6eb2af31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\SoftwareInstall\\Anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:19: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b2ce48818644e3977287248379fc4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall:  0.075\n"
     ]
    }
   ],
   "source": [
    "def get_recall(true_y, pred_y, top_n=50):\n",
    "    \"\"\" 召回率 \"\"\"\n",
    "    return len(set(pred_y[:top_n])&set(true_y)) * 1.0 / len(true_y)\n",
    "\n",
    "# 1. 提取embedding向量。\n",
    "user_embeddings = user_embed_model.predict(user_input, batch_size=2**12)\n",
    "item_embeddings = item_embed_model.predict(item_input, batch_size=2**12)\n",
    "test_user_item_dict = test_df[test_df.label == 1][['user_id', 'item_id']].set_index('user_id').item_id.to_dict()\n",
    "\n",
    "# 2. faiss求TopN相似物品。\n",
    "embedding_size = 4\n",
    "index = faiss.IndexFlatIP(embedding_size)\n",
    "index.add(item_embeddings)\n",
    "D, I = index.search(np.ascontiguousarray(user_embeddings), 50)\n",
    "\n",
    "# 3. 评估召回率和hit rate。\n",
    "hit = 0\n",
    "recall_list = []\n",
    "for i, uid in tqdm(enumerate(user_input['user_id'])):\n",
    "    preds = [item_df['item_id'].values[j] for j in I[i]]\n",
    "    recall = get_recall([test_user_item_dict[uid]], preds, top_n=50)\n",
    "    recall_list.append(recall)\n",
    "    if test_user_item_dict[uid] in preds:\n",
    "        hit += 1\n",
    "\n",
    "print('recall: ', np.mean(recall_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6f6b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
