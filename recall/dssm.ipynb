{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5a05050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import  MinMaxScaler, LabelEncoder\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import classification_report, roc_curve\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2187a481",
   "metadata": {},
   "source": [
    "##### 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0c81e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (1956191, 14), test.shape: (12078, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>hist_item_id</th>\n",
       "      <th>hist_s1</th>\n",
       "      <th>hist_s2</th>\n",
       "      <th>item_id</th>\n",
       "      <th>label</th>\n",
       "      <th>rating</th>\n",
       "      <th>click_timestamp</th>\n",
       "      <th>hist_len</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>item_date</th>\n",
       "      <th>item_title</th>\n",
       "      <th>item_cate_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>597744</th>\n",
       "      <td>3547</td>\n",
       "      <td>2289,1029,791,1172,1034,605,1516,2531,2694,117...</td>\n",
       "      <td>2234,1961,124,757,1252,2817,2056,1898,3187,226...</td>\n",
       "      <td>45,3431,535,263,1570,2087,1577,1783,2268,1583,...</td>\n",
       "      <td>506</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>966836312</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>74</td>\n",
       "      <td>971,26,0,0,0,0,0,0,0,0,0,0,0,0,0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903300</th>\n",
       "      <td>4551</td>\n",
       "      <td>901,1210,1010,258,3100,1576,3530,3730,3503,351...</td>\n",
       "      <td>2624,438,1844,3178,161,498,1389,344,2632,3358,...</td>\n",
       "      <td>605,2262,1671,189,304,25,3660,1041,2371,316,0,...</td>\n",
       "      <td>1776</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>964559469</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>78</td>\n",
       "      <td>671,877,0,0,0,0,0,0,0,0,0,0,0,0,0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084615</th>\n",
       "      <td>3311</td>\n",
       "      <td>1272,3725,225,2126,589,1569,2844,3510,1205,111...</td>\n",
       "      <td>350,1035,2917,1406,3232,1547,10,1933,1798,2335...</td>\n",
       "      <td>2695,374,3696,2338,1350,1702,3636,1036,1733,34...</td>\n",
       "      <td>2341</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>967955234</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>2952,15,0,0,0,0,0,0,0,0,0,0,0,0,0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398942</th>\n",
       "      <td>5782</td>\n",
       "      <td>1246,1999,1016,505,1109,1283,2284,1020,2729,28...</td>\n",
       "      <td>353,150,583,280,1177,2069,1059,3353,2459,1945,...</td>\n",
       "      <td>1929,584,2450,1893,1227,316,34,62,294,3184,0,0...</td>\n",
       "      <td>412</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>74</td>\n",
       "      <td>826,827,0,0,0,0,0,0,0,0,0,0,0,0,0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788932</th>\n",
       "      <td>4444</td>\n",
       "      <td>2615,2631,2639,2937,2642,2702,2703,2948,2365,2...</td>\n",
       "      <td>2695,2919,258,1179,1181,1240,587,1180,2803,119...</td>\n",
       "      <td>2513,2434,3151,2530,1479,2624,2327,2328,2694,2...</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>965090434</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>75</td>\n",
       "      <td>711,19,712,713,0,0,0,0,0,0,0,0,0,0,0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id                                       hist_item_id  \\\n",
       "597744      3547  2289,1029,791,1172,1034,605,1516,2531,2694,117...   \n",
       "1903300     4551  901,1210,1010,258,3100,1576,3530,3730,3503,351...   \n",
       "1084615     3311  1272,3725,225,2126,589,1569,2844,3510,1205,111...   \n",
       "1398942     5782  1246,1999,1016,505,1109,1283,2284,1020,2729,28...   \n",
       "788932      4444  2615,2631,2639,2937,2642,2702,2703,2948,2365,2...   \n",
       "\n",
       "                                                   hist_s1  \\\n",
       "597744   2234,1961,124,757,1252,2817,2056,1898,3187,226...   \n",
       "1903300  2624,438,1844,3178,161,498,1389,344,2632,3358,...   \n",
       "1084615  350,1035,2917,1406,3232,1547,10,1933,1798,2335...   \n",
       "1398942  353,150,583,280,1177,2069,1059,3353,2459,1945,...   \n",
       "788932   2695,2919,258,1179,1181,1240,587,1180,2803,119...   \n",
       "\n",
       "                                                   hist_s2  item_id  label  \\\n",
       "597744   45,3431,535,263,1570,2087,1577,1783,2268,1583,...      506      1   \n",
       "1903300  605,2262,1671,189,304,25,3660,1041,2371,316,0,...     1776      1   \n",
       "1084615  2695,374,3696,2338,1350,1702,3636,1036,1733,34...     2341      1   \n",
       "1398942  1929,584,2450,1893,1227,316,34,62,294,3184,0,0...      412      0   \n",
       "788932   2513,2434,3151,2530,1479,2624,2327,2328,2694,2...      346      1   \n",
       "\n",
       "         rating  click_timestamp  hist_len  gender  age  item_date  \\\n",
       "597744        4        966836312        50       1    4         74   \n",
       "1903300       4        964559469        35       1    2         78   \n",
       "1084615       2        967955234        50       1    3         60   \n",
       "1398942       0                0        50       2    4         74   \n",
       "788932        2        965090434        50       2    3         75   \n",
       "\n",
       "                                   item_title  item_cate_id  \n",
       "597744       971,26,0,0,0,0,0,0,0,0,0,0,0,0,0             8  \n",
       "1903300     671,877,0,0,0,0,0,0,0,0,0,0,0,0,0             8  \n",
       "1084615     2952,15,0,0,0,0,0,0,0,0,0,0,0,0,0             1  \n",
       "1398942     826,827,0,0,0,0,0,0,0,0,0,0,0,0,0             5  \n",
       "788932   711,19,712,713,0,0,0,0,0,0,0,0,0,0,0             1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/ml-1m/train_df.csv')\n",
    "test_df = pd.read_csv('../data/ml-1m/test_df.csv')\n",
    "train_df = train_df.sample(frac=1.0)\n",
    "data = train_df.append(test_df)\n",
    "print('train.shape: {}, test.shape: {}'.format(train_df.shape, test_df.shape))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff4da77",
   "metadata": {},
   "source": [
    "##### 模型构建\n",
    "\n",
    "用户侧\n",
    "\n",
    "- 用户ID\n",
    "- 性别\n",
    "- 年龄\n",
    "- 序列长度\n",
    "- 序列物品ID\n",
    "\n",
    "物品侧\n",
    "\n",
    "- 物品ID\n",
    "- 物品类别ID\n",
    "\n",
    "提取预训练embedding向量，例如提取user侧embedding向量。\n",
    "\n",
    "1. 构建一个完整的模型结构。\n",
    "2. 将用户/物品侧输入和输出张量作为模型属性。\n",
    "3. 实例化一个新Model，选取用户输入输出，输入为用户Input层特征，输出为用户embedding向量。\n",
    "\n",
    "tf.clip_by_value(t, min, max) 将张量t的元素范围限制在[min, max]之间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb21089a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([SparseFeature(name='user_id', vocabulary_size=6041, embedding_size=4),\n",
       "  SparseFeature(name='gender', vocabulary_size=3, embedding_size=4),\n",
       "  SparseFeature(name='age', vocabulary_size=8, embedding_size=4),\n",
       "  DenseFeature(name='hist_len', dimension=1),\n",
       "  VarLenSparseFeature(name='hist_item_id', vocabulary_size=3884, embedding_size=4, maxlen=50)],\n",
       " [SparseFeature(name='item_id', vocabulary_size=3884, embedding_size=4),\n",
       "  SparseFeature(name='item_cate_id', vocabulary_size=19, embedding_size=4)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparseFeature = namedtuple('SparseFeature', ['name', 'vocabulary_size', 'embedding_size'])\n",
    "DenseFeature = namedtuple('DenseFeature', ['name', 'dimension'])\n",
    "VarLenSparseFeature = namedtuple('VarLenSparseFeature', ['name', 'vocabulary_size', 'embedding_size', 'maxlen'])\n",
    "\n",
    "user_feature_columns = [\n",
    "    SparseFeature('user_id', data.user_id.max()+1, embedding_size=4),\n",
    "    SparseFeature('gender', data.gender.max()+1, embedding_size=4),\n",
    "    SparseFeature('age', data.age.max()+1, embedding_size=4),\n",
    "    DenseFeature('hist_len', 1),\n",
    "    VarLenSparseFeature('hist_item_id', data.item_id.max()+1, embedding_size=4, maxlen=50)\n",
    "]\n",
    "item_feature_columns = [\n",
    "    SparseFeature('item_id', data.item_id.max()+1, embedding_size=4),\n",
    "    SparseFeature('item_cate_id', data.item_cate_id.max()+1, embedding_size=4),\n",
    "    #DenseFeature('item_date', 1),\n",
    "]\n",
    "user_feature_columns, item_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "173e937b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_units:  [4]\n",
      "hidden_units:  [4]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_id (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gender (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "age (InputLayer)                [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_id (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_cate_id (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "emb_user_id (Embedding)         (None, 1, 4)         24168       user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "emb_gender (Embedding)          (None, 1, 4)         16          gender[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "emb_age (Embedding)             (None, 1, 4)         36          age[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "emb_item_id (Embedding)         (None, 1, 4)         15540       item_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "emb_item_cate_id (Embedding)    (None, 1, 4)         80          item_cate_id[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "hist_len (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 4)            0           emb_user_id[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4)            0           emb_gender[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 4)            0           emb_age[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 4)            0           emb_item_id[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 4)            0           emb_item_cate_id[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 13)           0           hist_len[0][0]                   \n",
      "                                                                 flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8)            0           flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 4)            56          concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4)            36          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 4)            0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 4)            0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "similarity (Similarity)         (None, 1)            0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            2           similarity[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 39,934\n",
      "Trainable params: 39,934\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def model_metric(prob, label, thr=0.5):\n",
    "    \"\"\" 模型评估 \"\"\"\n",
    "    # AUC\n",
    "    fpr, tpr, threshold = metrics.roc_curve(label, prob)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    score = metrics.accuracy_score(label, prob > thr)\n",
    "    # LogLoss\n",
    "    logloss = log_loss(label, prob)\n",
    "    print('模型准确率:{}, AUC得分:{}, LogLoss:{}'.format(score, auc, logloss))\n",
    "    print(classification_report(label, prob > thr, digits=2))\n",
    "    print('==========================================================')\n",
    "\n",
    "def build_input_layers(feature_columns):\n",
    "    \"\"\" 构建输入层 \"\"\"\n",
    "    dense_input_dict, sparse_input_dict, varlen_sparse_input_dict = {}, {}, {}\n",
    "    for f in feature_columns:\n",
    "        if isinstance(f, DenseFeature):\n",
    "            dense_input_dict[f.name] = Input(shape=(f.dimension, ), name=f.name)\n",
    "        elif isinstance(f, SparseFeature):\n",
    "            sparse_input_dict[f.name] = Input(shape=(1, ), name=f.name)\n",
    "        elif isinstance(f, VarLenSparseFeature):\n",
    "            varlen_sparse_input_dict[f.name] = Input(shape=(f.maxlen, ), name=f.name)\n",
    "    return dense_input_dict, sparse_input_dict, varlen_sparse_input_dict\n",
    "\n",
    "def concat_input_list(input_list):\n",
    "    \"\"\" 合并input列表 \"\"\"\n",
    "    _num = len(input_list)\n",
    "    if _num > 1:\n",
    "        return Concatenate(axis=1)(input_list)\n",
    "    elif len(input_list) == 1:\n",
    "        return input_list[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def build_embedding_layers(feature_columns):\n",
    "    \"\"\" 构建embedding层 \"\"\"\n",
    "    embedding_layer_dict = {}\n",
    "    for f in feature_columns:\n",
    "        if isinstance(f, SparseFeature):\n",
    "            embedding_layer_dict[f.name] = Embedding(f.vocabulary_size+1, f.embedding_size, name='emb_' + f.name)\n",
    "        elif isinstance(f, VarLenSparseFeature):\n",
    "            embedding_layer_dict[f.name] = Embedding(f.vocabulary_size+1, f.embedding_size, name='var_emb_' + f.name, mask_zero=True)\n",
    "    return embedding_layer_dict\n",
    "    \n",
    "def embedding_lookup(columns, input_dict, embedding_layer_dict, flatten=False):\n",
    "    \"\"\" 根据feature_columns或column_names查表，得到对应embedding向量列表 \"\"\"\n",
    "    embedding_list = []\n",
    "    for f in columns:\n",
    "        if type(f) == str:\n",
    "            column_name = f\n",
    "        else:\n",
    "            column_name = f.name\n",
    "        _input = input_dict[column_name]\n",
    "        _embed = embedding_layer_dict[column_name]\n",
    "        embed_layer = _embed(_input)\n",
    "        if flatten:\n",
    "            embed_layer = Flatten()(embed_layer)\n",
    "        embedding_list.append(embed_layer)\n",
    "    return embedding_list\n",
    "    \n",
    "def get_dnn(dnn_input, hidden_units=[64, 32], activation='relu', l2=0.01):\n",
    "    print('hidden_units: ', hidden_units)\n",
    "    dnn_list = [Dense(unit, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(l2=l2)) for unit in hidden_units]\n",
    "    dnn_out = dnn_input\n",
    "    for dnn in dnn_list:\n",
    "        dnn_out = Dropout(0.5)(dnn(dnn_out)) # dnn(dnn_out)\n",
    "    return dnn_out\n",
    "\n",
    "class Similarity(Layer):\n",
    "    def __init__(self, gamma=1, similarity_type='cosine'):\n",
    "        super(Similarity, self).__init__()\n",
    "        self.gamma = 1\n",
    "        self.similarity_type = similarity_type\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        query, candidate = inputs\n",
    "        if self.similarity_type == 'cosine':\n",
    "            score = tf.reduce_sum(tf.multiply(query, candidate), -1, keepdims=True) # 点积\n",
    "            norm_query = tf.norm(query, axis=-1, keepdims=True) \n",
    "            norm_candidate = tf.norm(candidate, axis=-1, keepdims=True)\n",
    "            score = tf.divide(score, norm_query * norm_candidate + 1e-8)\n",
    "        elif self.similarity_type == 'inner_product':\n",
    "            score = tf.reduce_sum(tf.multiply(query, candidate), -1, keepdims=True) # 点积\n",
    "        else:\n",
    "            raise Exception(\"similarity_type error\")\n",
    "        return score\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 1)\n",
    "    \n",
    "def DSSM(user_feature_columns,\n",
    "         item_feature_columns,\n",
    "         user_dnn_hidden_units=[8, 4],\n",
    "         item_dnn_hidden_units=[8, 4],\n",
    "         dnn_activation='relu',\n",
    "         l2=0.01, dnn_dropout=0.5):\n",
    "    \"\"\" Deep Structured Semantic Model \"\"\"\n",
    "    user_dense_input_dict, user_sparse_input_dict, _ = build_input_layers(user_feature_columns)\n",
    "    item_dense_input_dict, item_sparse_input_dict, _ = build_input_layers(item_feature_columns)\n",
    "    \n",
    "    # user/item Input\n",
    "    user_input_list = list(user_dense_input_dict.values()) + list(user_sparse_input_dict.values())\n",
    "    item_input_list = list(item_dense_input_dict.values()) + list(item_sparse_input_dict.values())\n",
    "\n",
    "    # 用户侧 concat(dense feature: input + sparse feature: input->flatten embed->concat) =》DNN =》user_embedding\n",
    "    user_dense_input_list = list(user_dense_input_dict.values())\n",
    "    user_embedding_layer_dict = build_embedding_layers(user_feature_columns)\n",
    "    user_sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeature), user_feature_columns))\n",
    "    flatten_user_sparse_embed_list = embedding_lookup(user_sparse_feature_columns, user_sparse_input_dict, user_embedding_layer_dict, flatten=True)\n",
    "    user_dnn_input = concat_input_list(user_dense_input_list + flatten_user_sparse_embed_list)\n",
    "    user_dnn_out = get_dnn(user_dnn_input, hidden_units=user_dnn_hidden_units)\n",
    "    \n",
    "    # 物品侧 concat(dense feature: input + sparse feature: input->flatten embed->concat) =》DNN =》item_embedding\n",
    "    item_dense_input_list = list(item_dense_input_dict.values())\n",
    "    item_embedding_layer_dict = build_embedding_layers(item_feature_columns)\n",
    "    item_sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeature), item_feature_columns))\n",
    "    flatten_item_sparse_embed_list = embedding_lookup(item_sparse_feature_columns, item_sparse_input_dict, item_embedding_layer_dict, flatten=True)\n",
    "    item_dnn_input = concat_input_list(item_dense_input_list + flatten_item_sparse_embed_list)\n",
    "    item_dnn_out = get_dnn(item_dnn_input, hidden_units=item_dnn_hidden_units)\n",
    "    \n",
    "    # cosine inner_product\n",
    "    score = Similarity()([user_dnn_out, item_dnn_out]) # (None, 1)\n",
    "    \n",
    "    output_layer = Dense(1, activation='sigmoid')(score)\n",
    "    model = Model(user_input_list+item_input_list, output_layer)\n",
    "    \n",
    "    model.__setattr__(\"user_input\", user_input_list)\n",
    "    model.__setattr__(\"item_input\", item_input_list)\n",
    "    model.__setattr__(\"user_embedding\", user_dnn_out)\n",
    "    model.__setattr__(\"item_embedding\", item_dnn_out)\n",
    "    \n",
    "    user_embed_model = Model(inputs=model.user_input, outputs=model.user_embedding)\n",
    "    item_embed_model = Model(inputs=model.item_input, outputs=model.item_embedding)\n",
    "    return model, user_embed_model, item_embed_model\n",
    "\n",
    "model, user_embed_model, item_embed_model = DSSM(user_feature_columns,\n",
    "                                                 item_feature_columns, \n",
    "                                                 user_dnn_hidden_units=[4],\n",
    "                                                 item_dnn_hidden_units=[4])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c17bff",
   "metadata": {},
   "source": [
    "##### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "054d2c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = {\n",
    "    'user_id': np.array(train_df['user_id']),\n",
    "    'gender': np.array(train_df['gender']),\n",
    "    'age': np.array(train_df['age']),\n",
    "    'item_id': np.array(train_df['item_id']),\n",
    "    'item_cate_id': np.array(train_df['item_cate_id']),\n",
    "    'item_date': np.array(train_df['item_date']),\n",
    "    'hist_item_id': np.array([[int(i) for i in s.split(',')] for s in train_df['hist_item_id']]),\n",
    "    'hist_len': np.array(train_df['hist_len']),\n",
    "}\n",
    "test_input = {\n",
    "    'user_id': np.array(test_df['user_id']),\n",
    "    'gender': np.array(test_df['gender']),\n",
    "    'age': np.array(test_df['age']),\n",
    "    'item_id': np.array(test_df['item_id']),\n",
    "    'item_cate_id': np.array(test_df['item_cate_id']),\n",
    "    'item_date': np.array(test_df['item_date']),\n",
    "    'hist_item_id': np.array([[int(i) for i in s.split(',')] for s in test_df['hist_item_id']]),\n",
    "    'hist_len': np.array(test_df['hist_len']),\n",
    "}\n",
    "# 用户表\n",
    "user_input = {\n",
    "    'user_id': np.array(test_df[test_df.label == 1]['user_id']),\n",
    "    'gender': np.array(test_df[test_df.label == 1]['gender']),\n",
    "    'age': np.array(test_df[test_df.label == 1]['age']),\n",
    "    'hist_item_id': np.array([[int(i) for i in s.split(',')] for s in test_df[test_df.label == 1]['hist_item_id']]),\n",
    "    'hist_len': np.array(test_df[test_df.label == 1]['hist_len']),\n",
    "}\n",
    "# 物品表\n",
    "item_df = data[['item_id', 'item_cate_id', 'item_date']].drop_duplicates(['item_id'])\n",
    "item_input = {\n",
    "    'item_id': np.array(item_df['item_id']),\n",
    "    'item_cate_id': np.array(item_df['item_cate_id']),\n",
    "    'item_date': np.array(item_df['item_date']),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a78c3c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1529/1529 [==============================] - 4s 2ms/step - loss: 0.6878 - binary_crossentropy: 0.6700 - auc: 0.5973 - val_loss: 0.5850 - val_binary_crossentropy: 0.5839 - val_auc: 0.7729- binary_crossentropy: 0.6767 - auc: 0. - ETA: 0s - loss: 0.6965 - binary_crossentropy: 0.6744 - a\n",
      "Epoch 2/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6528 - binary_crossentropy: 0.6521 - auc: 0.6091 - val_loss: 0.5739 - val_binary_crossentropy: 0.5733 - val_auc: 0.7905\n",
      "Epoch 3/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6547 - binary_crossentropy: 0.6538 - auc: 0.6014 - val_loss: 0.5778 - val_binary_crossentropy: 0.5756 - val_auc: 0.8295- loss: 0.6504 - binary_crossentrop\n",
      "Epoch 4/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6646 - binary_crossentropy: 0.6630 - auc: 0.5905 - val_loss: 0.5708 - val_binary_crossentropy: 0.5698 - val_auc: 0.8425\n",
      "Epoch 5/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6534 - binary_crossentropy: 0.6527 - auc: 0.6191 - val_loss: 0.5628 - val_binary_crossentropy: 0.5621 - val_auc: 0.83790.6540 - binary_crossentropy: 0.6531 - auc: 0.61 - ETA: 0s - loss: 0.6540 - binary_crossentropy: 0.65\n",
      "Epoch 6/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6506 - binary_crossentropy: 0.6500 - auc: 0.6234 - val_loss: 0.5614 - val_binary_crossentropy: 0.5609 - val_auc: 0.8319\n",
      "Epoch 7/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6492 - binary_crossentropy: 0.6488 - auc: 0.6250 - val_loss: 0.5598 - val_binary_crossentropy: 0.5594 - val_auc: 0.8292rossentropy: 0.6488 - auc: 0. - ETA: 0s - loss: 0.6493 - binary_crossentropy: 0.6489 - auc: 0.6\n",
      "Epoch 8/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6491 - binary_crossentropy: 0.6487 - auc: 0.6243 - val_loss: 0.5600 - val_binary_crossentropy: 0.5596 - val_auc: 0.8257\n",
      "Epoch 9/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6487 - binary_crossentropy: 0.6483 - auc: 0.6246 - val_loss: 0.5589 - val_binary_crossentropy: 0.5586 - val_auc: 0.8266\n",
      "Epoch 10/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6487 - binary_crossentropy: 0.6482 - auc: 0.6237 - val_loss: 0.5623 - val_binary_crossentropy: 0.5617 - val_auc: 0.8150\n",
      "Epoch 11/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6482 - binary_crossentropy: 0.6476 - auc: 0.6246 - val_loss: 0.5597 - val_binary_crossentropy: 0.5592 - val_auc: 0.8242\n",
      "Epoch 12/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6484 - binary_crossentropy: 0.6480 - auc: 0.6246 - val_loss: 0.5605 - val_binary_crossentropy: 0.5601 - val_auc: 0.81730.6488 - binary_crossent\n",
      "Epoch 13/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6483 - binary_crossentropy: 0.6480 - auc: 0.6249 - val_loss: 0.5607 - val_binary_crossentropy: 0.5604 - val_auc: 0.8233.6482 - binary_crossentropy:  - ETA: 0s - loss: 0.6484 - binary_crossentropy: 0.6481 - auc: 0.6 - ETA: 0s - loss: 0.6484 - binary_crossentropy: 0.6480 - auc: 0.6\n",
      "Epoch 14/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6482 - binary_crossentropy: 0.6479 - auc: 0.6247 - val_loss: 0.5593 - val_binary_crossentropy: 0.5590 - val_auc: 0.8200nary_crossentropy: 0.64\n",
      "Epoch 15/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6481 - binary_crossentropy: 0.6479 - auc: 0.6258 - val_loss: 0.5596 - val_binary_crossentropy: 0.5594 - val_auc: 0.8241loss: 0.6481 - binary_crossentropy: 0.6479 - auc: 0.62\n",
      "Epoch 16/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6481 - binary_crossentropy: 0.6479 - auc: 0.6250 - val_loss: 0.5606 - val_binary_crossentropy: 0.5604 - val_auc: 0.8247\n",
      "Epoch 17/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6488 - binary_crossentropy: 0.6487 - auc: 0.6240 - val_loss: 0.5593 - val_binary_crossentropy: 0.5592 - val_auc: 0.8238\n",
      "Epoch 18/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6477 - binary_crossentropy: 0.6475 - auc: 0.6252 - val_loss: 0.5604 - val_binary_crossentropy: 0.5603 - val_auc: 0.8229\n",
      "Epoch 19/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6486 - binary_crossentropy: 0.6484 - auc: 0.6247 - val_loss: 0.5575 - val_binary_crossentropy: 0.5574 - val_auc: 0.8259\n",
      "Epoch 20/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6478 - binary_crossentropy: 0.6476 - auc: 0.6244 - val_loss: 0.5605 - val_binary_crossentropy: 0.5602 - val_auc: 0.8237- loss: 0.6479 - binary_cros\n",
      "Epoch 21/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6481 - binary_crossentropy: 0.6479 - auc: 0.6250 - val_loss: 0.5599 - val_binary_crossentropy: 0.5598 - val_auc: 0.8233\n",
      "Epoch 22/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6482 - binary_crossentropy: 0.6481 - auc: 0.6248 - val_loss: 0.5614 - val_binary_crossentropy: 0.5612 - val_auc: 0.8235\n",
      "Epoch 23/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6479 - binary_crossentropy: 0.6476 - auc: 0.6245 - val_loss: 0.5601 - val_binary_crossentropy: 0.5598 - val_auc: 0.8213\n",
      "Epoch 24/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6480 - binary_crossentropy: 0.6478 - auc: 0.6240 - val_loss: 0.5608 - val_binary_crossentropy: 0.5605 - val_auc: 0.8228\n",
      "Epoch 25/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6491 - binary_crossentropy: 0.6489 - auc: 0.6233 - val_loss: 0.5709 - val_binary_crossentropy: 0.5705 - val_auc: 0.8190\n",
      "Epoch 26/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6691 - binary_crossentropy: 0.6688 - auc: 0.5701 - val_loss: 0.5798 - val_binary_crossentropy: 0.5796 - val_auc: 0.8178 0.6692 - binary_crossentropy: 0.6689 - auc:  - ETA: 0s - loss: 0.6691 - binary_crossentropy: 0.6689 - auc: 0.570 - ETA: 0s - loss: 0.6691 - binary_crossentropy: 0.6688\n",
      "Epoch 27/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6682 - binary_crossentropy: 0.6681 - auc: 0.5704 - val_loss: 0.5760 - val_binary_crossentropy: 0.5758 - val_auc: 0.8204binary_crossentropy: \n",
      "Epoch 28/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6594 - binary_crossentropy: 0.6591 - auc: 0.6102 - val_loss: 0.5607 - val_binary_crossentropy: 0.5604 - val_auc: 0.8196\n",
      "Epoch 29/100\n",
      "1529/1529 [==============================] - 3s 2ms/step - loss: 0.6482 - binary_crossentropy: 0.6478 - auc: 0.6248 - val_loss: 0.5612 - val_binary_crossentropy: 0.5608 - val_auc: 0.8149\n",
      "Epoch 00029: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x138e1586668>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型训练\n",
    "my_callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, verbose=2, mode='auto')\n",
    "]\n",
    "model.compile('adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[\"binary_crossentropy\", tf.keras.metrics.AUC(name='auc')])\n",
    "model.fit(train_input,\n",
    "          train_df['label'].values,\n",
    "          batch_size=1024,\n",
    "          epochs=100,\n",
    "          validation_split=0.2,\n",
    "          callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c54593a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型准确率:0.7321576419937076, AUC得分:0.7706611301711677, LogLoss:0.5971096853772957\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.76      0.74      6038\n",
      "           1       0.74      0.71      0.73      6040\n",
      "\n",
      "    accuracy                           0.73     12078\n",
      "   macro avg       0.73      0.73      0.73     12078\n",
      "weighted avg       0.73      0.73      0.73     12078\n",
      "\n",
      "==========================================================\n"
     ]
    }
   ],
   "source": [
    "# 模型预测与评估\n",
    "result = model.predict(test_input)\n",
    "model_metric(np.array([i[0] for i in result]), test_df['label'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5874344c",
   "metadata": {},
   "source": [
    "##### Embedding召回\n",
    "\n",
    "① 提取user和item embedding向量。\n",
    "\n",
    "② 构建faiss索引求用户TopN相似物品。\n",
    "\n",
    "③ 评估召回率和hit rate。\n",
    "\n",
    "④ 保存用户、物品向量到本地。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de92a61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f1a7e8a4b6494eaccbac1ea3cf725b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall:  0.10231788079470198\n"
     ]
    }
   ],
   "source": [
    "def get_recall(true_y, pred_y, top_n=50):\n",
    "    \"\"\" 召回率 \"\"\"\n",
    "    return len(set(pred_y[:top_n])&set(true_y)) * 1.0 / len(true_y)\n",
    "\n",
    "# 1. 提取embedding向量。\n",
    "user_embeddings = user_embed_model.predict(user_input, batch_size=2**12)\n",
    "item_embeddings = item_embed_model.predict(item_input, batch_size=2**12)\n",
    "test_user_item_dict = test_df[test_df.label == 1][['user_id', 'item_id']].set_index('user_id').item_id.to_dict()\n",
    "\n",
    "# 2. faiss求TopN相似物品。\n",
    "embedding_size = 4\n",
    "index = faiss.IndexFlatIP(embedding_size)\n",
    "index.add(item_embeddings)\n",
    "D, I = index.search(np.ascontiguousarray(user_embeddings), 50)\n",
    "\n",
    "# 3. 评估召回率和hit rate。\n",
    "hit = 0\n",
    "recall_list = []\n",
    "for i, uid in tqdm(enumerate(user_input['user_id'])):\n",
    "    preds = [item_df['item_id'].values[j] for j in I[i]]\n",
    "    recall = get_recall([test_user_item_dict[uid]], preds, top_n=50)\n",
    "    recall_list.append(recall)\n",
    "    if test_user_item_dict[uid] in preds:\n",
    "        hit += 1\n",
    "\n",
    "print('recall: ', np.mean(recall_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f04ec7",
   "metadata": {},
   "source": [
    "##### 模型评估\n",
    "recall:  0.10233482364629906"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60db333c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
